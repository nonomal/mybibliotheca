from flask import Blueprint, current_app, render_template, request, redirect, url_for, jsonify, flash, send_file, abort, make_response, session
from flask_login import login_required, current_user
from .domain.models import Book as DomainBook, Author, Person, BookContribution, ContributionType, Publisher, Series, User, ReadingStatus, OwnershipStatus, CustomFieldDefinition, ImportMappingTemplate
from .services import book_service, user_service, reading_log_service, custom_field_service, import_mapping_service, direct_import_service
from .simplified_book_service import SimplifiedBookService, SimplifiedBook
from .utils import fetch_book_data, get_reading_streak, get_google_books_cover, generate_month_review_image, fetch_author_data
from .infrastructure.kuzu_clean_repositories import CleanKuzuPersonRepository
from datetime import datetime, date, timedelta
import secrets
import requests
from io import BytesIO
import pytz
import tempfile
import csv
import threading
import uuid
import os
from werkzeug.utils import secure_filename
import csv # Ensure csv is imported
import calendar
import uuid
import tempfile
import json
import threading  # Add threading import
import traceback  # Add traceback import
import inspect  # Add inspect import
from werkzeug.utils import secure_filename

bp = Blueprint('main', __name__)

# Import Progress Tracking
import_jobs = {}  # In-memory storage for demo - use Kuzu database in production

def _convert_published_date_to_date(published_date_str):
    """Convert published_date string to date object using enhanced date parser."""
    if not published_date_str or not isinstance(published_date_str, str):
        return None
    
    try:
        # Use the enhanced date parser from the direct import service
        # This handles all the comprehensive date formats
        return direct_import_service._detect_and_parse_date(published_date_str, "published_date from form")
    except Exception as e:
        current_app.logger.warning(f"Failed to parse published date '{published_date_str}': {e}")
        return None
    
    return None

@bp.route('/log_book', methods=['POST'])
@login_required
def log_book():
    data = request.json
    title = data.get('title')
    author = data.get('author')
    isbn = data.get('isbn')
    
    try:
        # Create a basic book using the service
        contributors = []
        if author:
            person = Person(id=str(uuid.uuid4()), name=author)
            contribution = BookContribution(
                person=person,
                contribution_type=ContributionType.AUTHORED,
                order=0
            )
            contributors.append(contribution)
            
        domain_book = DomainBook(
            id=str(uuid.uuid4()),
            title=title,
            contributors=contributors,
            isbn13=isbn if isbn and len(isbn.replace('-', '')) == 13 else None,
            isbn10=isbn if isbn and len(isbn.replace('-', '')) == 10 else None,
            created_at=datetime.now(),
            updated_at=datetime.now()
        )
        
        # Use find_or_create_book to avoid duplicates (global)
        saved_book = book_service.find_or_create_book_sync(domain_book)
        
        # Add book to user's library
        book_service.add_book_to_user_library_sync(
            user_id=current_user.id,
            book_id=saved_book.id,
            reading_status=ReadingStatus.PLAN_TO_READ
        )
        
        return jsonify({
            'message': 'Book logged successfully', 
            'book': {
                'id': saved_book.id,
                'title': saved_book.title,
                'authors': [author.name for author in saved_book.authors] if saved_book.authors else []
            }
        }), 201
    except Exception as e:
        current_app.logger.error(f"Error logging book: {e}")
        return jsonify({'error': 'Failed to log book'}), 500

@bp.route('/health')
def health_check():
    """Health check endpoint for monitoring and testing."""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'bibliotheca'
    }), 200

@bp.route('/reading_history', methods=['GET'])
@login_required
def reading_history():
    # Use service layer with global book visibility
    domain_books = book_service.get_all_books_with_user_overlay_sync(str(current_user.id))
    
    # Convert domain books to dict format for API response
    books_data = []
    for user_book in domain_books:
        book_dict = {
            'id': user_book.get('id') if isinstance(user_book, dict) else getattr(user_book, 'id', ''),
            'uid': user_book.get('uid') if isinstance(user_book, dict) else getattr(user_book, 'uid', ''),
            'title': user_book.get('title') if isinstance(user_book, dict) else getattr(user_book, 'title', ''),
            'author': user_book.get('author') if isinstance(user_book, dict) else getattr(user_book, 'author', ''),
            'isbn': user_book.get('isbn') if isinstance(user_book, dict) else getattr(user_book, 'isbn', ''),
            'description': user_book.get('description') if isinstance(user_book, dict) else getattr(user_book, 'description', ''),
            'start_date': (user_book.get('start_date') if isinstance(user_book, dict) else getattr(user_book, 'start_date', None)).isoformat() if (user_book.get('start_date') if isinstance(user_book, dict) else getattr(user_book, 'start_date', None)) else None,
            'finish_date': (user_book.get('finish_date') if isinstance(user_book, dict) else getattr(user_book, 'finish_date', None)).isoformat() if (user_book.get('finish_date') if isinstance(user_book, dict) else getattr(user_book, 'finish_date', None)) else None,
            'want_to_read': user_book.get('want_to_read') if isinstance(user_book, dict) else getattr(user_book, 'want_to_read', False),
            'library_only': user_book.get('library_only') if isinstance(user_book, dict) else getattr(user_book, 'library_only', False),
            'cover_url': user_book.get('cover_url') if isinstance(user_book, dict) else getattr(user_book, 'cover_url', None),
            'user_rating': user_book.get('user_rating') if isinstance(user_book, dict) else getattr(user_book, 'user_rating', None),
            'personal_notes': user_book.get('personal_notes') if isinstance(user_book, dict) else getattr(user_book, 'personal_notes', None),
            'status': user_book.get('status') if isinstance(user_book, dict) else getattr(user_book, 'status', None),
            'date_added': (user_book.get('date_added') if isinstance(user_book, dict) else getattr(user_book, 'date_added', None)).isoformat() if (user_book.get('date_added') if isinstance(user_book, dict) else getattr(user_book, 'date_added', None)) else None
        }
        books_data.append(book_dict)
    
    return jsonify(books_data), 200

@bp.route('/fetch_book/<isbn>', methods=['GET'])
def fetch_book(isbn):
    book_data = fetch_book_data(isbn) or {}
    
    # Get comprehensive data from Google Books including description
    google_data = get_google_books_cover(isbn, fetch_title_author=True)
    if google_data:
        # Merge Google Books data, prioritizing existing data
        for key, value in google_data.items():
            if key not in book_data or not book_data[key]:
                book_data[key] = value
        # Ensure cover field is set correctly
        if google_data.get('cover'):
            book_data['cover'] = google_data['cover']
    
    # If neither source provides a cover, set a default
    if not book_data.get('cover'):
        book_data['cover'] = url_for('static', filename='bookshelf.png')
    return jsonify(book_data), 200 if book_data else 404

@bp.route('/')
@login_required
def index():
    """Redirect to library page as the main landing page"""
    return redirect(url_for('main.library'))

@bp.route('/add', methods=['GET', 'POST'])
@login_required
def add_book():
    """Add a new book to the library"""
    if request.method == 'GET':
        # Load existing custom fields for the user
        try:
            personal_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global=False)
            global_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global=True)
        except Exception as e:
            current_app.logger.error(f"Error loading custom fields for add book: {e}")
            personal_fields = []
            global_fields = []
        
        return render_template('add_book_new.html', 
                             personal_fields=personal_fields,
                             global_fields=global_fields)
    
    # Handle POST request for adding book
    # This would handle form submission for manual book entry
    flash('Manual book addition is not implemented yet. Please use the Google Books search from the Library page.', 'info')
    return redirect(url_for('main.library'))

@bp.route('/search', methods=['GET', 'POST'])
@login_required
def search_books():
    """Redirect to library page where search functionality is now integrated"""
    flash('Search functionality has been moved to the Library page for a better experience.', 'info')
    return redirect(url_for('main.library'))

@bp.route('/month_wrapup')
@login_required
def month_wrapup():
    """Redirect to stats page where month wrap-up is now integrated"""
    flash('Month Wrap-up has been moved to the Stats page along with other reading statistics.', 'info')
    return redirect(url_for('main.stats'))

@bp.route('/community_activity')
@login_required
def community_activity():
    """Redirect to stats page where community activity is now integrated"""
    flash('Community activity has been moved to the Stats page along with your personal reading statistics.', 'info')
    return redirect(url_for('main.stats'))

@bp.route('/bulk_import', methods=['GET', 'POST'])
@login_required
def bulk_import():
    """Redirect to new import interface."""
    flash('Book import has been upgraded! You can now map CSV fields and track progress in real-time.', 'info')
    return redirect(url_for('main.import_books'))

# Legacy route removed - all book views now use enhanced view directly

@bp.route('/book/<uid>/log', methods=['POST'])
@login_required
def log_reading(uid):
    # Get book through service layer
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
    
    log_date_str = request.form.get('log_date')
    log_date = datetime.strptime(log_date_str, '%Y-%m-%d').date() if log_date_str else date.today()
    
    # Check for existing log using service layer
    book_id = user_book.get('id') if isinstance(user_book, dict) else getattr(user_book, 'id', None)
    if book_id:
        existing_log = reading_log_service.get_existing_log_sync(book_id, current_user.id, log_date)
        if existing_log:
            flash('You have already logged reading for this day.')
        else:
            # Create reading log using service layer
            reading_log_service.create_reading_log_sync(book_id, current_user.id, log_date)
            flash('Reading day logged.')
    else:
        flash('Error: Book ID not found.', 'error')
    return redirect(url_for('main.view_book_enhanced', uid=uid))

@bp.route('/book/<uid>/delete', methods=['POST'])
@login_required
def delete_book(uid):
    # Delete globally through service layer
    success = book_service.delete_book_globally_sync(uid)
    
    if success:
        flash('Book deleted globally for all users.')
    else:
        flash('Failed to delete book.', 'error')
        
    return redirect(url_for('main.library'))

@bp.route('/book/<uid>/toggle_finished', methods=['POST'])
@login_required
def toggle_finished(uid):
    # Get book through service layer
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
    
    finish_date = user_book.get('finish_date') if isinstance(user_book, dict) else getattr(user_book, 'finish_date', None)
    if finish_date:
        # Mark as currently reading
        book_service.update_book_sync(uid, str(current_user.id), finish_date=None)
        flash('Book marked as currently reading.')
    else:
        # Mark as finished
        book_service.update_book_sync(uid, str(current_user.id), finish_date=date.today())
        flash('Book marked as finished.')
    
    return redirect(url_for('main.view_book_enhanced', uid=uid))

@bp.route('/book/<uid>/start_reading', methods=['POST'])
@login_required
def start_reading(uid):
    # Get book through service layer
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
    
    update_data = {'want_to_read': False}
    start_date = user_book.get('start_date') if isinstance(user_book, dict) else getattr(user_book, 'start_date', None)
    if not start_date:
        update_data['start_date'] = datetime.today().date()
    
    book_service.update_book_sync(uid, str(current_user.id), **update_data)
    title = user_book.get('title', 'Unknown Title') if isinstance(user_book, dict) else getattr(user_book, 'title', 'Unknown Title')
    flash(f'Started reading "{title}".')
    return redirect(url_for('main.library'))

@bp.route('/book/<uid>/update_status', methods=['POST'])
@login_required
def update_status(uid):
    # Get book through service layer
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
    
    # Set status based on checkboxes
    want_to_read = 'want_to_read' in request.form
    library_only = 'library_only' in request.form
    finished = 'finished' in request.form
    currently_reading = 'currently_reading' in request.form

    update_data = {
        'want_to_read': want_to_read,
        'library_only': library_only
    }

    if finished:
        update_data.update({
            'finish_date': datetime.now().date(),
            'want_to_read': False,
            'library_only': False
        })
    elif currently_reading:
        update_data.update({
            'finish_date': None,
            'want_to_read': False,
            'library_only': False
        })
        start_date = user_book.get('start_date') if isinstance(user_book, dict) else getattr(user_book, 'start_date', None)
        if not start_date:
            update_data['start_date'] = datetime.now().date()
    elif want_to_read:
        update_data.update({
            'finish_date': None,
            'library_only': False
        })
    elif library_only:
        update_data.update({
            'finish_date': None,
            'want_to_read': False
        })

    book_service.update_book_sync(uid, str(current_user.id), **update_data)
    flash('Book status updated.')
    return redirect(url_for('main.view_book_enhanced', uid=uid))

@bp.route('/library')
@login_required
def library():
    # Get filter parameters from URL - default to "all" to show all books
    status_filter = request.args.get('status_filter', 'all')  # Changed from 'reading' to 'all'
    category_filter = request.args.get('category', '')
    publisher_filter = request.args.get('publisher', '')
    language_filter = request.args.get('language', '')
    location_filter = request.args.get('location', '')
    search_query = request.args.get('search', '')
    sort_option = request.args.get('sort', 'title_asc')  # Default to title A-Z

    # Use service layer with global book visibility
    user_books = book_service.get_all_books_with_user_overlay_sync(str(current_user.id))
    
    # Add location debugging via debug system
    from app.debug_system import debug_log
    debug_log(f"Retrieved {len(user_books)} books for user {current_user.id}", "LIBRARY")
    books_with_locations = 0
    books_without_locations = 0
    location_counts = {}
    
    for book in user_books:
        # Handle both dict and object formats for compatibility
        book_title = book.get('title') if isinstance(book, dict) else getattr(book, 'title', 'Unknown Title')
        book_locations = book.get('locations') if isinstance(book, dict) else getattr(book, 'locations', None)
        
        if book_locations:
            books_with_locations += 1
            for location in book_locations:
                # Extract location ID from location object/dict
                loc_id = location.get('id') if isinstance(location, dict) else getattr(location, 'id', location)
                location_counts[loc_id] = location_counts.get(loc_id, 0) + 1
            debug_log(f"Book '{book_title}' has locations: {book_locations}", "LIBRARY")
        else:
            books_without_locations += 1
            debug_log(f"Book '{book_title}' has NO locations", "LIBRARY")
    
    debug_log(f"Summary: {books_with_locations} books WITH locations, {books_without_locations} books WITHOUT locations", "LIBRARY")
    debug_log(f"Location distribution: {location_counts}", "LIBRARY")
    
    # Calculate statistics for filter buttons - handle both dict and object formats
    def get_reading_status(book):
        if isinstance(book, dict):
            return book.get('ownership', {}).get('reading_status')
        return getattr(book, 'reading_status', None)
    
    def get_ownership_status(book):
        if isinstance(book, dict):
            return book.get('ownership', {}).get('ownership_status')
        return getattr(book, 'ownership_status', None)
    
    stats = {
        'total_books': len(user_books),
        'books_read': len([b for b in user_books if get_reading_status(b) == 'read']),
        'currently_reading': len([b for b in user_books if get_reading_status(b) == 'reading']),
        'want_to_read': len([b for b in user_books if get_reading_status(b) == 'plan_to_read']),
        'on_hold': len([b for b in user_books if get_reading_status(b) == 'on_hold']),
        'wishlist': len([b for b in user_books if get_ownership_status(b) == 'wishlist']),
        # Add location stats
        'books_with_locations': books_with_locations,
        'books_without_locations': books_without_locations,
        'location_counts': location_counts
    }
    
    # Apply status filter first
    filtered_books = user_books
    if status_filter and status_filter != 'all':
        if status_filter == 'wishlist':
            filtered_books = [book for book in filtered_books if get_ownership_status(book) == 'wishlist']
        else:
            filtered_books = [book for book in filtered_books if get_reading_status(book) == status_filter]
    
    # Apply other filters
    if search_query:
        search_lower = search_query.lower()
        filtered_books = [
            book for book in filtered_books 
            if (search_lower in (book.get('title', '') if isinstance(book, dict) else getattr(book, 'title', '')).lower()) or
               (search_lower in (book.get('author', '') if isinstance(book, dict) else getattr(book, 'author', '')).lower()) or
               (search_lower in (book.get('description', '') if isinstance(book, dict) else getattr(book, 'description', '')).lower())
        ]
    
    if publisher_filter:
        filtered_books = [
            book for book in filtered_books 
            if (book.get('publisher') if isinstance(book, dict) else getattr(book, 'publisher', None)) and 
               publisher_filter.lower() in (book.get('publisher', '') if isinstance(book, dict) else getattr(book, 'publisher', '')).lower()
        ]
    
    if language_filter:
        filtered_books = [
            book for book in filtered_books 
            if (book.get('language') if isinstance(book, dict) else getattr(book, 'language', None)) == language_filter
        ]
    
    if location_filter:
        # Handle locations which are now returned as strings (location names) from KuzuIntegrationService
        filtered_books = [
            book for book in filtered_books 
            if (book.get('locations') if isinstance(book, dict) else getattr(book, 'locations', None)) and any(
                location_filter.lower() in (loc.lower() if isinstance(loc, str) else 
                                           (loc.get('name', '') if isinstance(loc, dict) else getattr(loc, 'name', '')).lower())
                for loc in (book.get('locations', []) if isinstance(book, dict) else getattr(book, 'locations', []))
            )
        ]
    
    if category_filter:
        filtered_books = [
            book for book in filtered_books 
            if (book.get('categories') if isinstance(book, dict) else getattr(book, 'categories', None)) and any(
                category_filter.lower() in (cat.get('name', '') if isinstance(cat, dict) else getattr(cat, 'name', '')).lower() 
                for cat in (book.get('categories', []) if isinstance(book, dict) else getattr(book, 'categories', []))
            )
        ]

    # Apply sorting
    def get_author_name(book):
        """Helper function to get author name safely"""
        if isinstance(book, dict):
            authors = book.get('authors', [])
            author = book.get('author', '')
            if authors and isinstance(authors, list) and len(authors) > 0:
                first_author = authors[0]
                if isinstance(first_author, dict):
                    return first_author.get('name', 'Unknown Author')
                elif hasattr(first_author, 'name'):
                    return first_author.name
                else:
                    return str(first_author)
            elif author:
                return author
            return "Unknown Author"
        else:
            # Handle object format
            if hasattr(book, 'authors') and book.authors:
                # Handle list of Author objects
                if isinstance(book.authors, list) and len(book.authors) > 0:
                    author = book.authors[0]
                    if hasattr(author, 'name'):
                        return author.name
                    elif hasattr(author, 'first_name') and hasattr(author, 'last_name'):
                        return f"{author.first_name} {author.last_name}".strip()
                    else:
                        return str(author)
                else:
                    return str(book.authors)
            elif hasattr(book, 'author') and book.author:
                return book.author
            return "Unknown Author"
    
    def get_author_last_first(book):
        """Helper function to get author name in Last, First format"""
        author_name = get_author_name(book)
        if ',' in author_name:
            return author_name  # Already in Last, First format
        name_parts = author_name.split()
        if len(name_parts) >= 2:
            last_name = name_parts[-1]
            first_names = ' '.join(name_parts[:-1])
            return f"{last_name}, {first_names}"
        return author_name
    
    if sort_option == 'title_asc':
        filtered_books.sort(key=lambda x: (x.get('title', '') if isinstance(x, dict) else getattr(x, 'title', '')).lower())
    elif sort_option == 'title_desc':
        filtered_books.sort(key=lambda x: (x.get('title', '') if isinstance(x, dict) else getattr(x, 'title', '')).lower(), reverse=True)
    elif sort_option == 'author_first_asc':
        filtered_books.sort(key=lambda x: get_author_name(x).lower())
    elif sort_option == 'author_first_desc':
        filtered_books.sort(key=lambda x: get_author_name(x).lower(), reverse=True)
    elif sort_option == 'author_last_asc':
        filtered_books.sort(key=lambda x: get_author_last_first(x).lower())
    elif sort_option == 'author_last_desc':
        filtered_books.sort(key=lambda x: get_author_last_first(x).lower(), reverse=True)
    else:
        # Default to title A-Z
        filtered_books.sort(key=lambda x: (x.get('title', '') if isinstance(x, dict) else getattr(x, 'title', '')).lower())

    # Books are already in the right format for the template
    books = filtered_books

    # Convert dictionary books to object-like structures for template compatibility
    converted_books = []
    for book in books:
        if isinstance(book, dict):
            # Create an object-like structure that the template can work with
            class BookObj:
                def __init__(self, data):
                    for key, value in data.items():
                        setattr(self, key, value)
                    # Ensure common attributes have defaults
                    if not hasattr(self, 'authors'):
                        self.authors = []
                    if not hasattr(self, 'contributors'):
                        self.contributors = []
                    if not hasattr(self, 'categories'):
                        self.categories = []
                    if not hasattr(self, 'publisher'):
                        self.publisher = None
                    if not hasattr(self, 'series'):
                        self.series = None
                    if not hasattr(self, 'locations'):
                        self.locations = []
                    # Handle ownership data
                    ownership = data.get('ownership', {})
                    for key, value in ownership.items():
                        setattr(self, key, value)
                
                def get_contributors_by_type(self, contribution_type):
                    """Get contributors by type for template compatibility."""
                    if hasattr(self, 'contributors') and self.contributors:
                        return [c for c in self.contributors if getattr(c, 'contribution_type', None) == contribution_type]
                    return []
            
            converted_books.append(BookObj(book))
        else:
            converted_books.append(book)
    
    books = converted_books

    # Get distinct values for filter dropdowns (from all books, not filtered)
    all_books = user_books
    
    categories = set()
    publishers = set()
    languages = set()
    locations = set()

    for book in all_books:
        # Handle categories
        book_categories = book.get('categories', []) if isinstance(book, dict) else getattr(book, 'categories', [])
        if book_categories:
            # book.categories is a list of Category objects, not a string
            for cat in book_categories:
                if isinstance(cat, dict):
                    categories.add(cat.get('name', ''))
                elif hasattr(cat, 'name'):
                    categories.add(cat.name)
                else:
                    categories.add(str(cat))
        
        # Handle publisher
        book_publisher = book.get('publisher') if isinstance(book, dict) else getattr(book, 'publisher', None)
        if book_publisher:
            # Handle Publisher domain object or string
            if isinstance(book_publisher, dict):
                publisher_name = book_publisher.get('name', str(book_publisher))
            elif hasattr(book_publisher, 'name'):
                publisher_name = book_publisher.name
            else:
                publisher_name = str(book_publisher)
            publishers.add(publisher_name)
        
        # Handle language
        book_language = book.get('language') if isinstance(book, dict) else getattr(book, 'language', None)
        if book_language:
            languages.add(book_language)
        
        # Handle locations - they are now returned as strings (location names) from KuzuIntegrationService
        book_locations = book.get('locations', []) if isinstance(book, dict) else getattr(book, 'locations', [])
        if book_locations:
            for loc in book_locations:
                if isinstance(loc, str):
                    # Location is already a string (location name)
                    locations.add(loc)
                elif isinstance(loc, dict):
                    locations.add(loc.get('name', ''))
                elif hasattr(loc, 'name'):
                    locations.add(loc.name)
                else:
                    locations.add(str(loc))

    # Get users through Kuzu service layer
    domain_users = user_service.get_all_users_sync()
    
    # Convert domain users to simple objects for template compatibility
    users = []
    for domain_user in domain_users:
        user_data = {
            'id': domain_user.id,
            'username': domain_user.username,
            'email': domain_user.email
        }
        users.append(type('User', (), user_data))

    return render_template(
        'library_enhanced.html',
        books=books,
        stats=stats,
        categories=sorted(categories),
        publishers=sorted(publishers),
        languages=sorted(languages),
        locations=sorted(locations),
        current_status_filter=status_filter,
        current_category=category_filter,
        current_publisher=publisher_filter,
        current_language=language_filter,
        current_location=location_filter,
        current_search=search_query,
        current_sort=sort_option,
        users=users
    )

@bp.route('/public-library')
def public_library():
    filter_status = request.args.get('filter', 'all')
    
    # Use Kuzu service to get all books from all users
    # TODO: Implement public library functionality in Kuzu service
    # For now, return empty list
    books = []
    
    return render_template('public_library.html', books=books, filter_status=filter_status)

@bp.route('/book/<uid>/edit', methods=['GET', 'POST'])
@login_required
def edit_book(uid):
    # Get book through service layer
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
        
    if request.method == 'POST':
        new_isbn13 = request.form.get('isbn13', '').strip() or None
        new_isbn10 = request.form.get('isbn10', '').strip() or None
        
        # Process contributors
        contributors = []
        contributor_data = {}
        
        # Parse contributor form data
        for key, value in request.form.items():
            if key.startswith('contributors[') and '][' in key:
                # Extract index and field from key like "contributors[0][name]"
                parts = key.split('][')
                if len(parts) == 2:
                    index_part = parts[0].replace('contributors[', '')
                    field = parts[1].replace(']', '')
                    
                    if index_part not in contributor_data:
                        contributor_data[index_part] = {}
                    contributor_data[index_part][field] = value
        
        # Process categories
        categories = []
        category_data = {}
        
        # Parse category form data
        for key, value in request.form.items():
            if key.startswith('categories[') and '][' in key:
                # Extract index and field from key like "categories[0][name]"
                parts = key.split('][')
                if len(parts) == 2:
                    index_part = parts[0].replace('categories[', '')
                    field = parts[1].replace(']', '')
                    
                    if index_part not in category_data:
                        category_data[index_part] = {}
                    category_data[index_part][field] = value
        
        # Create category list for processing
        for cat_data in category_data.values():
            if cat_data.get('name'):
                categories.append(cat_data['name'])  # Just pass the name, service will handle creation/linking
        
        # Create BookContribution objects
        for contrib in contributor_data.values():
            if contrib.get('name'):
                from .domain.models import Person, BookContribution, ContributionType
                
                person_name = contrib['name']
                
                try:
                    # Always use find_or_create approach - the most reliable method
                    from app.debug_system import debug_log
                    debug_log(f"Finding or creating person: {person_name}", "PERSON_CREATION")
                    
                    # Use the same logic as the repository's find_or_create_person method
                    from .infrastructure.kuzu_graph import get_graph_storage
                    storage = get_graph_storage()
                    
                    # Search for existing person by name (same as repository method)
                    normalized_name = person_name.strip().lower()
                    all_persons = storage.find_nodes_by_type('person')
                    person = None
                    
                    for person_data in all_persons:
                        existing_name = person_data.get('name', '').strip().lower()
                        existing_normalized = person_data.get('normalized_name', '').strip().lower()
                        
                        if existing_name == normalized_name or existing_normalized == normalized_name:
                            debug_log(f"Found existing person: {person_data.get('name')} (ID: {person_data.get('id')})", "PERSON_CREATION")
                            # Convert back to Person object
                            person = Person(
                                id=person_data.get('id'),
                                name=person_data.get('name', ''),
                                normalized_name=person_data.get('normalized_name', ''),
                                birth_year=person_data.get('birth_year'),
                                death_year=person_data.get('death_year'),
                                birth_place=person_data.get('birth_place'),
                                bio=person_data.get('bio'),
                                website=person_data.get('website'),
                                created_at=datetime.now(),  # Set defaults for dates
                                updated_at=datetime.now()
                            )
                            break
                    
                    # If not found, create new person using clean repository (with auto-fetch)
                    if not person:
                        debug_log(f"Creating new person via clean repository: {person_name}", "PERSON_CREATION")
                        from .infrastructure.kuzu_clean_repositories import CleanKuzuPersonRepository
                        from .infrastructure.kuzu_graph import get_kuzu_connection
                        
                        try:
                            # Use clean repository with auto-fetch capability
                            kuzu_connection = get_kuzu_connection()
                            person_repo = CleanKuzuPersonRepository(kuzu_connection.connect())
                            
                            # Create person using repository (will auto-fetch OpenLibrary metadata)
                            person_dict = {
                                'id': str(uuid.uuid4()),
                                'name': person_name,
                                'normalized_name': normalized_name,
                                'created_at': datetime.now(),
                                'updated_at': datetime.now()
                            }
                            
                            created_person = person_repo.create(person_dict)
                            if created_person:
                                # Convert back to Person object for the rest of the workflow
                                person = Person(
                                    id=created_person.get('id'),
                                    name=created_person.get('name', ''),
                                    normalized_name=created_person.get('normalized_name', ''),
                                    birth_year=created_person.get('birth_year'),
                                    death_year=created_person.get('death_year'),
                                    birth_place=created_person.get('birth_place'),
                                    bio=created_person.get('bio'),
                                    website=created_person.get('website'),
                                    openlibrary_id=created_person.get('openlibrary_id'),
                                    image_url=created_person.get('image_url'),
                                    created_at=datetime.now(),
                                    updated_at=datetime.now()
                                )
                                debug_log(f"Created new person with auto-fetch: {person.name} (ID: {person.id})", "PERSON_CREATION")
                            else:
                                debug_log(f"Failed to create person via repository: {person_name}", "PERSON_CREATION_ERROR")
                                continue
                        except Exception as repo_error:
                            debug_log(f"Repository creation failed for {person_name}: {repo_error}", "PERSON_CREATION_ERROR")
                            continue
                    
                    # Validate person has valid ID
                    if not person or not person.id:
                        debug_log(f"Person has invalid ID: {person}", "PERSON_CREATION_ERROR")
                        continue
                    
                except Exception as e:
                    print(f"❌ [DEBUG] Error processing person {person_name}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
                
                # Map contribution type
                contrib_type_map = {
                    'authored': ContributionType.AUTHORED,
                    'narrated': ContributionType.NARRATED,
                    'edited': ContributionType.EDITED,
                    'contributed': ContributionType.CONTRIBUTED
                }
                
                contrib_type = contrib_type_map.get(contrib.get('type', 'authored'), ContributionType.AUTHORED)
                
                contribution = BookContribution(
                    person=person,
                    contribution_type=contrib_type,
                    created_at=datetime.now()
                )
                contributors.append(contribution)
        
        update_data = {
            'title': request.form['title'],
            'subtitle': request.form.get('subtitle', '').strip() or None,
            'description': request.form.get('description', '').strip() or None,
            'published_date': _convert_published_date_to_date(request.form.get('published_date', '').strip()) if request.form.get('published_date', '').strip() else None,
            'page_count': int(request.form.get('page_count')) if request.form.get('page_count', '').strip() else None,
            'language': request.form.get('language', '').strip() or 'en',
            'cover_url': request.form.get('cover_url', '').strip() or None,
            'isbn13': new_isbn13,
            'isbn10': new_isbn10,
            'series': request.form.get('series', '').strip() or None,
            'series_volume': request.form.get('series_volume', '').strip() or None,
            'series_order': int(request.form.get('series_order')) if request.form.get('series_order', '').strip() else None,
            'contributors': contributors,
            'raw_categories': ','.join(categories) if categories else None,
            # Additional metadata fields - these are Book properties, not user-specific
            'publisher': request.form.get('publisher', '').strip() or None,
            'asin': request.form.get('asin', '').strip() or None,
            'google_books_id': request.form.get('google_books_id', '').strip() or None,
            'openlibrary_id': request.form.get('openlibrary_id', '').strip() or None,
            'average_rating': float(request.form.get('average_rating')) if request.form.get('average_rating', '').strip() else None,
            'rating_count': int(request.form.get('rating_count')) if request.form.get('rating_count', '').strip() else None,
        }
        
        # Remove None values except for specific fields that can be null
        filtered_data = {}
        for k, v in update_data.items():
            if k in ['contributors', 'raw_categories', 'series', 'series_volume', 'series_order', 'publisher', 'asin', 'google_books_id', 'openlibrary_id', 'average_rating', 'rating_count'] or v is not None:
                filtered_data[k] = v
        
        success = book_service.update_book_sync(uid, str(current_user.id), **filtered_data)
        if success:
            flash('Book updated successfully.', 'success')
        else:
            flash('Failed to update book.', 'error')
        return redirect(url_for('main.view_book_enhanced', uid=uid))
        
    # Get book categories for editing
    book_categories = []
    try:
        book_id = user_book.get('id') if isinstance(user_book, dict) else getattr(user_book, 'id', None)
        if book_id:
            book_categories = book_service.get_book_categories_sync(book_id)
    except Exception as e:
        print(f"❌ [EDIT] Error loading book categories: {e}")
    
    # Convert dictionary to object-like structure for template compatibility
    if isinstance(user_book, dict):
        class BookObj:
            def __init__(self, data):
                for key, value in data.items():
                    setattr(self, key, value)
                # Ensure common attributes have defaults
                if not hasattr(self, 'authors'):
                    self.authors = []
                if not hasattr(self, 'contributors'):
                    self.contributors = []
                if not hasattr(self, 'categories'):
                    self.categories = []
                if not hasattr(self, 'publisher'):
                    self.publisher = None
                # Handle ownership data
                ownership = data.get('ownership', {})
                for key, value in ownership.items():
                    setattr(self, key, value)
            
            def get_contributors_by_type(self, contribution_type):
                """Get contributors filtered by type."""
                if not hasattr(self, 'contributors') or not self.contributors:
                    return []
                
                # Handle both string and enum contribution types
                type_str = contribution_type.upper() if isinstance(contribution_type, str) else str(contribution_type).upper()
                
                filtered = []
                for contributor in self.contributors:
                    if isinstance(contributor, dict):
                        role = contributor.get('role', '').upper()
                        if role == type_str or role == type_str.replace('ED', ''):  # Handle past tense
                            filtered.append(contributor)
                    else:
                        # Handle object-like contributors
                        role = getattr(contributor, 'role', '').upper()
                        if role == type_str or role == type_str.replace('ED', ''):
                            filtered.append(contributor)
                
                return filtered
        
        user_book = BookObj(user_book)
    
    return render_template('edit_book_enhanced.html', book=user_book, book_categories=book_categories)


@bp.route('/book/<uid>/enhanced')
@login_required
def view_book_enhanced(uid):
    """Enhanced book view with new status system."""
    from app.debug_system import debug_log, debug_book_details, debug_service_call, debug_template_data
    
    debug_log(f"🔍 [VIEW] Starting enhanced book view for UID: {uid}, User: {current_user.id}", "BOOK_VIEW")
    
    # Service call debugging
    debug_service_call("book_service", "get_book_by_uid_sync", {"uid": uid, "user_id": str(current_user.id)}, None, "BEFORE")
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    debug_service_call("book_service", "get_book_by_uid_sync", {"uid": uid, "user_id": str(current_user.id)}, user_book, "AFTER")
    
    if not user_book:
        debug_log(f"❌ [VIEW] Book {uid} not found for user {current_user.id}", "BOOK_VIEW")
        abort(404)

    # Enhanced book debugging
    debug_book_details(user_book, uid, str(current_user.id), "VIEW")

    title = user_book.get('title', 'Unknown Title') if isinstance(user_book, dict) else getattr(user_book, 'title', 'Unknown Title')
    debug_log(f"✅ [VIEW] Found book: {title}", "BOOK_VIEW")

    # Convert dictionary to object-like structure for template compatibility
    if isinstance(user_book, dict):
        debug_log(f"🔄 [VIEW] Converting dict to object for template compatibility", "BOOK_VIEW")
        # Create an object-like structure that the template can work with
        class BookObj:
            def __init__(self, data):
                for key, value in data.items():
                    setattr(self, key, value)
                # Ensure common attributes have defaults
                if not hasattr(self, 'authors'):
                    self.authors = []
                if not hasattr(self, 'contributors'):
                    self.contributors = []
                if not hasattr(self, 'categories'):
                    self.categories = []
                if not hasattr(self, 'publisher'):
                    self.publisher = None
                if not hasattr(self, 'series'):
                    self.series = None
                if not hasattr(self, 'custom_metadata'):
                    self.custom_metadata = {}
                # DEBUG: Log all cover-related fields
                cover_url_value = getattr(self, 'cover_url', None)
                debug_log(f"🖼️ [VIEW] Cover URL field - cover_url: {cover_url_value}", "BOOK_VIEW")
                
                # Cover URL field is already correct (cover_url is the database field)
                # No field mapping needed since we're using the correct field name everywhere
                
                # DEBUG: Log all ISBN-related fields
                isbn13_value = getattr(self, 'isbn13', None)
                isbn10_value = getattr(self, 'isbn10', None)
                isbn_value = getattr(self, 'isbn', None)
                debug_log(f"📚 [VIEW] ISBN fields - isbn13: {isbn13_value}, isbn10: {isbn10_value}, isbn: {isbn_value}", "BOOK_VIEW")
                
                # Handle ISBN field consistency - Template expects isbn13/isbn10 specifically
                if not hasattr(self, 'isbn') and (hasattr(self, 'isbn13') or hasattr(self, 'isbn10')):
                    self.isbn = getattr(self, 'isbn13', None) or getattr(self, 'isbn10', None)
                    debug_log(f"📚 [VIEW] Mapped primary ISBN: {self.isbn}", "BOOK_VIEW")
                
                # REVERSE NORMALIZATION: If we have 'isbn' but not isbn13/isbn10, normalize it
                isbn_value = getattr(self, 'isbn', None)
                if isbn_value and (not hasattr(self, 'isbn13') or not getattr(self, 'isbn13')):
                    import re
                    # Clean and normalize the ISBN
                    clean_isbn = re.sub(r'[^0-9X]', '', str(isbn_value).upper())
                    if len(clean_isbn) == 13:
                        self.isbn13 = clean_isbn
                        debug_log(f"📚 [VIEW] Normalized generic ISBN to isbn13: {clean_isbn}", "BOOK_VIEW")
                    elif len(clean_isbn) == 10:
                        self.isbn10 = clean_isbn
                        debug_log(f"📚 [VIEW] Normalized generic ISBN to isbn10: {clean_isbn}", "BOOK_VIEW")
                
                # Ensure both fields exist for template (even if None)
                if not hasattr(self, 'isbn13'):
                    self.isbn13 = None
                if not hasattr(self, 'isbn10'):
                    self.isbn10 = None
                
                # DEBUG: Log all category-related fields
                categories_value = getattr(self, 'categories', None)
                genre_value = getattr(self, 'genre', None)
                genres_value = getattr(self, 'genres', None)
                debug_log(f"🏷️ [VIEW] Category fields - categories: {categories_value}, genre: {genre_value}, genres: {genres_value}", "BOOK_VIEW")
                
                # Handle category field consistency
                if not hasattr(self, 'categories') or not self.categories:
                    if hasattr(self, 'genres') and self.genres:
                        self.categories = self.genres
                        debug_log(f"🏷️ [VIEW] Mapped genres to categories: {self.categories}", "BOOK_VIEW")
                    elif hasattr(self, 'genre') and self.genre:
                        self.categories = [self.genre] if isinstance(self.genre, str) else self.genre
                        debug_log(f"🏷️ [VIEW] Mapped genre to categories: {self.categories}", "BOOK_VIEW")
                
                # Final debug
                final_cover_url = getattr(self, 'cover_url', None)
                final_cover_url_alt = getattr(self, 'cover_url', None)
                debug_log(f"🖼️ [VIEW] Final cover URLs - cover_url: {final_cover_url}, cover_url_alt: {final_cover_url_alt}", "BOOK_VIEW")
                
                # Handle location field - if location_id exists but locations is empty, get location name
                if hasattr(self, 'location_id') and self.location_id and (not hasattr(self, 'locations') or not self.locations):
                    try:
                        from app.location_service import LocationService
                        from app.infrastructure.kuzu_graph import get_kuzu_connection
                        
                        kuzu_connection = get_kuzu_connection()
                        location_service = LocationService(kuzu_connection.connect())
                        
                        # Get current user ID properly
                        user_id_for_location = str(current_user.id) if 'current_user' in globals() and hasattr(current_user, 'id') else str(data.get('user_id', ''))
                        if user_id_for_location:
                            user_locations = location_service.get_user_locations(user_id_for_location)
                            
                            # Find the location object by ID
                            for user_loc in user_locations:
                                if hasattr(user_loc, 'id') and str(user_loc.id) == str(self.location_id):
                                    self.locations = [{'id': user_loc.id, 'name': user_loc.name}]
                                    break
                            
                            # If location not found by ID, check if location_id is actually a name
                            if not hasattr(self, 'locations') or not self.locations:
                                for user_loc in user_locations:
                                    if hasattr(user_loc, 'name') and str(user_loc.name) == str(self.location_id):
                                        self.locations = [{'id': user_loc.id, 'name': user_loc.name}]
                                        break
                                        
                    except Exception as e:
                        print(f"Error populating location data: {e}")
                        # Fallback: treat location_id as the name
                        if self.location_id:
                            self.locations = [{'id': self.location_id, 'name': self.location_id}]
                        # Fallback: treat location_id as the name
                        if self.location_id:
                            self.locations = [{'id': self.location_id, 'name': self.location_id}]
                
                # Handle ownership data
                ownership = data.get('ownership', {})
                for key, value in ownership.items():
                    setattr(self, key, value)
            
            def get_contributors_by_type(self, contribution_type):
                """Get contributors filtered by type."""
                if not hasattr(self, 'contributors') or not self.contributors:
                    return []
                
                # Handle both string and enum contribution types
                type_str = contribution_type.upper() if isinstance(contribution_type, str) else str(contribution_type).upper()
                
                filtered = []
                for contributor in self.contributors:
                    if isinstance(contributor, dict):
                        role = contributor.get('role', '').upper()
                        if role == type_str or role == type_str.replace('ED', ''):  # Handle past tense
                            filtered.append(contributor)
                    else:
                        # Handle object-like contributors
                        role = getattr(contributor, 'role', '').upper()
                        if role == type_str or role == type_str.replace('ED', ''):
                            filtered.append(contributor)
                
                return filtered
        
        user_book = BookObj(user_book)
        debug_log(f"✅ [VIEW] Converted dictionary to object for template compatibility", "BOOK_VIEW")
        
        # Debug ISBN fields specifically and ensure final normalization
        isbn13 = getattr(user_book, 'isbn13', None)
        isbn10 = getattr(user_book, 'isbn10', None) 
        isbn_generic = getattr(user_book, 'isbn', None)
        cover_url = getattr(user_book, 'cover_url', None)
        
        debug_log(f"🔍 [VIEW] ISBN Debug - ISBN13: {isbn13}, ISBN10: {isbn10}, Generic ISBN: {isbn_generic}", "BOOK_VIEW")
        debug_log(f"🔍 [VIEW] Cover Debug - Cover URL: {cover_url}", "BOOK_VIEW")
        debug_log(f"🔍 [VIEW] Available book attributes: {[attr for attr in dir(user_book) if not attr.startswith('_')]}", "BOOK_VIEW")
        
        # FINAL NORMALIZATION: Ensure ISBN fields are available for template
        if not isbn13 and not isbn10 and isbn_generic:
            import re
            clean_isbn = re.sub(r'[^0-9X]', '', str(isbn_generic).upper())
            if len(clean_isbn) == 13:
                user_book.isbn13 = clean_isbn
                debug_log(f"📚 [VIEW] Final: Normalized generic ISBN to isbn13: {clean_isbn}", "BOOK_VIEW")
            elif len(clean_isbn) == 10:
                user_book.isbn10 = clean_isbn
                debug_log(f"📚 [VIEW] Final: Normalized generic ISBN to isbn10: {clean_isbn}", "BOOK_VIEW")
        
        # Ensure both fields exist (template expects them)
        if not hasattr(user_book, 'isbn13'):
            user_book.isbn13 = isbn13
        if not hasattr(user_book, 'isbn10'):
            user_book.isbn10 = isbn10
        if not hasattr(user_book, 'isbn'):
            user_book.isbn = isbn13 or isbn10  # Fallback for any other template expectations

    # Get book categories
    book_categories = []
    try:
        book_id = getattr(user_book, 'id', None)
        debug_log(f"🔍 [VIEW] Getting categories for book ID: {book_id}", "BOOK_VIEW")
        if book_id:
            debug_service_call("book_service", "get_book_categories_sync", {"book_id": book_id}, None, "BEFORE")
            book_categories = book_service.get_book_categories_sync(book_id)
            debug_service_call("book_service", "get_book_categories_sync", {"book_id": book_id}, book_categories, "AFTER")
            debug_log(f"✅ [VIEW] Found {len(book_categories)} categories", "BOOK_VIEW")
        else:
            debug_log(f"❌ [VIEW] No book ID found for category lookup", "BOOK_VIEW")
    except Exception as e:
        current_app.logger.error(f"Error loading book categories: {e}")
        debug_log(f"❌ [VIEW] Error loading book categories: {e}", "BOOK_VIEW")

    # Get custom metadata for display
    global_metadata_display = []
    personal_metadata_display = []
    
    try:
        debug_log(f"🔍 [VIEW] Processing custom metadata", "BOOK_VIEW")
        # Global metadata is stored on the book itself, but we don't have a separate method to get just the book
        # For now, assume no global metadata since we're storing everything on relationships
        # TODO: Implement proper global vs personal metadata separation
        
        custom_metadata = getattr(user_book, 'custom_metadata', None)
        if custom_metadata:
            debug_log(f"✅ [VIEW] Personal metadata found: {custom_metadata}", "BOOK_VIEW")
            personal_metadata_display = custom_field_service.get_custom_metadata_for_display(
                custom_metadata
            )
            debug_log(f"✅ [VIEW] Converted to {len(personal_metadata_display)} display items", "BOOK_VIEW")
        else:
            debug_log(f"ℹ️ [VIEW] No personal metadata found", "BOOK_VIEW")
    except Exception as e:
        current_app.logger.error(f"Error loading custom metadata for display: {e}")
        debug_log(f"❌ [VIEW] Error loading custom metadata for display: {e}", "BOOK_VIEW")
    
    # Get user locations for the location dropdown and debug info
    user_locations = []
    try:
        debug_log(f"🔍 [VIEW] Getting user locations", "BOOK_VIEW")
        from app.location_service import LocationService
        from app.infrastructure.kuzu_graph import get_kuzu_connection
        from config import Config
        
        kuzu_connection = get_kuzu_connection()
        location_service = LocationService(kuzu_connection.connect())
        user_locations = location_service.get_user_locations(str(current_user.id))
        debug_log(f"✅ [VIEW] Found {len(user_locations)} locations for user {current_user.id}", "BOOK_VIEW")
    except Exception as e:
        current_app.logger.error(f"Error loading user locations: {e}")
        debug_log(f"❌ [VIEW] Error loading user locations: {e}", "BOOK_VIEW")
    
    # Prepare template data
    template_data = {
        'book': user_book,
        'book_categories': book_categories,
        'global_metadata_display': global_metadata_display,
        'personal_metadata_display': personal_metadata_display,
        'user_locations': user_locations
    }
    
    debug_template_data('view_book_enhanced.html', template_data, "VIEW")
    debug_log(f"🎨 [VIEW] Rendering template with {len(global_metadata_display)} global and {len(personal_metadata_display)} personal metadata items", "BOOK_VIEW")
    
    return render_template(
        'view_book_enhanced.html', 
        book=user_book,
        book_categories=book_categories,
        global_metadata_display=global_metadata_display,
        personal_metadata_display=personal_metadata_display,
        user_locations=user_locations
    )


@bp.route('/book/<uid>/update_details', methods=['POST'])
@login_required
def update_book_details(uid):
    """Update book details including new status system."""
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
    
    update_data = {}
    
    # Reading status
    reading_status = request.form.get('reading_status')
    if reading_status:
        update_data['reading_status'] = reading_status
    
    # Ownership status
    ownership_status = request.form.get('ownership_status')
    if ownership_status:
        update_data['ownership_status'] = ownership_status
    
    # Media type
    media_type = request.form.get('media_type')
    if media_type:
        update_data['media_type'] = media_type
    
    # Location
    location_id = request.form.get('location_id')
    if location_id:
        update_data['location_id'] = location_id
    
    # Borrowing details
    borrowed_from = request.form.get('borrowed_from', '').strip()
    borrowed_due_date = request.form.get('borrowed_due_date')
    if ownership_status == 'borrowed':
        update_data['borrowed_from'] = borrowed_from if borrowed_from else None
        if borrowed_due_date:
            update_data['borrowed_due_date'] = datetime.strptime(borrowed_due_date, '%Y-%m-%d').date()
    
    # Loaning details
    loaned_to = request.form.get('loaned_to', '').strip()
    loaned_due_date = request.form.get('loaned_due_date')
    if ownership_status == 'loaned':
        update_data['loaned_to'] = loaned_to if loaned_to else None
        if loaned_due_date:
            update_data['loaned_due_date'] = datetime.strptime(loaned_due_date, '%Y-%m-%d').date()
    
    # Update reading dates based on status
    start_date = user_book.get('start_date') if isinstance(user_book, dict) else getattr(user_book, 'start_date', None)
    finish_date = user_book.get('finish_date') if isinstance(user_book, dict) else getattr(user_book, 'finish_date', None)
    
    if reading_status == 'reading' and not start_date:
        update_data['start_date'] = date.today()
    elif reading_status == 'read' and not finish_date:
        update_data['finish_date'] = date.today()
        if not start_date:
            update_data['start_date'] = date.today()
    
    # Use service layer to update
    try:
        updated_book = book_service.update_book_sync(uid, str(current_user.id), **update_data)
        if updated_book is not None:
            flash('Book details updated successfully.', 'success')
            
            # Redirect back to library with appropriate filter if requested
            redirect_to_library = request.form.get('redirect_to_library')
            if redirect_to_library:
                # Determine the appropriate filter based on the new reading status
                if reading_status:
                    if reading_status == 'wishlist':
                        return redirect(url_for('main.library', status_filter='wishlist'))
                    else:
                        return redirect(url_for('main.library', status_filter=reading_status))
                elif ownership_status == 'wishlist':
                    return redirect(url_for('main.library', status_filter='wishlist'))
                else:
                    return redirect(url_for('main.library'))
        else:
            flash('Failed to update book details.', 'error')
    except Exception as e:
        flash(f'Error updating book: {str(e)}', 'error')
    
    return redirect(url_for('main.view_book_enhanced', uid=uid))


@bp.route('/book/<uid>/update_reading_dates', methods=['POST'])
@login_required
def update_reading_dates(uid):
    """Update reading start and finish dates."""
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
    
    update_data = {}
    
    start_date_str = request.form.get('start_date', '').strip()
    finish_date_str = request.form.get('finish_date', '').strip()
    
    if start_date_str:
        update_data['start_date'] = datetime.strptime(start_date_str, '%Y-%m-%d').date()
    
    if finish_date_str:
        update_data['finish_date'] = datetime.strptime(finish_date_str, '%Y-%m-%d').date()
    
    try:
        success = book_service.update_book_sync(uid, str(current_user.id), **update_data)
        if success:
            flash('Reading dates updated successfully.', 'success')
        else:
            flash('Failed to update reading dates.', 'error')
    except Exception as e:
        flash(f'Error updating dates: {str(e)}', 'error')
    
    return redirect(url_for('main.view_book_enhanced', uid=uid))


@bp.route('/book/<uid>/update_notes', methods=['POST'])
@login_required
def update_book_notes(uid):
    """Update personal notes and rating."""
    user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
    if not user_book:
        abort(404)
    
    update_data = {}
    
    personal_notes = request.form.get('personal_notes', '').strip()
    user_rating = request.form.get('user_rating', '').strip()
    
    update_data['personal_notes'] = personal_notes if personal_notes else None
    
    if user_rating:
        try:
            update_data['user_rating'] = float(user_rating)
        except ValueError:
            flash('Invalid rating value.', 'error')
            return redirect(url_for('main.view_book_enhanced', uid=uid))
    
    try:
        success = book_service.update_book_sync(uid, str(current_user.id), **update_data)
        if success:
            flash('Notes and rating updated successfully.', 'success')
        else:
            flash('Failed to update notes and rating.', 'error')
    except Exception as e:
        flash(f'Error updating notes: {str(e)}', 'error')
    
    return redirect(url_for('main.view_book_enhanced', uid=uid))


@bp.route('/book/<uid>/custom_metadata', methods=['GET', 'POST'])
@login_required
def edit_book_custom_metadata(uid):
    """Edit custom metadata for a book."""
    from app.debug_system import debug_log, debug_metadata_operation, debug_service_call, debug_template_data
    
    try:
        debug_log(f"🔍 [EDIT_META] Starting custom metadata edit for book {uid}, user {current_user.id}", "METADATA")
        
        # Get user book with relationship data (includes custom metadata)
        debug_service_call("book_service", "get_book_by_uid_sync", {"uid": uid, "user_id": str(current_user.id)}, None, "BEFORE")
        user_book = book_service.get_book_by_uid_sync(uid, str(current_user.id))
        debug_service_call("book_service", "get_book_by_uid_sync", {"uid": uid, "user_id": str(current_user.id)}, user_book, "AFTER")
        
        if not user_book:
            debug_log(f"❌ [EDIT_META] User book {uid} not found for user {current_user.id}", "METADATA")
            flash('Book not found in your library.', 'error')
            return redirect(url_for('main.library'))
        
        # Enhanced metadata debugging
        book_id = getattr(user_book, 'id', 'NO_ID')
        existing_metadata = getattr(user_book, 'custom_metadata', {})
        debug_metadata_operation(book_id, uid, str(current_user.id), existing_metadata, "LOAD")
        
        title = user_book.get('title', 'Unknown Title') if isinstance(user_book, dict) else getattr(user_book, 'title', 'Unknown Title')
        debug_log(f"✅ [EDIT_META] Found user book: {title}", "METADATA")
        debug_log(f"📊 [EDIT_META] User book custom metadata: {getattr(user_book, 'custom_metadata', 'NO ATTR')}", "METADATA")
        
        if request.method == 'POST':
            debug_log(f"🔍 [EDIT_META] Processing POST request", "METADATA")
            debug_log(f"🔍 [EDIT_META] Form data keys: {list(request.form.keys())}", "METADATA")
            debug_log(f"🔍 [EDIT_META] Full form data: {dict(request.form)}", "METADATA")
            
            # Process form data for custom metadata
            # Note: In current architecture, we're storing everything as personal metadata
            personal_metadata = {}
            
            # Get available fields (treating all as personal for now)
            debug_service_call("custom_field_service", "get_available_fields_sync", {"user_id": current_user.id, "is_global": False}, None, "BEFORE")
            personal_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global=False)
            debug_service_call("custom_field_service", "get_available_fields_sync", {"user_id": current_user.id, "is_global": False}, personal_fields, "AFTER")
            
            debug_service_call("custom_field_service", "get_available_fields_sync", {"user_id": current_user.id, "is_global": True}, None, "BEFORE")
            global_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global=True)
            debug_service_call("custom_field_service", "get_available_fields_sync", {"user_id": current_user.id, "is_global": True}, global_fields, "AFTER")
            
            all_fields = personal_fields + global_fields
            
            debug_log(f"🔍 [EDIT_META] Found {len(personal_fields)} personal fields, {len(global_fields)} global fields", "METADATA")
            
            # Process all fields as personal metadata
            for field in all_fields:
                # Check both global_ and personal_ prefixes for backward compatibility
                personal_key = f'personal_{field.name}'
                global_key = f'global_{field.name}'
                
                personal_value = request.form.get(personal_key, '').strip()
                global_value = request.form.get(global_key, '').strip()
                
                print(f"🔍 [EDIT_META] Field {field.name}: personal_key='{personal_key}' value='{personal_value}', global_key='{global_key}' value='{global_value}'")
                
                value = personal_value or global_value
                if value:
                    personal_metadata[field.name] = value
                    print(f"✅ [EDIT_META] Added to metadata: {field.name} = {value}")
            
            print(f"📝 [EDIT_META] Final processed metadata: {personal_metadata}")
            
            # Validate metadata - TODO: Implement validation
            valid, errors = True, []  # custom_field_service.validate_and_save_metadata(personal_metadata, current_user.id, is_global=False)
            
            if valid:
                print(f"📝 [EDIT_META] Updating personal metadata: {personal_metadata}")
                
                # Update user book relationship with personal metadata
                book_id = user_book.get('id') or uid  # user_book is a dict, not an object
                success = book_service.update_user_book_sync(str(current_user.id), book_id, custom_metadata=personal_metadata)
                if success:
                    print(f"✅ [EDIT_META] Updated user book personal metadata")
                    flash('Custom metadata updated successfully!', 'success')
                    return redirect(url_for('main.view_book_enhanced', uid=uid))
                else:
                    print(f"❌ [EDIT_META] Failed to update user book personal metadata")
                    flash('Failed to update custom metadata.', 'error')
            else:
                # Show validation errors
                for error in errors:
                    flash(f'Validation error: {error}', 'error')
        
        # Get display data for template
        # For now, treat all fields as personal since that's how they're stored
        personal_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global=False)
        global_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global=True)
        
        print(f"🔍 [EDIT_META] Template data preparation:")
        print(f"   📋 [EDIT_META] Personal fields count: {len(personal_fields)}")
        print(f"   📋 [EDIT_META] Global fields count: {len(global_fields)}")
        
        # Get existing custom metadata
        existing_metadata = getattr(user_book, 'custom_metadata', {}) or {}
        print(f"   📊 [EDIT_META] Existing metadata: {existing_metadata}")
        
        # Prepare template data
        global_metadata = {}  # Empty since we're storing everything as personal
        personal_metadata = existing_metadata
        
        print(f"   📤 [EDIT_META] Passing to template:")
        print(f"      🌐 global_fields: {[f.name for f in global_fields]}")
        print(f"      👤 personal_fields: {[f.name for f in personal_fields]}")
        print(f"      🌐 global_metadata: {global_metadata}")
        print(f"      👤 personal_metadata: {personal_metadata}")
        
        return render_template(
            'edit_book_custom_metadata.html',
            book=user_book,
            user_book=user_book,
            global_fields=global_fields,
            personal_fields=personal_fields,
            global_metadata=global_metadata,  # Empty since we're storing everything as personal
            personal_metadata=personal_metadata,
            global_metadata_display=[],
            personal_metadata_display=custom_field_service.get_custom_metadata_for_display(existing_metadata)
        )
        
    except Exception as e:
        print(f"❌ [EDIT_META] Exception in edit_book_custom_metadata: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        flash(f'Error loading custom metadata: {str(e)}', 'error')
        return redirect(url_for('main.view_book_enhanced', uid=uid))


@bp.route('/month_review/<int:year>/<int:month>.jpg')
@login_required  
def month_review(year, month):
    # Query books finished in the given month/year by current user using global book visibility
    user_books = book_service.get_all_books_with_user_overlay_sync(str(current_user.id))
    
    # Filter books finished in the specified month/year
    start_date = datetime(year, month, 1).date()
    if month == 12:
        end_date = datetime(year + 1, 1, 1).date()
    else:
        end_date = datetime(year, month + 1, 1).date()
    
    books = [
        book for book in user_books 
        if getattr(book, 'finish_date', None) and start_date <= getattr(book, 'finish_date', None) < end_date
    ]
    
    if not books:
        # This should only be accessed if there are books (from month_wrapup)
        return "No books found", 404

    # Convert to format expected by generate_month_review_image
    # The function likely expects objects with title, author, cover_url attributes
    book_objects = []
    for book in books:
        # Create a simple object with the expected attributes
        book_obj = type('Book', (), {
            'title': book.get('title', '') if isinstance(book, dict) else getattr(book, 'title', ''),
            'author': book.get('author', '') if isinstance(book, dict) else getattr(book, 'author', ''),
            'cover_url': book.get('cover_url', '') if isinstance(book, dict) else getattr(book, 'cover_url', ''),
            'finish_date': book.get('finish_date') if isinstance(book, dict) else getattr(book, 'finish_date', None)
        })()
        book_objects.append(book_obj)

    img = generate_month_review_image(book_objects, month, year)
    buf = BytesIO()
    img.save(buf, format='JPEG')
    buf.seek(0)
    return send_file(buf, mimetype='image/jpeg', as_attachment=True, download_name=f"month_review_{year}_{month}.jpg")

@bp.route('/add_book_from_search', methods=['POST'])
@login_required
def add_book_from_search():
    # 🔥 SIMPLIFIED ARCHITECTURE INTERCEPT FOR SEARCH RESULTS
    # Use simplified service to avoid complex transaction issues
    try:
        print(f"📚 [SEARCH_INTERCEPT] Using simplified architecture for search book addition")
        
        title = request.form.get('title', '').strip()
        author = request.form.get('author', '').strip()
        isbn = request.form.get('isbn', '').strip()
        cover_url = request.form.get('cover_url', '').strip()
        description = request.form.get('description', '').strip()
        publisher = request.form.get('publisher', '').strip()
        published_date = request.form.get('published_date', '').strip()
        page_count_str = request.form.get('page_count', '').strip()
        categories_str = request.form.get('categories', '').strip()
        language = request.form.get('language', '').strip() or 'en'
        
        if not title:
            flash('Error: Title is required to add a book.', 'danger')
            return redirect(url_for('main.library'))
        
        # Parse page count
        page_count = None
        if page_count_str:
            try:
                page_count = int(page_count_str)
            except ValueError:
                pass
        
        # Parse categories
        categories = []
        if categories_str:
            categories = [cat.strip() for cat in categories_str.split(',') if cat.strip()]
        
        # Normalize ISBN
        isbn13 = None
        isbn10 = None
        if isbn:
            clean_isbn = ''.join(filter(str.isdigit, isbn))
            if len(clean_isbn) == 13:
                isbn13 = clean_isbn
            elif len(clean_isbn) == 10:
                isbn10 = clean_isbn
        
        # Auto-fetch cover if not provided and ISBN is available
        if not cover_url and (isbn13 or isbn10):
            try:
                from .utils import get_google_books_cover
                isbn_for_cover = isbn13 or isbn10
                auto_cover = get_google_books_cover(isbn_for_cover)
                if auto_cover:
                    cover_url = auto_cover
                    print(f"📷 [SEARCH] Auto-fetched cover from Google Books: {cover_url}")
                else:
                    print(f"📷 [SEARCH] No cover found for ISBN: {isbn_for_cover}")
            except Exception as e:
                print(f"⚠️ [SEARCH] Failed to auto-fetch cover: {e}")
        
        # Create simplified book data
        book_data = SimplifiedBook(
            title=title,
            author=author or "Unknown Author",
            isbn13=isbn13,
            isbn10=isbn10,
            description=description,
            publisher=publisher,
            published_date=published_date,
            page_count=page_count,
            language=language,
            cover_url=cover_url,
            categories=categories
        )
        
        # Use simplified service with default ownership settings
        service = SimplifiedBookService()
        success = service.add_book_to_user_library(
            book_data=book_data,
            user_id=current_user.id,
            reading_status='plan_to_read',  # Default for search results
            ownership_status='owned',       # Default 
            media_type='physical'          # Default
        )
        
        if success:
            flash(f'Successfully added "{title}" to your library!', 'success')
            print(f"✅ [SEARCH_INTERCEPT] Successfully added book using simplified architecture")
        else:
            flash('Failed to add book. Please try again.', 'danger')
            print(f"❌ [SEARCH_INTERCEPT] Failed to add book using simplified architecture")
            
        return redirect(url_for('main.library'))
        
    except Exception as e:
        print(f"❌ [SEARCH_INTERCEPT] Simplified architecture failed: {e}")
        flash('Error adding book. Please try again.', 'danger')
        return redirect(url_for('main.library'))
    
    # ORIGINAL FUNCTION BELOW (kept as fallback, but should never be reached)
    title = request.form.get('title')
    author = request.form.get('author')
    isbn = request.form.get('isbn')
    cover_url = request.form.get('cover_url')

    # Normalize ISBN by extracting digits only
    normalized_isbn = None
    if isbn:
        from .services import normalize_isbn_upc
        normalized_isbn = normalize_isbn_upc(isbn)
        if normalized_isbn:
            print(f"📚 [SEARCH] Normalized ISBN: {isbn} -> {normalized_isbn}")
        else:
            print(f"⚠️ [SEARCH] Could not normalize ISBN: {isbn}")

    # Prevent duplicate ISBNs
    if normalized_isbn:
        existing_book = book_service.get_books_by_isbn_sync(normalized_isbn)
        if existing_book:
            flash('A book with this ISBN already exists.', 'danger')
            return redirect(url_for('main.search_books'))

    # Get additional metadata if available
    google_data = None  # Initialize google_data
    if normalized_isbn:
        google_data = get_google_books_cover(normalized_isbn, fetch_title_author=True)
        if google_data:
            description = google_data.get('description')
            published_date = google_data.get('published_date')
            page_count = google_data.get('page_count')
            categories = google_data.get('categories')
            publisher = google_data.get('publisher')
            language = google_data.get('language')
            average_rating = google_data.get('average_rating')
            rating_count = google_data.get('rating_count')
        else:
            description = published_date = page_count = categories = publisher = language = average_rating = rating_count = None
    else:
        description = published_date = page_count = categories = publisher = language = average_rating = rating_count = None

    # Create contributors from author data
    contributors = []
    
    # Check if we have individual authors from Google Books API
    if google_data and google_data.get('authors_list'):
        # Use individual authors from Google Books API for better Person separation
        for i, author_name in enumerate(google_data['authors_list']):
            author_name = author_name.strip()
            if author_name:
                person = Person(name=author_name)
                contribution = BookContribution(
                    person=person,
                    contribution_type=ContributionType.AUTHORED,
                    order=i  # Maintain author order
                )
                contributors.append(contribution)
                print(f"Added author from Google Books API: {author_name}")
    elif author:
        # Fallback to manual/form author entry
        person = Person(name=author)
        contribution = BookContribution(
            person=person,
            contribution_type=ContributionType.AUTHORED,
            order=0
        )
        contributors.append(contribution)
        print(f"Added author from form: {author}")

    # Create a domain book object
    domain_book = DomainBook(
        title=title,
        isbn13=normalized_isbn if normalized_isbn and len(normalized_isbn) == 13 else None,
        isbn10=normalized_isbn if normalized_isbn and len(normalized_isbn) == 10 else None,
        contributors=contributors,
        cover_url=cover_url,
        description=description,
        published_date=_convert_published_date_to_date(published_date),
        page_count=page_count,
        raw_categories=categories,  # Use raw_categories instead of processing here
        publisher=Publisher(name=publisher) if publisher else None,
        language=language,
        average_rating=average_rating,
        rating_count=rating_count
    )
    
    # Use the service to create the book
    try:
        created_book = book_service.find_or_create_book_sync(domain_book)
        # Add to user's library
        book_service.add_book_to_user_library_sync(
            user_id=current_user.id,
            book_id=created_book.id,
            reading_status=ReadingStatus.PLAN_TO_READ
        )
        flash(f'Added "{title}" to your library.', 'success')
    except Exception as e:
        current_app.logger.error(f"Error creating book: {e}")
        flash('Error adding book to library.', 'danger')
    
    return redirect(url_for('main.library'))


@bp.route('/download_db', methods=['GET'])
@login_required
def download_db():
    """Export user data from Kuzu to CSV format."""
    try:
        # Get all user books with global visibility
        user_books = book_service.get_all_books_with_user_overlay_sync(str(current_user.id))
        
        # Create CSV export
        import io
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Write header
        writer.writerow(['Title', 'Author', 'ISBN', 'Reading Status', 'Start Date', 'Finish Date', 'Rating', 'Notes'])
        
        # Write book data
        for book in user_books:
            author_names = ', '.join([author.name for author in book.authors]) if book.authors else ''
            isbn = book.isbn13 or book.isbn10 or ''
            start_date = book.start_date.isoformat() if hasattr(book, 'start_date') and book.start_date else ''
            finish_date = book.finish_date.isoformat() if hasattr(book, 'finish_date') and book.finish_date else ''
            rating = getattr(book, 'user_rating', '') or ''
            notes = getattr(book, 'personal_notes', '') or ''
            reading_status = getattr(book, 'reading_status', '') or ''
            
            title = book.get('title', '') if isinstance(book, dict) else getattr(book, 'title', '')
            writer.writerow([title, author_names, isbn, reading_status, start_date, finish_date, rating, notes])
        
        # Create response
        output.seek(0)
        response = make_response(output.getvalue())
        response.headers['Content-Type'] = 'text/csv'
        response.headers['Content-Disposition'] = f'attachment; filename=bibliotheca_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        
        return response
    except Exception as e:
        current_app.logger.error(f"Error exporting data: {e}")
        flash('Error exporting data.', 'danger')
        return redirect(url_for('main.reading_history'))

@bp.route('/community_activity/active_readers')
@login_required
def community_active_readers():
    """Redirect to stats page where community activity is now integrated"""
    flash('Community activity has been moved to the Stats page.', 'info')
    return redirect(url_for('main.stats'))

@bp.route('/community_activity/books_this_month')
@login_required
def community_books_this_month():
    """Redirect to stats page where community activity is now integrated"""
    flash('Community activity has been moved to the Stats page.', 'info')
    return redirect(url_for('main.stats'))

@bp.route('/community_activity/currently_reading')
@login_required
def community_currently_reading():
    """Redirect to stats page where community activity is now integrated"""
    flash('Community activity has been moved to the Stats page.', 'info')
    return redirect(url_for('main.stats'))

@bp.route('/community_activity/recent_activity')
@login_required
def community_recent_activity():
    """Redirect to stats page where community activity is now integrated"""
    flash('Community activity has been moved to the Stats page.', 'info')
    return redirect(url_for('main.stats'))

@bp.route('/user/<string:user_id>/profile')
@login_required
def user_profile(user_id):
    """Show public profile for a user if they're sharing"""
    try:
        user = user_service.get_user_by_id_sync(user_id)
        if not user:
            abort(404)
        
        # Check if user allows profile viewing
        if not user.share_reading_activity:
            flash('This user has not enabled profile sharing.', 'warning')
            return redirect(url_for('main.stats'))
        
        # Get user's books with global visibility
        all_user_books = book_service.get_all_books_with_user_overlay_sync(str(user_id))
        
        # Calculate statistics from the book list
        current_year = datetime.now().year
        current_month = datetime.now().date().replace(day=1)
        
        # Helper function to safely get date from finish_date (handles both date and datetime objects)
        def get_finish_date_safe(book):
            finish_date = getattr(book, 'finish_date', None)
            if finish_date is None:
                return None
            # Convert datetime to date if needed
            if isinstance(finish_date, datetime):
                return finish_date.date()
            return finish_date
        
        total_books = len([book for book in all_user_books if get_finish_date_safe(book)])
        
        books_this_year = len([book for book in all_user_books 
                              if get_finish_date_safe(book) and 
                              get_finish_date_safe(book) >= date(current_year, 1, 1)])
        
        books_this_month = len([book for book in all_user_books 
                               if get_finish_date_safe(book) and 
                               get_finish_date_safe(book) >= current_month])
        
        currently_reading = [book for book in all_user_books 
                           if getattr(book, 'start_date', None) and not getattr(book, 'finish_date', None)] if user.share_current_reading else []
        
        recent_finished = sorted([book for book in all_user_books if get_finish_date_safe(book)],
                                key=lambda x: get_finish_date_safe(x) or date.min, reverse=True)[:10]
        
        # Get reading logs count from service
        reading_logs_count = reading_log_service.get_user_logs_count_sync(user_id)
        
        return render_template('user_profile.html',
                             profile_user=user,
                             total_books=total_books,
                             books_this_year=books_this_year,
                             books_this_month=books_this_month,
                             currently_reading=currently_reading,
                             recent_finished=recent_finished,
                             reading_logs_count=reading_logs_count)
    
    except Exception as e:
        current_app.logger.error(f"Error loading user profile {user_id}: {e}")
        flash('Error loading user profile.', 'danger')
        return redirect(url_for('main.stats'))

@bp.route('/book/<uid>/assign', methods=['POST'])
@login_required
def assign_book(uid):
    try:
        book = book_service.get_book_by_uid_sync(uid, current_user.id)
        if not book:
            abort(404)
            
        if not current_user.is_admin:
            flash('Only admins can assign books.', 'danger')
            return redirect(url_for('main.library'))

        user_id = request.form.get('user_id')
        user = user_service.get_user_by_id_sync(user_id)
        if not user:
            flash('Invalid user selected.', 'danger')
            return redirect(url_for('main.library'))

        # For Kuzu, this would involve transferring the book to another user
        # This is a complex operation that would need to be implemented in the service layer
        flash('Book assignment feature needs to be implemented for Kuzu backend.', 'warning')
        return redirect(url_for('main.library'))
    except Exception as e:
        current_app.logger.error(f"Error assigning book: {e}")
        flash('Error assigning book.', 'danger')
        return redirect(url_for('main.library'))

@bp.route('/books/bulk_delete', methods=['POST'])
@login_required
def bulk_delete_books():
    """Delete multiple books selected from the library view."""
    print(f"Bulk delete route called by user {current_user.id}")
    print(f"Request method: {request.method}")
    print(f"Request form keys: {list(request.form.keys())}")
    print(f"Full form data: {dict(request.form)}")
    
    # Try both possible field names
    selected_uids = request.form.getlist('selected_books')
    if not selected_uids:
        selected_uids = request.form.getlist('book_ids')
    
    # If still empty, try getting as single values
    if not selected_uids:
        single_value = request.form.get('selected_books') or request.form.get('book_ids')
        if single_value:
            # Split by comma or newline if multiple values in single field
            selected_uids = [uid.strip() for uid in single_value.replace(',', '\n').split('\n') if uid.strip()]
    
    # Filter out empty strings
    selected_uids = [uid for uid in selected_uids if uid and uid.strip()]
    
    print(f"Selected UIDs after filtering: {selected_uids}")
    
    if not selected_uids:
        print("No books selected for deletion")
        flash('No books selected for deletion.', 'warning')
        return redirect(url_for('main.library'))
    
    deleted_count = 0
    failed_count = 0
    
    for uid in selected_uids:
        try:
            print(f"Attempting to globally delete book {uid}")
            success = book_service.delete_book_globally_sync(uid)
            print(f"Global delete result for {uid}: {success}")
            if success:
                deleted_count += 1
            else:
                failed_count += 1
        except Exception as e:
            print(f"Error globally deleting book {uid}: {e}")
            import traceback
            traceback.print_exc()
            failed_count += 1
    
    print(f"Bulk global delete completed: {deleted_count} deleted, {failed_count} failed")
    
    if deleted_count > 0:
        flash(f'Successfully deleted {deleted_count} book(s) globally.', 'success')
    if failed_count > 0:
        flash(f'Failed to delete {failed_count} book(s).', 'error')
    
    return redirect(url_for('main.library'))

@bp.route('/csrf-guide')
def csrf_guide():
    """Demo page showing CSRF protection implementation."""
    return render_template('csrf_guide.html')

@bp.route('/stats')
@login_required
def stats():
    """Combined user and community statistics page"""
    # Get current date for time-based calculations
    timezone = pytz.timezone(current_app.config.get('TIMEZONE', 'UTC'))
    now = datetime.now(timezone)
    today = now.date()
    
    # Calculate date ranges
    week_start = today - timedelta(days=today.weekday())
    month_start = today.replace(day=1)
    year_start = today.replace(month=1, day=1)
    
    # Get user books for stats calculations with global visibility
    user_books = book_service.get_all_books_with_user_overlay_sync(str(current_user.id))
    
    # Calculate user stats - use getattr for safe attribute access
    # Helper function to safely get date from finish_date (handles both date and datetime objects)
    def get_finish_date(book):
        finish_date = getattr(book, 'finish_date', None)
        if finish_date is None:
            return None
        # Convert datetime to date if needed
        if isinstance(finish_date, datetime):
            return finish_date.date()
        return finish_date
    
    books_finished_this_week = len([b for b in user_books if get_finish_date(b) and get_finish_date(b) >= week_start])
    books_finished_this_month = len([b for b in user_books if get_finish_date(b) and get_finish_date(b) >= month_start])
    books_finished_this_year = len([b for b in user_books if get_finish_date(b) and get_finish_date(b) >= year_start])
    books_finished_total = len([b for b in user_books if get_finish_date(b)])
    
    currently_reading = [b for b in user_books if not getattr(b, 'finish_date', None) and not getattr(b, 'want_to_read', False) and not getattr(b, 'library_only', False)]
    want_to_read = [b for b in user_books if getattr(b, 'want_to_read', False)]
    
    # Get reading streak for current user
    streak = current_user.get_reading_streak()
    
    # Community stats (only if users share their activity)
    try:
        # Use the imported book service for community features
        
        # Get sharing users count
        sharing_users = user_service.get_sharing_users_sync() if hasattr(user_service, 'get_sharing_users_sync') else []
        
        # Recent community activity (books finished in last 30 days)
        recent_finished_books = []  # TODO: Implement community features with Kuzu
        
        # Community currently reading
        community_currently_reading = []  # TODO: Implement community features with Kuzu
        
        # Community stats summary - count finished books this month
        month_finished_books = []  # TODO: Implement community features with Kuzu
        month_start = datetime.now().date().replace(day=1)
        
        # Helper function to safely get date from finish_date for community stats
        def get_finish_date_community(book):
            finish_date = getattr(book, 'finish_date', None)
            if finish_date is None:
                return None
            # Convert datetime to date if needed
            if isinstance(finish_date, datetime):
                return finish_date.date()
            return finish_date
        
        total_books_this_month = len([book for book in month_finished_books 
                                    if get_finish_date_community(book) and get_finish_date_community(book) >= month_start])
        
        total_active_readers = len(sharing_users)
        
        # Recent reading logs (from last 7 days) - use service
        recent_logs = reading_log_service.get_recent_shared_logs_sync(days_back=7, limit=50) if hasattr(reading_log_service, 'get_recent_shared_logs_sync') else []
        
    except Exception as e:
        current_app.logger.error(f"Error loading community stats: {e}")
        # Fallback to empty data
        recent_finished_books = []
        community_currently_reading = []
        total_books_this_month = 0
        total_active_readers = 0
        recent_logs = []
    
    return render_template('stats.html',
                         # User stats
                         books_finished_this_week=books_finished_this_week,
                         books_finished_this_month=books_finished_this_month,
                         books_finished_this_year=books_finished_this_year,
                         books_finished_total=books_finished_total,
                         currently_reading=currently_reading,
                         want_to_read=want_to_read,
                         streak=streak,
                         # Community stats
                         recent_finished_books=recent_finished_books,
                         community_currently_reading=community_currently_reading,
                         total_books_this_month=total_books_this_month,
                         total_active_readers=total_active_readers,
                         recent_logs=recent_logs,
                         sharing_users=sharing_users)

@bp.route('/search_books_in_library', methods=['POST'])
@login_required
def search_books_in_library():
    """Search for books to add from the library page"""
    query = request.form.get('query', '')
    results = []
    
    if query:
        # Google Books API search
        resp = requests.get(
            'https://www.googleapis.com/books/v1/volumes',
            params={'q': query, 'maxResults': 10}
        )
        data = resp.json()
        for item in data.get('items', []):
            volume_info = item.get('volumeInfo', {})
            image = volume_info.get('imageLinks', {}).get('thumbnail')
            isbn = None
            for iden in volume_info.get('industryIdentifiers', []):
                if iden['type'] in ('ISBN_13', 'ISBN_10'):
                    isbn = iden['identifier']
                    break
            results.append({
                'title': volume_info.get('title'),
                'authors': ', '.join(volume_info.get('authors', [])),
                'image': image,
                'isbn': isbn
            })
    
    # Get all the data needed for library page
    # Get filter parameters from URL
    category_filter = request.args.get('category', '')
    publisher_filter = request.args.get('publisher', '')
    language_filter = request.args.get('language', '')
    location_filter = request.args.get('location', '')
    search_query = request.args.get('search', '')

    # Use service layer with global book visibility
    user_books = book_service.get_all_books_with_user_overlay_sync(str(current_user.id))
    
    # Apply filters in Python (Kuzu doesn't have complex querying like SQL)
    filtered_books = user_books
    
    if search_query:
        search_lower = search_query.lower()
        filtered_books = [
            book for book in filtered_books 
            if (search_lower in (book.get('title', '') if isinstance(book, dict) else getattr(book, 'title', '')).lower()) or
               (search_lower in (book.get('author', '') if isinstance(book, dict) else getattr(book, 'author', '')).lower()) or
               (search_lower in (book.get('description', '') if isinstance(book, dict) else getattr(book, 'description', '')).lower())
        ]
    if publisher_filter:
        filtered_books = [
            book for book in filtered_books 
            if (book.get('publisher', '') if isinstance(book, dict) else getattr(book, 'publisher', '')) and publisher_filter.lower() in (book.get('publisher', '') if isinstance(book, dict) else getattr(book, 'publisher', '')).lower()
        ]
    if language_filter:
        filtered_books = [
            book for book in filtered_books 
            if (book.get('language', '') if isinstance(book, dict) else getattr(book, 'language', '')) == language_filter
        ]
    if location_filter:
        filtered_books = [
            book for book in filtered_books 
            if book.locations and any(
                location_filter.lower() in loc.name.lower() 
                for loc in book.locations 
                if hasattr(loc, 'name')
            )
        ]
    if category_filter:
        filtered_books = [
            book for book in filtered_books 
            if book.categories and any(category_filter.lower() in cat.name.lower() for cat in book.categories if hasattr(cat, 'name'))
        ]

    # Books are already in the right format for the template
    books = filtered_books

    # Get distinct values for filter dropdowns
    all_books = user_books  # Use same data
    
    categories = set()
    publishers = set()
    languages = set()
    locations = set()

    for book in all_books:
        if book.categories:
            # book.categories is a list of Category objects, not a string
            categories.update([cat.name for cat in book.categories if hasattr(cat, 'name')])
        if book.publisher:
            # Handle Publisher domain object or string
            publisher_name = book.publisher.name if hasattr(book.publisher, 'name') else str(book.publisher)
            publishers.add(publisher_name)
        if book.language:
            languages.add(book.language)
        if book.locations:
            # book.locations is a list of Location objects
            locations.update([loc.name for loc in book.locations if hasattr(loc, 'name')])

    # Get users through Kuzu service layer
    domain_users = user_service.get_all_users_sync()
    
    # Convert domain users to simple objects for template compatibility
    users = []
    for domain_user in domain_users:
        user_data = {
            'id': domain_user.id,
            'username': domain_user.username,
            'email': domain_user.email
        }
        users.append(type('User', (), user_data))

    # Calculate statistics for filter buttons
    stats = {
        'total_books': len(user_books),
        'books_read': len([b for b in user_books if getattr(b, 'reading_status', None) == 'read']),
        'currently_reading': len([b for b in user_books if getattr(b, 'reading_status', None) == 'reading']),
        'want_to_read': len([b for b in user_books if getattr(b, 'reading_status', None) == 'plan_to_read']),
        'on_hold': len([b for b in user_books if getattr(b, 'reading_status', None) == 'on_hold']),
        'wishlist': len([b for b in user_books if getattr(b, 'ownership_status', None) == 'wishlist']),
    }

    return render_template(
        'library_enhanced.html',
        books=books,
        stats=stats,
        categories=sorted(categories),
        publishers=sorted(publishers),
        languages=sorted(languages),
        locations=sorted(locations),
        current_status_filter='all',
        current_category=category_filter,
        current_publisher=publisher_filter,
        current_language=language_filter,
        current_location=location_filter,
        current_search=search_query,
        current_sort='title_asc',
        users=users,
        search_results=results,
        search_query=query
    )

@bp.route('/add_book_manual', methods=['POST'])
@login_required
def add_book_manual():
    """Add a book manually from the library page"""
    # 🔥 SIMPLIFIED ARCHITECTURE INTERCEPT
    # Use simplified service to avoid complex transaction issues
    try:
        print(f"📚 [INTERCEPT] Using simplified architecture for manual book addition")
        
        # Get form data
        title = request.form['title'].strip()
        if not title:
            flash('Error: Title is required to add a book.', 'danger')
            return redirect(url_for('main.library'))
        
        author = request.form.get('author', '').strip()
        isbn = request.form.get('isbn', '').strip()
        subtitle = request.form.get('subtitle', '').strip()
        publisher_name = request.form.get('publisher', '').strip()
        description = request.form.get('description', '').strip()
        page_count_str = request.form.get('page_count', '').strip()
        language = request.form.get('language', '').strip() or 'en'
        cover_url = request.form.get('cover_url', '').strip()
        published_date_str = request.form.get('published_date', '').strip()
        series = request.form.get('series', '').strip()
        series_volume = request.form.get('series_volume', '').strip()
        
        # Parse page count
        page_count = None
        if page_count_str:
            try:
                page_count = int(page_count_str)
            except ValueError:
                pass
        
        # Parse categories from manual_categories field
        categories = []
        if request.form.get('manual_categories'):
            categories = [cat.strip() for cat in request.form.get('manual_categories').split(',') if cat.strip()]
        
        # Get location
        location_id = request.form.get('location_id')
        
        # Get ownership details
        reading_status = request.form.get('reading_status', 'plan_to_read')
        ownership_status = request.form.get('ownership_status', 'owned')
        media_type = request.form.get('media_type', 'physical')
        
        # Enhanced ISBN processing with comprehensive field mapping
        isbn13 = None
        isbn10 = None
        api_data = None
        cached_cover_url = None
        
        if isbn:
            # Import required modules
            import re
            import requests
            import uuid
            from pathlib import Path
            
            # Normalize ISBN with proper conversion
            clean_isbn = re.sub(r'[^0-9X]', '', str(isbn).upper())
            
            if len(clean_isbn) == 10:
                # ISBN10 - validate and convert to ISBN13
                isbn10 = clean_isbn
                # Convert ISBN10 to ISBN13
                isbn13_base = "978" + clean_isbn[:9]
                check_sum = 0
                for i, digit in enumerate(isbn13_base):
                    check_sum += int(digit) * (1 if i % 2 == 0 else 3)
                check_digit = (10 - (check_sum % 10)) % 10
                isbn13 = isbn13_base + str(check_digit)
                print(f"📚 [MANUAL] Converted ISBN10 {isbn10} to ISBN13 {isbn13}")
            elif len(clean_isbn) == 13:
                # ISBN13 - try to convert to ISBN10 if it starts with 978
                isbn13 = clean_isbn
                if clean_isbn.startswith('978'):
                    isbn10_base = clean_isbn[3:12]
                    check_sum = 0
                    for i, digit in enumerate(isbn10_base):
                        check_sum += int(digit) * (10 - i)
                    check_digit = (11 - (check_sum % 11)) % 11
                    if check_digit == 10:
                        check_digit = 'X'
                    elif check_digit == 11:
                        check_digit = '0'
                    isbn10 = isbn10_base + str(check_digit)
                    print(f"📚 [MANUAL] Converted ISBN13 {isbn13} to ISBN10 {isbn10}")
            
            # Enhanced API lookup with both Google Books and OpenLibrary
            normalized_isbn = isbn13 or isbn10
            if normalized_isbn:
                print(f"🔍 [MANUAL] Performing enhanced API lookup for ISBN: {normalized_isbn}")
                
                try:
                    # Google Books lookup
                    google_data = None
                    url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{normalized_isbn}"
                    response = requests.get(url, timeout=10)
                    if response.status_code == 200:
                        data = response.json()
                        items = data.get("items")
                        if items:
                            volume_info = items[0]["volumeInfo"]
                            google_data = {
                                'title': volume_info.get('title', ''),
                                'subtitle': volume_info.get('subtitle', ''),
                                'description': volume_info.get('description', ''),
                                'authors_list': volume_info.get('authors', []),
                                'publisher': volume_info.get('publisher', ''),
                                'published_date': volume_info.get('publishedDate', ''),
                                'page_count': volume_info.get('pageCount'),
                                'language': volume_info.get('language', 'en'),
                                'categories': volume_info.get('categories', []),
                                'isbn10': isbn10,
                                'isbn13': isbn13,
                                'cover_url': None
                            }
                            
                            # Get best quality cover image
                            image_links = volume_info.get("imageLinks", {})
                            for size in ['extraLarge', 'large', 'medium', 'thumbnail', 'smallThumbnail']:
                                if size in image_links:
                                    google_data['cover_url'] = image_links[size].replace('http://', 'https://')
                                    break
                            
                            print(f"✅ [MANUAL] Google Books data retrieved")
                    
                    # OpenLibrary lookup
                    openlibrary_data = None
                    url = f"https://openlibrary.org/api/books?bibkeys=ISBN:{normalized_isbn}&format=json&jscmd=data"
                    response = requests.get(url, timeout=10)
                    if response.status_code == 200:
                        data = response.json()
                        book_key = f"ISBN:{normalized_isbn}"
                        if book_key in data:
                            book = data[book_key]
                            
                            # Extract categories/subjects
                            subjects = book.get('subjects', [])
                            ol_categories = []
                            for subject in subjects[:10]:  # Limit to 10
                                if isinstance(subject, dict):
                                    ol_categories.append(subject.get('name', ''))
                                else:
                                    ol_categories.append(str(subject))
                            ol_categories = [cat for cat in ol_categories if cat]
                            
                            openlibrary_data = {
                                'categories': ol_categories,
                                'description': book.get('notes', {}).get('value', '') if isinstance(book.get('notes'), dict) else str(book.get('notes', '')),
                                'cover_url': book.get('cover', {}).get('large') or book.get('cover', {}).get('medium') or book.get('cover', {}).get('small')
                            }
                            print(f"✅ [MANUAL] OpenLibrary data retrieved")
                    
                    # Merge API data
                    if google_data or openlibrary_data:
                        api_data = google_data or {}
                        if openlibrary_data:
                            # Merge categories
                            google_cats = set(api_data.get('categories', []))
                            ol_cats = set(openlibrary_data.get('categories', []))
                            api_data['categories'] = list(google_cats.union(ol_cats))
                            
                            # Use OpenLibrary description if Google doesn't have one
                            if not api_data.get('description') and openlibrary_data.get('description'):
                                api_data['description'] = openlibrary_data['description']
                        
                        print(f"🎯 [MANUAL] Merged API data: {len(api_data.get('categories', []))} categories, cover: {bool(api_data.get('cover_url'))}")
                        
                        # Use API data to fill in missing form fields
                        if not cover_url and api_data.get('cover_url'):
                            cover_url = api_data['cover_url']
                        if not description and api_data.get('description'):
                            description = api_data['description']
                        if not publisher_name and api_data.get('publisher'):
                            publisher_name = api_data['publisher']
                        if not page_count and api_data.get('page_count'):
                            page_count = api_data['page_count']
                        if not published_date_str and api_data.get('published_date'):
                            published_date_str = api_data['published_date']
                        if not categories and api_data.get('categories'):
                            categories = api_data['categories']
                        
                        # Download and cache cover image
                        if cover_url:
                            try:
                                book_temp_id = str(uuid.uuid4())
                                covers_dir = Path(__file__).parent.parent / 'static' / 'covers'
                                covers_dir.mkdir(exist_ok=True)
                                
                                # Generate filename
                                file_extension = '.jpg'
                                if cover_url.lower().endswith('.png'):
                                    file_extension = '.png'
                                
                                filename = f"{book_temp_id}{file_extension}"
                                filepath = covers_dir / filename
                                
                                # Download the image
                                response = requests.get(cover_url, timeout=10, stream=True)
                                response.raise_for_status()
                                
                                with open(filepath, 'wb') as f:
                                    for chunk in response.iter_content(chunk_size=8192):
                                        f.write(chunk)
                                
                                cached_cover_url = f"/static/covers/{filename}"
                                cover_url = cached_cover_url
                                print(f"🖼️ [MANUAL] Cover cached: {cached_cover_url}")
                                
                            except Exception as e:
                                print(f"⚠️ [MANUAL] Failed to cache cover: {e}")
                
                except Exception as e:
                    print(f"⚠️ [MANUAL] API lookup failed: {e}")
        
        # Create simplified book data with enhanced API fields
        book_data = SimplifiedBook(
            title=title,
            author=author or "Unknown Author",
            isbn13=isbn13,
            isbn10=isbn10,
            subtitle=subtitle,
            description=description,
            publisher=publisher_name,
            published_date=published_date_str,
            page_count=page_count,
            language=language,
            cover_url=cover_url,  # This will be the cached URL if available
            series=series,
            series_volume=series_volume,
            categories=categories  # Enhanced with API data
        )
        
        # Enhanced debugging for ISBN and cover
        print(f"🔍 [MANUAL] Final book data:")
        print(f"   Title: {book_data.title}")
        print(f"   Author: {book_data.author}")
        print(f"   ISBN13: {book_data.isbn13}")
        print(f"   ISBN10: {book_data.isbn10}")
        print(f"   Cover URL: {book_data.cover_url}")
        print(f"   Categories: {book_data.categories}")
        print(f"   Description: {book_data.description[:100] if book_data.description else 'None'}...")
        print(f"   Publisher: {book_data.publisher}")
        print(f"   Page count: {book_data.page_count}")
        
        # Use simplified service
        service = SimplifiedBookService()
        success = service.add_book_to_user_library(
            book_data=book_data,
            user_id=current_user.id,
            reading_status=reading_status,
            ownership_status=ownership_status,
            media_type=media_type,
            location_id=location_id
        )
        
        if success:
            flash(f'Successfully added "{title}" to your library!', 'success')
            print(f"✅ [INTERCEPT] Successfully added book using simplified architecture")
        else:
            flash('Failed to add book. Please try again.', 'danger')
            print(f"❌ [INTERCEPT] Failed to add book using simplified architecture")
            
        return redirect(url_for('main.library'))
        
    except Exception as e:
        print(f"❌ [INTERCEPT] Simplified architecture failed: {e}")
        flash('Error adding book. Please try again.', 'danger')
        return redirect(url_for('main.library'))
    
    # ORIGINAL FUNCTION BELOW (kept as fallback, but should never be reached)
    # Validate required fields
    title = request.form['title'].strip()
    if not title:
        flash('Error: Title is required to add a book.', 'danger')
        return redirect(url_for('main.library'))

    # Extract all form fields
    isbn = request.form.get('isbn', '').strip()
    author = request.form.get('author', '').strip()
    subtitle = request.form.get('subtitle', '').strip()
    publisher_name = request.form.get('publisher', '').strip()
    page_count_str = request.form.get('page_count', '').strip()
    language = request.form.get('language', '').strip() or 'en'
    cover_url = request.form.get('cover_url', '').strip()
    published_date_str = request.form.get('published_date', '').strip()
    series = request.form.get('series', '').strip()
    series_volume = request.form.get('series_volume', '').strip()
    series_order_str = request.form.get('series_order', '').strip()
    genres = request.form.get('genres', '').strip()  # Changed from 'genre' to 'genres'
    reading_status = request.form.get('reading_status', '').strip()
    ownership_status = request.form.get('ownership_status', '').strip()
    media_type = request.form.get('media_type', '').strip()
    location_id = request.form.get('location_id', '').strip()
    user_rating_str = request.form.get('user_rating', '').strip()
    personal_notes = request.form.get('personal_notes', '').strip()
    user_tags = request.form.get('user_tags', '').strip()
    description = request.form.get('description', '').strip()
    
    # Convert numeric fields
    page_count = None
    if page_count_str:
        try:
            page_count = int(page_count_str)
        except ValueError:
            pass
    
    series_order = None
    if series_order_str:
        try:
            series_order = int(series_order_str)
        except ValueError:
            pass
    
    user_rating = None
    if user_rating_str:
        try:
            user_rating = float(user_rating_str)
        except ValueError:
            pass
    
    # Convert date fields
    published_date = None
    if published_date_str:
        try:
            published_date = datetime.strptime(published_date_str, '%Y-%m-%d').date()
        except ValueError:
            pass
    
    # Enhanced ISBN processing with proper normalization
    isbn10 = None
    isbn13 = None
    normalized_isbn = None
    
    if isbn:
        # Enhanced ISBN normalization
        import re
        clean_isbn = re.sub(r'[^0-9X]', '', str(isbn).upper())
        
        if len(clean_isbn) == 10:
            # ISBN10 - validate and convert to ISBN13
            isbn10 = clean_isbn
            # Convert ISBN10 to ISBN13
            isbn13_base = "978" + clean_isbn[:9]
            check_sum = 0
            for i, digit in enumerate(isbn13_base):
                check_sum += int(digit) * (1 if i % 2 == 0 else 3)
            check_digit = (10 - (check_sum % 10)) % 10
            isbn13 = isbn13_base + str(check_digit)
            normalized_isbn = isbn13  # Prefer ISBN13 for API lookups
        elif len(clean_isbn) == 13:
            # ISBN13 - try to convert to ISBN10 if it starts with 978
            isbn13 = clean_isbn
            normalized_isbn = isbn13
            if clean_isbn.startswith('978'):
                isbn10_base = clean_isbn[3:12]
                check_sum = 0
                for i, digit in enumerate(isbn10_base):
                    check_sum += int(digit) * (10 - i)
                check_digit = (11 - (check_sum % 11)) % 11
                if check_digit == 10:
                    check_digit = 'X'
                elif check_digit == 11:
                    check_digit = '0'
                isbn10 = isbn10_base + str(check_digit)
        
        print(f"📚 [MANUAL] Enhanced ISBN processing: {isbn} -> ISBN10: {isbn10}, ISBN13: {isbn13}")
    
    # Process manual genre input
    manual_categories = []
    if genres:
        # Split by comma and clean up
        manual_categories = [cat.strip() for cat in genres.split(',') if cat.strip()]
        print(f"📚 [MANUAL] Manual categories: {manual_categories}")
    
    start_date_str = request.form.get('start_date') or None
    finish_date_str = request.form.get('finish_date') or None
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date() if start_date_str else None
    finish_date = datetime.strptime(finish_date_str, '%Y-%m-%d').date() if finish_date_str else None

    # Process custom metadata - only allow selection of existing fields
    custom_metadata = {}
    
    # Get all submitted custom field selections
    for key, value in request.form.items():
        if key.startswith('custom_field_') and value and value.strip():
            # Extract field name from the form key (custom_field_<field_name>)
            field_name = key.replace('custom_field_', '')
            field_value = value.strip()
            
            print(f"📋 [MANUAL] Selected existing custom field: {field_name} = {field_value}")
            custom_metadata[field_name] = field_value

    print(f"📊 [MANUAL] Final custom metadata: {custom_metadata}")

    # Enhanced API lookup with comprehensive field mapping
    api_data = None
    final_cover_url = cover_url
    cached_cover_url = None
    
    if normalized_isbn:
        print(f"🔍 [MANUAL] Performing enhanced API lookup for ISBN: {normalized_isbn}")
        
        # Enhanced Google Books API lookup
        def enhanced_google_books_lookup(isbn):
            url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
            try:
                import requests
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                items = data.get("items")
                if not items:
                    return None
                
                volume_info = items[0]["volumeInfo"]
                
                # Extract ISBN information from industryIdentifiers
                api_isbn10 = None
                api_isbn13 = None
                asin = None
                
                industry_identifiers = volume_info.get('industryIdentifiers', [])
                for identifier in industry_identifiers:
                    id_type = identifier.get('type', '')
                    id_value = identifier.get('identifier', '')
                    
                    if id_type == 'ISBN_10':
                        api_isbn10 = id_value
                    elif id_type == 'ISBN_13':
                        api_isbn13 = id_value
                    elif id_type == 'OTHER' and 'ASIN' in id_value:
                        asin = id_value
                
                # Get best quality cover image
                image_links = volume_info.get("imageLinks", {})
                cover_url = None
                for size in ['extraLarge', 'large', 'medium', 'thumbnail', 'smallThumbnail']:
                    if size in image_links:
                        cover_url = image_links[size]
                        break
                
                # Force HTTPS for cover URLs
                if cover_url and cover_url.startswith('http://'):
                    cover_url = cover_url.replace('http://', 'https://')
                
                return {
                    'title': volume_info.get('title', ''),
                    'subtitle': volume_info.get('subtitle', ''),
                    'description': volume_info.get('description', ''),
                    'authors_list': volume_info.get('authors', []),
                    'publisher': volume_info.get('publisher', ''),
                    'published_date': volume_info.get('publishedDate', ''),
                    'page_count': volume_info.get('pageCount'),
                    'language': volume_info.get('language', 'en'),
                    'average_rating': volume_info.get('averageRating'),
                    'rating_count': volume_info.get('ratingsCount'),
                    'categories': volume_info.get('categories', []),
                    'isbn10': api_isbn10,
                    'isbn13': api_isbn13,
                    'asin': asin,
                    'cover_url': cover_url,
                    'google_books_id': items[0].get('id', ''),
                    'source': 'google_books'
                }
            except Exception as e:
                print(f"❌ Google Books API error: {e}")
                return None
        
        # Enhanced OpenLibrary API lookup
        def enhanced_openlibrary_lookup(isbn):
            url = f"https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}&format=json&jscmd=data"
            try:
                import requests
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                book_key = f"ISBN:{isbn}"
                if book_key not in data:
                    return None
                
                book = data[book_key]
                
                # Extract authors
                authors_list = []
                authors_data = book.get('authors', [])
                for author in authors_data:
                    if isinstance(author, dict) and 'name' in author:
                        authors_list.append(author['name'])
                    elif isinstance(author, str):
                        authors_list.append(author)
                
                # Extract publisher
                publishers = book.get('publishers', [])
                publisher = ''
                if publishers:
                    if isinstance(publishers[0], dict):
                        publisher = publishers[0].get('name', '')
                    else:
                        publisher = str(publishers[0])
                
                # Extract categories/subjects
                subjects = book.get('subjects', [])
                categories = []
                for subject in subjects[:10]:  # Limit to 10 categories
                    if isinstance(subject, dict):
                        categories.append(subject.get('name', ''))
                    else:
                        categories.append(str(subject))
                categories = [cat for cat in categories if cat]
                
                # Extract cover image
                cover_data = book.get('cover', {})
                cover_url = None
                for size in ['large', 'medium', 'small']:
                    if size in cover_data:
                        cover_url = cover_data[size]
                        break
                
                # Description handling
                description = book.get('notes', {})
                if isinstance(description, dict):
                    description = description.get('value', '')
                elif not isinstance(description, str):
                    description = ''
                
                return {
                    'title': book.get('title', ''),
                    'subtitle': book.get('subtitle', ''),
                    'description': description,
                    'authors_list': authors_list,
                    'publisher': publisher,
                    'published_date': book.get('publish_date', ''),
                    'page_count': book.get('number_of_pages'),
                    'language': 'en',  # Default, could be enhanced
                    'categories': categories,
                    'cover_url': cover_url,
                    'openlibrary_id': book.get('key', '').replace('/books/', '') if book.get('key') else '',
                    'source': 'openlibrary'
                }
            except Exception as e:
                print(f"❌ OpenLibrary API error: {e}")
                return None
        
        # Perform API lookups
        google_data = enhanced_google_books_lookup(normalized_isbn)
        ol_data = enhanced_openlibrary_lookup(normalized_isbn)
        
        # Merge API data with priority to Google Books
        if google_data or ol_data:
            if google_data and ol_data:
                # Merge strategy: Google Books for most fields, OpenLibrary for fallbacks
                api_data = google_data.copy()
                
                # Use OpenLibrary data for missing fields
                if not api_data.get('description') and ol_data.get('description'):
                    api_data['description'] = ol_data['description']
                if not api_data.get('publisher') and ol_data.get('publisher'):
                    api_data['publisher'] = ol_data['publisher']
                if not api_data.get('page_count') and ol_data.get('page_count'):
                    api_data['page_count'] = ol_data['page_count']
                
                # Merge categories
                google_cats = set(api_data.get('categories', []))
                ol_cats = set(ol_data.get('categories', []))
                api_data['categories'] = list(google_cats.union(ol_cats))
                
                # Add OpenLibrary ID
                if ol_data.get('openlibrary_id'):
                    api_data['openlibrary_id'] = ol_data['openlibrary_id']
                
                api_data['sources'] = ['google_books', 'openlibrary']
                
            elif google_data:
                api_data = google_data
            elif ol_data:
                api_data = ol_data
            
            print(f"✅ [MANUAL] API data retrieved from {api_data.get('sources', api_data.get('source', 'unknown'))}")
            
            # Enhanced ISBN handling from API
            if not isbn13 and api_data.get('isbn13'):
                isbn13 = api_data['isbn13']
            if not isbn10 and api_data.get('isbn10'):
                isbn10 = api_data['isbn10']
            
            # Enhanced cover image handling with caching
            if api_data.get('cover_url') and not cover_url:  # Only if no manual cover URL provided
                final_cover_url = api_data['cover_url']
                
                # Download and cache cover image
                try:
                    import requests
                    from pathlib import Path
                    
                    # Create covers directory
                    covers_dir = Path('static/covers')
                    covers_dir.mkdir(exist_ok=True)
                    
                    # Generate filename
                    book_temp_id = str(uuid.uuid4())
                    file_extension = '.jpg'
                    if final_cover_url.lower().endswith('.png'):
                        file_extension = '.png'
                    elif final_cover_url.lower().endswith('.gif'):
                        file_extension = '.gif'
                    
                    filename = f"{book_temp_id}{file_extension}"
                    filepath = covers_dir / filename
                    
                    # Download image
                    response = requests.get(final_cover_url, timeout=10, stream=True)
                    response.raise_for_status()
                    
                    with open(filepath, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    
                    cached_cover_url = f"/static/covers/{filename}"
                    print(f"🖼️ [MANUAL] Cover cached: {final_cover_url} -> {cached_cover_url}")
                    
                except Exception as e:
                    print(f"❌ [MANUAL] Failed to cache cover: {e}")
                    cached_cover_url = final_cover_url  # Fallback to original URL
        else:
            print(f"❌ [MANUAL] No API data found for ISBN: {normalized_isbn}")
    
    # Enhanced field mapping with API fallbacks
    final_title = title or (api_data.get('title') if api_data else '')
    final_subtitle = subtitle if subtitle else (api_data.get('subtitle') if api_data else None)
    final_description = description or (api_data.get('description') if api_data else '')
    final_publisher = publisher_name or (api_data.get('publisher') if api_data else '')
    final_language = language or (api_data.get('language') if api_data else 'en')
    final_page_count = page_count or (api_data.get('page_count') if api_data else None)
    final_cover_url = cached_cover_url or final_cover_url or cover_url
    
    # Enhanced published date handling
    final_published_date = published_date
    if not final_published_date and api_data and api_data.get('published_date'):
        try:
            api_date_str = api_data['published_date']
            final_published_date = datetime.strptime(api_date_str, '%Y-%m-%d').date()
        except ValueError:
            try:
                final_published_date = datetime.strptime(api_date_str, '%Y').date()
            except ValueError:
                pass

    # Enhanced category processing with API integration
    final_categories = []
    
    # Add API categories first
    if api_data and api_data.get('categories'):
        api_categories = api_data['categories']
        if isinstance(api_categories, list):
            final_categories.extend(api_categories)
        elif isinstance(api_categories, str):
            # Handle comma-separated string categories
            final_categories.extend([cat.strip() for cat in api_categories.split(',') if cat.strip()])
    
    # Add manual categories
    final_categories.extend(manual_categories)
    
    # Remove duplicates while preserving order and normalize
    seen = set()
    unique_categories = []
    for cat in final_categories:
        cat_normalized = cat.lower().strip()
        if cat_normalized and cat_normalized not in seen:
            unique_categories.append(cat.strip())
            seen.add(cat_normalized)
    
    print(f"📚 [MANUAL] Final categories (API + manual): {unique_categories}")
    
    # Use manual publisher if provided, otherwise use API publisher
    final_publisher = publisher_name or publisher

    try:
        # Enhanced contributors creation from API data
        contributors = []
        
        # Prioritize individual authors from enhanced API data
        if api_data and api_data.get('authors_list'):
            for i, author_name in enumerate(api_data['authors_list']):
                author_name = author_name.strip()
                if author_name:
                    person = Person(id=str(uuid.uuid4()), name=author_name)
                    contribution = BookContribution(
                        person=person,
                        contribution_type=ContributionType.AUTHORED,
                        order=i
                    )
                    contributors.append(contribution)
                    print(f"👤 [MANUAL] Added author from API: {author_name}")
        elif author:
            # Fallback to manual form entry
            person = Person(id=str(uuid.uuid4()), name=author)
            contribution = BookContribution(
                person=person,
                contribution_type=ContributionType.AUTHORED,
                order=0
            )
            contributors.append(contribution)
            print(f"👤 [MANUAL] Added author from form: {author}")
            
        # Enhanced domain book object creation with comprehensive field mapping
        domain_book = DomainBook(
            id=str(uuid.uuid4()),
            title=final_title,
            subtitle=final_subtitle,
            description=final_description if final_description else None,
            published_date=final_published_date,
            page_count=final_page_count,
            language=final_language,
            cover_url=final_cover_url,
            isbn13=isbn13,
            isbn10=isbn10,
            asin=api_data.get('asin') if api_data else None,
            google_books_id=api_data.get('google_books_id') if api_data else None,
            openlibrary_id=api_data.get('openlibrary_id') if api_data else None,
            average_rating=api_data.get('average_rating') if api_data else None,
            rating_count=api_data.get('rating_count') if api_data else None,
            contributors=contributors,
            publisher=Publisher(id=str(uuid.uuid4()), name=final_publisher) if final_publisher else None,
            series=Series(id=str(uuid.uuid4()), name=series) if series else None,
            series_volume=series_volume if series_volume else None,
            series_order=series_order,
            categories=unique_categories or [],
            raw_categories=unique_categories,
            created_at=datetime.now(),
            updated_at=datetime.now()
        )
        
        # Use find_or_create_book to avoid duplicates (global)
        existing_book = book_service.find_or_create_book_sync(domain_book)
        
        # Categories are already processed by find_or_create_book_sync, no need to process again
        if unique_categories:
            print(f"📚 [MANUAL] Categories already processed during book creation: {unique_categories}")
        else:
            print(f"📚 [MANUAL] No categories found for book {title}")
        
        # Add to user's library with custom metadata and location
        # Determine location to use: form-selected location takes priority, then default location
        final_locations = []
        try:
            from .location_service import LocationService
            from .infrastructure.kuzu_graph import get_kuzu_connection
            from config import Config
            
            kuzu_connection = get_kuzu_connection()
            location_service = LocationService(kuzu_connection.connect())
            
            # Check if user selected a location in the form
            if location_id:
                print(f"📍 [MANUAL] User selected location from form: {location_id}")
                final_locations = [location_id]
            else:
                print(f"📍 [MANUAL] No location selected in form, attempting to get default location for user {current_user.id}")
                
                default_location = location_service.get_default_location(str(current_user.id))
                
                if default_location:
                    final_locations = [default_location.id]
                    print(f"📍 [MANUAL] ✅ Found default location: {default_location.name} (ID: {default_location.id})")
                else:
                    print(f"📍 [MANUAL] ❌ No default location found for user {current_user.id}")
                    # Check if user has any locations at all
                    all_locations = location_service.get_user_locations(str(current_user.id))
                    if not all_locations:
                        print(f"📍 [MANUAL] 🏗️ User has no locations, creating default location...")
                        default_locations_created = location_service.setup_default_locations(str(current_user.id))
                        if default_locations_created:
                            final_locations = [default_locations_created[0].id]
                            print(f"📍 [MANUAL] ✅ Created and assigned default location: {default_locations_created[0].name} (ID: {default_locations_created[0].id})")
                        else:
                            print(f"📍 [MANUAL] ❌ Failed to create default locations")
                    else:
                        print(f"📍 [MANUAL] User has {len(all_locations)} locations but none are default")
                
        except Exception as e:
            print(f"❌ [MANUAL] Error handling location: {e}")
            import traceback
            traceback.print_exc()
        
        print(f"📍 [MANUAL] Final locations list: {final_locations}")
        
        # Convert reading status string to enum
        reading_status_enum = ReadingStatus.PLAN_TO_READ  # Default
        if reading_status:
            try:
                reading_status_enum = ReadingStatus(reading_status)
            except ValueError:
                print(f"⚠️ [MANUAL] Invalid reading status: {reading_status}, using default")
        
        # Convert ownership status string to enum
        ownership_status_enum = None
        if ownership_status:
            try:
                from .domain.models import OwnershipStatus
                ownership_status_enum = OwnershipStatus(ownership_status)
            except ValueError:
                print(f"⚠️ [MANUAL] Invalid ownership status: {ownership_status}")
        
        # Convert media type string to enum
        media_type_enum = None
        if media_type:
            try:
                from .domain.models import MediaType
                media_type_enum = MediaType(media_type)
            except ValueError:
                print(f"⚠️ [MANUAL] Invalid media type: {media_type}")
        
        print(f"📚 [MANUAL] Adding book to user library with reading status: {reading_status_enum}")
        result = book_service.add_book_to_user_library_sync(
            user_id=current_user.id,
            book_id=existing_book.id,
            reading_status=reading_status_enum,
            locations=final_locations,
            custom_metadata=custom_metadata if custom_metadata else None
        )
        print(f"📚 [MANUAL] Add to library result: {result}")
        
        # Update additional fields if specified
        update_data = {}
        if ownership_status_enum:
            update_data['ownership_status'] = ownership_status_enum
        if media_type_enum:
            update_data['media_type'] = media_type_enum
        if user_rating:
            update_data['user_rating'] = user_rating
        if personal_notes:
            update_data['personal_notes'] = personal_notes
        if user_tags:
            update_data['user_tags'] = user_tags
        if start_date:
            update_data['start_date'] = start_date
        if finish_date:
            update_data['finish_date'] = finish_date
        # Note: location is already handled via the locations parameter above
            
        if update_data:
            print(f"📚 [MANUAL] Updating book with additional data: {update_data}")
            book_service.update_book_sync(existing_book.uid, str(current_user.id), **update_data)
        
        if existing_book.id == domain_book.id:
            # New book was created
            if custom_metadata:
                flash(f'Book "{title}" added successfully with {len(custom_metadata)} custom fields.', 'success')
            else:
                flash(f'Book "{title}" added successfully to your library.', 'success')
        else:
            # Existing book was found
            if custom_metadata:
                flash(f'Book "{title}" already exists. Added to your collection with {len(custom_metadata)} custom fields.', 'info')
            else:
                flash(f'Book "{title}" already exists in the library. Added to your collection.', 'info')
            
    except Exception as e:
        current_app.logger.error(f"Error adding book manually: {e}")
        flash('An error occurred while adding the book. Please try again.', 'danger')

    return redirect(url_for('main.library'))

@bp.route('/import-books', methods=['GET', 'POST'])
@login_required
def import_books():
    """New unified import interface."""
    if request.method == 'POST':
        # Check if user wants to force custom mapping (bypass template detection)
        force_custom = request.form.get('force_custom', 'false').lower() == 'true'
        
        # Check if we're coming from confirmation screen with existing file
        existing_csv_path = request.form.get('csv_file_path')
        
        if existing_csv_path and force_custom:
            # Reuse existing CSV file for custom mapping
            temp_path = existing_csv_path
        else:
            # Handle new file upload and show field mapping
            file = request.files.get('csv_file')
            if not file or not file.filename.endswith('.csv'):
                flash('Please upload a valid CSV file.', 'danger')
                return redirect(url_for('main.import_books'))
            
            # Save file temporarily
            filename = secure_filename(file.filename)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.csv', prefix=f'import_{current_user.id}_')
            file.save(temp_file.name)
            temp_path = temp_file.name
        try:
            # Read CSV headers and first few rows for preview
            with open(temp_path, 'r', encoding='utf-8') as csvfile:
                # Try to detect delimiter with improved logic
                sample = csvfile.read(1024)
                csvfile.seek(0)
                
                delimiter = ','  # Default to comma
                try:
                    sniffer = csv.Sniffer()
                    delimiter = sniffer.sniff(sample).delimiter
                except Exception:
                    # If sniffer fails, try common delimiters
                    for test_delimiter in [',', ';', '\t', '|']:
                        if test_delimiter in sample:
                            delimiter = test_delimiter
                            break
                
                reader = csv.DictReader(csvfile, delimiter=delimiter)
                headers = reader.fieldnames or []
                
                # Get first 3 rows for preview
                preview_rows = []
                total_rows = 0
                for i, row in enumerate(reader):
                    if i < 3:
                        preview_rows.append([row.get(header, '') for header in headers])
                    total_rows += 1
                
                # If no headers detected, treat first column as ISBN list
                if not headers or len(headers) == 1:
                    csvfile.seek(0)
                    first_line = csvfile.readline().strip()
                    if first_line.isdigit() or len(first_line) in [10, 13]:  # Looks like ISBN
                        headers = ['ISBN']
                        csvfile.seek(0)
                        all_lines = csvfile.readlines()
                        preview_rows = [[line.strip()] for line in all_lines[:3]]
                        total_rows = len(all_lines)
            
            # Auto-detect field mappings
            suggested_mappings = auto_detect_fields(headers, current_user.id)
            
            # Check if this is a Goodreads or StoryGraph file - but still show mapping screen
            if not force_custom:
                goodreads_signatures = ['Book Id', 'Author l-f', 'Bookshelves', 'Exclusive Shelf']
                storygraph_signatures = ['Read Status', 'Moods', 'Pace', 'Character- or Plot-Driven?']
                
                is_goodreads = any(header in headers for header in goodreads_signatures)
                is_storygraph = any(header in headers for header in storygraph_signatures)
                
                if is_goodreads or is_storygraph:
                    # Store the temp file path in the session for the direct import option
                    session['direct_import_file'] = temp_path
                    if existing_csv_path:
                        session['direct_import_filename'] = 'existing_import_file.csv'
                    else:
                        session['direct_import_filename'] = file.filename
                    import_type = 'goodreads' if is_goodreads else 'storygraph'
                    
                    # Flash a message suggesting the file type was detected - but still show mapping
                    flash(f'Detected {import_type.title()} export file! We\'ve pre-mapped common fields for you. Review and customize the mappings below.', 'info')
            
            # Get custom fields for the user
            try:
                # Get user's personal and global fields separately for proper categorization
                user_fields = custom_field_service.get_user_fields_sync(current_user.id)
                global_custom_fields = [f for f in user_fields if f.is_global]
                personal_custom_fields = [f for f in user_fields if not f.is_global]
                
                # Also get shareable fields from other users
                shareable_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global=True)
                # Add shareable fields to global fields if they're not already included
                for field in shareable_fields:
                    if not any(gf.id == field.id for gf in global_custom_fields):
                        global_custom_fields.append(field)
                        
                print(f"🔧 [IMPORT] Loaded {len(global_custom_fields)} global fields and {len(personal_custom_fields)} personal fields")
            except Exception as e:
                current_app.logger.error(f"Error loading custom fields: {e}")
                global_custom_fields = []
                personal_custom_fields = []
            
            # Get import templates for the user
            try:
                import_templates = import_mapping_service.get_user_templates_sync(current_user.id)
                
                print(f"DEBUG: CSV headers: {headers}")
                print(f"DEBUG: Force custom mapping: {force_custom}")
                print(f"DEBUG: Available templates: {[t.name for t in import_templates]}")
                
                # Skip template detection if user wants custom mapping
                if force_custom:
                    detected_template = None
                    detected_template_name = None
                    print(f"DEBUG: Skipping template detection - user requested custom mapping")
                else:
                    # Detect best matching template based on headers
                    detected_template = import_mapping_service.detect_template_sync(headers, current_user.id)
                    detected_template_name = detected_template.name if detected_template else None
                    
                    print(f"DEBUG: Template detection - detected_template: {detected_template}")
                    print(f"DEBUG: Template detection - detected_template_name: {detected_template_name}")
                    if detected_template:
                        print(f"DEBUG: Template detection - template.field_mappings: {detected_template.field_mappings}")
                        print(f"DEBUG: Template detection - field_mappings type: {type(detected_template.field_mappings)}")
                        print(f"DEBUG: Template detection - field_mappings bool: {bool(detected_template.field_mappings)}")
                
                # If a default system template was detected, auto-create fields but still show mapping UI for review
                if not force_custom and detected_template and detected_template.user_id == "__system__" and detected_template.field_mappings:
                    print(f"DEBUG: System template detected - {detected_template.name}")
                    print(f"DEBUG: Auto-creating custom fields from template mappings: {detected_template.field_mappings}")
                    
                    # Auto-create any custom fields referenced in the template BEFORE showing mapping screen
                    auto_create_custom_fields(detected_template.field_mappings, current_user.id)
                    
                    # Reload custom fields after creating new ones so they appear in the mapping screen
                    user_fields = custom_field_service.get_user_fields_sync(current_user.id)
                    global_custom_fields = [f for f in user_fields if f.is_global]
                    personal_custom_fields = [f for f in user_fields if not f.is_global]
                    
                    # Use template mappings as suggested mappings
                    suggested_mappings = detected_template.field_mappings.copy()
                    print(f"DEBUG: Using template mappings as suggestions: {suggested_mappings}")
                    
                    # Add the detected template to the import_templates list so it appears in dropdown
                    if detected_template not in import_templates:
                        import_templates.append(detected_template)
                        print(f"DEBUG: Added detected template to dropdown: {detected_template.name}")
                
                # If a custom template was detected, use its mappings
                elif detected_template and detected_template.field_mappings:
                    print(f"DEBUG: Custom template detected - Using template mappings")
                    suggested_mappings = detected_template.field_mappings.copy()
                    print(f"DEBUG: Using template mappings: {suggested_mappings}")
                    # Auto-create any custom fields referenced in the template
                    auto_create_custom_fields(suggested_mappings, current_user.id)
                    # Reload custom fields after creating new ones
                    user_fields = custom_field_service.get_user_fields_sync(current_user.id)
                    global_custom_fields = [f for f in user_fields if f.is_global]
                    personal_custom_fields = [f for f in user_fields if not f.is_global]
                    
                    # Add the detected template to the import_templates list so it appears in dropdown
                    if detected_template not in import_templates:
                        import_templates.append(detected_template)
                        print(f"DEBUG: Added detected template to dropdown: {detected_template.name}")
                else:
                    print(f"DEBUG: No template detected, using auto-detected mappings: {suggested_mappings}")
                
            except Exception as e:
                current_app.logger.error(f"Error loading import templates: {e}")
                import_templates = []
                detected_template = None
                detected_template_name = None
            
            return render_template('import_books_mapping.html',
                                 csv_file_path=temp_path,
                                 csv_headers=headers,
                                 csv_preview=preview_rows,
                                 total_rows=total_rows,
                                 suggested_mappings=suggested_mappings,
                                 global_custom_fields=global_custom_fields,
                                 personal_custom_fields=personal_custom_fields,
                                 import_templates=import_templates,
                                 detected_template=detected_template,
                                 detected_template_name=detected_template_name)
                                 
        except Exception as e:
            current_app.logger.error(f"Error processing CSV file: {e}")
            flash('Error processing CSV file. Please check the format and try again.', 'danger')
            return redirect(url_for('main.import_books'))
    
    return render_template('import_books.html')

@bp.route('/import-books/execute', methods=['POST'])
@login_required
def import_books_execute():
    """Execute the import with user-defined field mappings or template mappings."""
    csv_file_path = request.form.get('csv_file_path')
    use_template = request.form.get('use_template')
    skip_mapping = request.form.get('skip_mapping', 'false').lower() == 'true'
    
    # Initialize template saving variables
    save_as_template = False
    template_name = ''
    
    # If using a template directly (from confirmation screen)
    if use_template and skip_mapping:
        print(f"DEBUG: Executing import with template: {use_template}")
        
        # Get the template to use its mappings
        try:
            template = import_mapping_service.get_template_by_id_sync(use_template)
            if not template:
                flash('Template not found', 'error')
                return redirect(url_for('main.import_books'))
            
            print(f"DEBUG: Template mappings: {template.field_mappings}")
            
            # Convert template mappings to the format expected by import
            mappings = {}
            for csv_field, mapping_info in template.field_mappings.items():
                if isinstance(mapping_info, dict):
                    target_field = mapping_info.get('target_field')
                else:
                    target_field = mapping_info  # Fallback for string format
                
                if target_field:
                    mappings[csv_field] = target_field
            
            print(f"DEBUG: Converted mappings for import: {mappings}")
            
        except Exception as e:
            current_app.logger.error(f"Error loading template for import: {e}")
            flash('Error loading template configuration', 'error')
            return redirect(url_for('main.import_books'))
    
    else:
        # Handle manual mapping (from mapping UI)
        field_mapping = request.form.to_dict()
        
        # Extract mapping from form data
        mappings = {}
        new_fields_to_create = {}
        
        for key, value in field_mapping.items():
            if key.startswith('field_mapping[') and value:
                csv_field = key[14:-1]  # Remove 'field_mapping[' and ']'
                
                # Handle new field creation
                if value in ['create_global_field', 'create_personal_field']:
                    label_key = f'new_field_label[{csv_field}]'
                    type_key = f'new_field_type[{csv_field}]'
                    
                    field_label = request.form.get(label_key, '').strip()
                    field_type = request.form.get(type_key, 'text')
                    
                    if not field_label:
                        flash(f'Field label is required for CSV column "{csv_field}"', 'error')
                        return redirect(url_for('main.import_books'))
                    
                    # Create the custom field
                    try:
                        is_global = (value == 'create_global_field')
                        field_name = field_label.lower().replace(' ', '_').replace('-', '_')
                        
                        # Check if field already exists
                        existing_fields = custom_field_service.get_available_fields_sync(current_user.id, is_global)
                        if any(f.name == field_name for f in existing_fields):
                            flash(f'A custom field with name "{field_name}" already exists', 'error')
                            return redirect(url_for('main.import_books'))
                        
                        field_definition = CustomFieldDefinition(
                            name=field_name,
                            display_name=field_label,
                            field_type=field_type,
                            is_global=is_global,
                            created_by_user_id=current_user.id,
                            created_at=datetime.utcnow(),
                            description=f'Created during CSV import for column "{csv_field}"'
                        )
                        
                        custom_field_service.create_field(field_definition)
                        
                        # Update mapping to use the new field
                        mappings[csv_field] = f'custom_{"global" if is_global else "personal"}_{field_name}'
                        
                        flash(f'Created new {"global" if is_global else "personal"} field: {field_label}', 'success')
                        
                    except Exception as e:
                        flash(f'Error creating custom field: {str(e)}', 'error')
                        return redirect(url_for('main.import_books'))
                else:
                    mappings[csv_field] = value
        
        # Handle template saving (only for manual mapping)
        save_as_template = request.form.get('save_as_template') == 'on'
        template_name = request.form.get('template_name', '').strip()
    
    if save_as_template and template_name:
        try:
            template = ImportMappingTemplate(
                name=template_name,
                user_id=current_user.id,
                field_mappings=mappings,
                description=f'Template created from import on {datetime.now().strftime("%Y-%m-%d")}',
                created_at=datetime.utcnow()
            )
            
            import_mapping_service.create_template(template)
            flash(f'Import template "{template_name}" saved successfully!', 'success')
            
        except Exception as e:
            flash(f'Error saving template: {str(e)}', 'warning')
            # Continue with import even if template saving fails
    
    default_reading_status = request.form.get('default_reading_status', 'library_only')
    duplicate_handling = request.form.get('duplicate_handling', 'skip')
    
    # Create import job with enhanced data
    task_id = str(uuid.uuid4())
    job_data = {
        'task_id': task_id,
        'user_id': current_user.id,
        'csv_file_path': csv_file_path,
        'field_mappings': mappings,
        'default_reading_status': default_reading_status,
        'duplicate_handling': duplicate_handling,
        'custom_fields_enabled': True,  # Flag to enable custom metadata processing
        'status': 'pending',
        'processed': 0,
        'success': 0,
        'errors': 0,
        'total': 0,
        'start_time': datetime.utcnow().isoformat(),
        'current_book': None,
        'error_messages': [],
        'recent_activity': []
    }
    
    # Count total rows
    try:
        with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            job_data['total'] = sum(1 for _ in reader)
    except:
        job_data['total'] = 0
    
    # Store job data in Kuzu instead of Redis
    print(f"🏗️ [EXECUTE] Creating job {task_id} for user {current_user.id}")
    kuzu_success = store_job_in_kuzu(task_id, job_data)
    
    # Also keep in memory for backward compatibility
    import_jobs[task_id] = job_data
    
    print(f"📊 [EXECUTE] Kuzu storage: {'✅' if kuzu_success else '❌'}")
    print(f"💾 [EXECUTE] Memory storage: ✅")
    print(f"🔧 [EXECUTE] Job status: {job_data['status']}")
    print(f"📈 [EXECUTE] Total rows to process: {job_data['total']}")
    
    # Start the import in background thread
    app = current_app._get_current_object()  # Get the actual app instance
    def run_import():
        with app.app_context():
            try:
                start_import_job(task_id)
            except Exception as e:
                # Update job with error
                if task_id in import_jobs:
                    import_jobs[task_id]['status'] = 'failed'
                    if 'error_messages' not in import_jobs[task_id]:
                        import_jobs[task_id]['error_messages'] = []
                    import_jobs[task_id]['error_messages'].append(str(e))
                    app.logger.error(f"Import job {task_id} failed: {e}")
    
    # Start the import process in background
    thread = threading.Thread(target=run_import)
    thread.daemon = True
    thread.start()
    
    return redirect(url_for('main.import_books_progress', task_id=task_id))

@bp.route('/import-books/progress/<task_id>')
@login_required
def import_books_progress(task_id):
    """Show import progress page."""
    print(f"🔍 [PROGRESS] Looking for job {task_id}")
    
    # Try Kuzu first, then fall back to memory
    kuzu_job = get_job_from_kuzu(task_id)
    memory_job = import_jobs.get(task_id)
    
    print(f"📊 [PROGRESS] Kuzu job found: {bool(kuzu_job)}")
    print(f"💾 [PROGRESS] Memory job found: {bool(memory_job)}")
    
    job = kuzu_job or memory_job
    
    if not job:
        print(f"❌ [PROGRESS] No job found for {task_id}")
        print(f"📝 [PROGRESS] Available jobs in memory: {list(import_jobs.keys())}")
        flash('Import job not found. This may be due to a server restart. Please start a new import.', 'warning')
        return redirect(url_for('main.import_books'))
    
    if job['user_id'] != current_user.id:
        print(f"🚫 [PROGRESS] Job belongs to user {job['user_id']}, current user is {current_user.id}")
        flash('Access denied to this import job.', 'danger')
        return redirect(url_for('main.import_books'))
    
    print(f"✅ [PROGRESS] Job found with status: {job.get('status', 'unknown')}")
    
    return render_template('import_books_progress.html',
                         task_id=task_id,
                         total_books=job['total'],
                         start_time=job['start_time'])

@bp.route('/api/import/progress/<task_id>')
@login_required
def api_import_progress(task_id):
    """API endpoint for import progress."""
    print(f"Progress API called for task_id: {task_id} by user: {current_user.id}")
    
    # Try Kuzu first, then fall back to memory
    job = get_job_from_kuzu(task_id) or import_jobs.get(task_id)
    print(f"Kuzu job found: {bool(get_job_from_kuzu(task_id))}")
    print(f"Memory job found: {bool(import_jobs.get(task_id))}")
    print(f"Job data: {job}")
    
    if not job:
        print(f"No job found for task_id: {task_id}")
        return jsonify({'error': 'Job not found'}), 404
        
    if job['user_id'] != current_user.id:
        print(f"Job user_id {job['user_id']} doesn't match current user {current_user.id}")
        return jsonify({'error': 'Job not found'}), 404
    
    return jsonify({
        'status': job['status'],
        'processed': job['processed'],
        'success': job['success'],
        'errors': job['errors'],
        'total': job['total'],
        'current_book': job['current_book'],
        'recent_activity': job['recent_activity'][-10:],  # Last 10 activities
        'error_messages': job.get('error_messages', [])
    })

@bp.route('/api/import/errors/<task_id>')
@login_required
def api_import_errors(task_id):
    """Download error report for import job."""
    job = import_jobs.get(task_id)
    if not job or job['user_id'] != current_user.id:
        return jsonify({'error': 'Job not found'}), 404
    
    if not job['error_messages']:
        return jsonify({'error': 'No errors to download'}), 404
    
    # Create CSV of errors
    output = []
    output.append('Row,Error,Details')
    for error in job['error_messages']:
        output.append(f"{error['row']},{error['error']},{error['details']}")
    
    response = make_response('\n'.join(output))
    response.headers['Content-Type'] = 'text/csv'
    response.headers['Content-Disposition'] = f'attachment; filename=import_errors_{task_id}.csv'
    return response

@bp.route('/debug/import-jobs')
@login_required
def debug_import_jobs():
    """Debug endpoint to check import jobs (remove in production)."""
    if not current_user.is_admin:
        abort(403)
    
    return jsonify({
        'total_jobs': len(import_jobs),
        'jobs': {
            job_id: {
                'status': job['status'],
                'user_id': job['user_id'], 
                'processed': job['processed'],
                'total': job['total'],
                'start_time': job['start_time']
            }
            for job_id, job in import_jobs.items()
        }
    })

def auto_detect_fields(headers, user_id=None):
    """Auto-detect field mappings based on header names."""
    mappings = {}
    
    # Get custom fields for auto-detection if user_id is provided
    custom_fields = []
    if user_id:
        try:
            # Use sync wrapper methods to avoid coroutine issues
            personal_fields = custom_field_service.get_available_fields_sync(user_id, is_global=False)
            global_fields = custom_field_service.get_available_fields_sync(user_id, is_global=True)
            
            # Combine and format for use in detection logic
            custom_fields.extend([(f, False) for f in personal_fields])
            custom_fields.extend([(f, True) for f in global_fields])
            
        except Exception as e:
            current_app.logger.error(f"Error loading custom fields for auto-detection: {e}")
    
    for header in headers:
        header_lower = header.lower().strip()
        
        # Use EXACT same mapping as template detection for consistency
        if header == 'Book Id':
            mappings[header] = 'custom_global_goodreads_id'
        elif header == 'Title':
            mappings[header] = 'title'
        elif header == 'Author':
            mappings[header] = 'author'
        elif header == 'Author l-f':
            mappings[header] = 'ignore'
        elif header == 'Additional Authors':
            mappings[header] = 'additional_authors'
        elif header == 'ISBN':
            mappings[header] = 'isbn'
        elif header == 'ISBN13':
            mappings[header] = 'isbn13'
        elif header == 'My Rating':
            mappings[header] = 'rating'
        elif header == 'Average Rating':
            mappings[header] = 'ignore'
        elif header == 'Publisher':
            mappings[header] = 'publisher'
        elif header == 'Binding':
            mappings[header] = 'custom_global_binding'
        elif header == 'Number of Pages':
            mappings[header] = 'page_count'
        elif header == 'Year Published':
            mappings[header] = 'published_date'
        elif header == 'Original Publication Year':
            mappings[header] = 'custom_global_original_publication_year'
        elif header == 'Date Read':
            mappings[header] = 'finish_date'
        elif header == 'Date Added':
            mappings[header] = 'created_at'
        elif header == 'Bookshelves':
            mappings[header] = 'custom_global_bookshelves'
        elif header == 'Bookshelves with positions':
            mappings[header] = 'custom_global_bookshelves_with_positions'
        elif header == 'Exclusive Shelf':
            mappings[header] = 'reading_status'
        elif header == 'My Review':
            mappings[header] = 'custom_global_my_review'
        elif header == 'Spoiler':
            mappings[header] = 'custom_global_spoiler'
        elif header == 'Private Notes':
            mappings[header] = 'personal_notes'
        elif header == 'Read Count':
            mappings[header] = 'custom_global_read_count'
        elif header == 'Owned Copies':
            mappings[header] = 'ignore'
        
        # Fallback to existing auto-detection logic for non-Goodreads headers
        else:
            # Check for custom field matches first (exact and partial matches)
            custom_match_found = False
            for field, is_global in custom_fields:
                field_name_lower = field.name.lower()
                field_display_name_lower = field.display_name.lower()
                
                # Exact match on name or display_name
                if header_lower == field_name_lower or header_lower == field_display_name_lower:
                    mappings[header] = f'custom_{"global" if is_global else "personal"}_{field.name}'
                    custom_match_found = True
                    break
                
                # Partial match on display_name
                elif any(word in header_lower for word in field_display_name_lower.split()) and len(field_display_name_lower.split()) > 1:
                    mappings[header] = f'custom_{"global" if is_global else "personal"}_{field.name}'
                    custom_match_found = True
                    break
            
            if custom_match_found:
                continue
            
            # Standard field detection if no custom field match
            # Title mappings
            if any(word in header_lower for word in ['title', 'book', 'name']):
                mappings[header] = 'title'
            
            # Author mappings - prefer primary author field
            elif header_lower in ['author', 'authors']:
                mappings[header] = 'author'
            elif header_lower in ['additional authors']:
                mappings[header] = 'additional_authors'
            elif any(word in header_lower for word in ['author', 'creator', 'writer']) and 'additional' not in header_lower:
                mappings[header] = 'author'
            
            # ISBN mappings - prefer ISBN13, handle both regular and Goodreads format
            elif header_lower in ['isbn13', 'isbn/uid']:
                mappings[header] = 'isbn13'
            elif header_lower in ['isbn', 'isbn10'] and 'isbn13' not in mappings.values():
                mappings[header] = 'isbn'
            elif any(word in header_lower for word in ['upc', 'barcode']):
                mappings[header] = 'isbn'
            
            # Description mappings
            elif any(word in header_lower for word in ['description', 'summary', 'plot', 'synopsis', 'review']) and 'my review' not in header_lower:
                mappings[header] = 'description'
            
            # Publisher mappings
            elif any(word in header_lower for word in ['publisher', 'publishing', 'imprint']):
                mappings[header] = 'publisher'
            
            # Page count mappings
            elif any(word in header_lower for word in ['number of pages', 'page', 'length']) and 'pages' in header_lower:
                mappings[header] = 'page_count'
            
            # Rating mappings - prefer "My Rating" over "Average Rating"
            elif header_lower in ['my rating', 'star rating']:
                mappings[header] = 'rating'
            elif 'rating' in header_lower and 'my rating' not in [h.lower() for h in headers]:
                mappings[header] = 'rating'
            
            # Reading status mappings - handle both Goodreads and Storygraph
            elif header_lower in ['exclusive shelf', 'read status']:
                mappings[header] = 'reading_status'
            elif any(word in header_lower for word in ['status', 'shelf']) and 'read' not in header_lower:
                mappings[header] = 'reading_status'
            
            # Date mappings
            elif any(word in header_lower for word in ['date read', 'last date read', 'finished', 'completed']):
                mappings[header] = 'finish_date'
            elif any(word in header_lower for word in ['date added', 'added']):
                mappings[header] = 'created_at'
            
            # Categories/genres - exclude Goodreads bookshelves (reading statuses)
            elif any(word in header_lower for word in ['genre', 'category', 'tag', 'subject']) and 'positions' not in header_lower:
                mappings[header] = 'categories'
            
            # Language
            elif any(word in header_lower for word in ['language', 'lang']):
                mappings[header] = 'language'
            
            # Year - prefer original publication year
            elif header_lower in ['original publication year']:
                mappings[header] = 'custom_global_original_publication_date'
            elif any(word in header_lower for word in ['year published', 'year', 'published', 'publication']) and 'original' not in header_lower:
                if 'published_date' not in mappings.values():
                    mappings[header] = 'published_date'
            
            # Notes/Review - prefer private notes over public review
            elif header_lower in ['private notes']:
                mappings[header] = 'personal_notes'
            elif any(word in header_lower for word in ['note', 'comment']) and 'private notes' not in [h.lower() for h in headers]:
                mappings[header] = 'personal_notes'
    
    return mappings


def auto_create_custom_fields(mappings, user_id):
    """Auto-create custom fields that are referenced in mappings but don't exist."""
    from app.domain.models import CustomFieldDefinition, CustomFieldType
    from app.services import custom_field_service
    from datetime import datetime
    
    print(f"DEBUG: auto_create_custom_fields called with mappings: {mappings}")
    print(f"DEBUG: mappings type: {type(mappings)}")
    
    # Get existing fields
    existing_global_fields = custom_field_service.get_available_fields_sync(user_id, is_global=True)
    existing_personal_fields = custom_field_service.get_available_fields_sync(user_id, is_global=False)
    
    existing_global_names = {f.name for f in existing_global_fields}
    existing_personal_names = {f.name for f in existing_personal_fields}
    
    # Predefined field configurations for common platform-specific metadata
    FIELD_CONFIGS = {
        # Goodreads fields - matches template detection exactly
        'goodreads_id': {'display_name': 'Goodreads_ID', 'type': CustomFieldType.NUMBER, 'global': True},
        'binding': {'display_name': 'Binding', 'type': CustomFieldType.TEXT, 'global': True},
        'original_publication_year': {'display_name': 'Original Publication Year', 'type': CustomFieldType.NUMBER, 'global': True},
        'bookshelves': {'display_name': 'Bookshelves', 'type': CustomFieldType.TAGS, 'global': True},
        'bookshelves_with_positions': {'display_name': 'Bookshelves with Positions', 'type': CustomFieldType.TEXTAREA, 'global': True},
        'my_review': {'display_name': 'My Review', 'type': CustomFieldType.TEXTAREA, 'global': True},
        'spoiler': {'display_name': 'Spoiler', 'type': CustomFieldType.BOOLEAN, 'global': True},
        'read_count': {'display_name': 'Read Count', 'type': CustomFieldType.NUMBER, 'global': True},
        
        # Legacy fields (keep for backward compatibility)
        'original_publication_date': {'display_name': 'Original Publication Date', 'type': CustomFieldType.DATE, 'global': True},
        'goodreads_book_id': {'display_name': 'Goodreads Book ID', 'type': CustomFieldType.TEXT, 'global': True},
        'average_rating': {'display_name': 'Average Rating', 'type': CustomFieldType.NUMBER, 'global': True},
        'spoiler_review': {'display_name': 'Contains Spoilers', 'type': CustomFieldType.BOOLEAN, 'global': True},
        'private_notes': {'display_name': 'Private Notes', 'type': CustomFieldType.TEXTAREA, 'global': False},
        'read_count': {'display_name': 'Number of Times Read', 'type': CustomFieldType.NUMBER, 'global': True},
        'owned_copies': {'display_name': 'Number of Owned Copies', 'type': CustomFieldType.NUMBER, 'global': False},
        
        # StoryGraph fields
        'format': {'display_name': 'Format', 'type': CustomFieldType.TEXT, 'global': True},
        'moods': {'display_name': 'Moods', 'type': CustomFieldType.TAGS, 'global': True},
        'pace': {'display_name': 'Reading Pace', 'type': CustomFieldType.TEXT, 'global': True},
        'character_plot_driven': {'display_name': 'Character or Plot Driven', 'type': CustomFieldType.TEXT, 'global': True},
        'strong_character_development': {'display_name': 'Strong Character Development', 'type': CustomFieldType.BOOLEAN, 'global': True},
        'loveable_characters': {'display_name': 'Loveable Characters', 'type': CustomFieldType.BOOLEAN, 'global': True},
        'diverse_characters': {'display_name': 'Diverse Characters', 'type': CustomFieldType.BOOLEAN, 'global': True},
        'flawed_characters': {'display_name': 'Flawed Characters', 'type': CustomFieldType.BOOLEAN, 'global': True},
        'content_warnings': {'display_name': 'Content Warnings', 'type': CustomFieldType.TAGS, 'global': True},
        'content_warning_description': {'display_name': 'Content Warning Description', 'type': CustomFieldType.TEXTAREA, 'global': True},
        'owned': {'display_name': 'Owned', 'type': CustomFieldType.BOOLEAN, 'global': False},
    }
    
    # Check mappings for custom fields that need to be created
    for csv_field, book_field_info in mappings.items():
        # Handle the new structure where mappings are dictionaries with 'target_field'
        if isinstance(book_field_info, dict):
            book_field = book_field_info.get('target_field', '')
        else:
            # Fallback for simple string mappings
            book_field = book_field_info
        
        # Ensure book_field is a string
        if not isinstance(book_field, str):
            continue
            
        if book_field.startswith('custom_global_'):
            field_name = book_field[14:]  # Remove 'custom_global_' prefix
            if field_name not in existing_global_names:
                # Create the global field
                config = FIELD_CONFIGS.get(field_name, {
                    'display_name': field_name.replace('_', ' ').title(),
                    'type': CustomFieldType.TEXT,
                    'global': True
                })
                
                field_definition = CustomFieldDefinition(
                    name=field_name,
                    display_name=config['display_name'],
                    field_type=config['type'],
                    is_global=True,
                    created_by_user_id=user_id,
                    created_at=datetime.utcnow(),
                    description=f'Auto-created for CSV column "{csv_field}"'
                )
                
                try:
                    custom_field_service.create_field(field_definition)
                    print(f"Auto-created global custom field: {config['display_name']} ({field_name})")
                except Exception as e:
                    print(f"Error auto-creating global field {field_name}: {e}")
        
        elif book_field.startswith('custom_personal_'):
            field_name = book_field[16:]  # Remove 'custom_personal_' prefix
            if field_name not in existing_personal_names:
                # Create the personal field
                config = FIELD_CONFIGS.get(field_name, {
                    'display_name': field_name.replace('_', ' ').title(),
                    'type': CustomFieldType.TEXT,
                    'global': False
                })
                
                field_definition = CustomFieldDefinition(
                    name=field_name,
                    display_name=config['display_name'],
                    field_type=config['type'],
                    is_global=False,
                    created_by_user_id=user_id,
                    created_at=datetime.utcnow(),
                    description=f'Auto-created for CSV column "{csv_field}"'
                )
                
                try:
                    custom_field_service.create_field(field_definition)
                    print(f"Auto-created personal custom field: {config['display_name']} ({field_name})")
                except Exception as e:
                    print(f"Error auto-creating personal field {field_name}: {e}")


def start_import_job(task_id):
    """Start the actual import process with improved entity-first architecture."""
    from app.domain.models import Book as DomainBook, Author, Publisher
    from app.services import book_service
    
    print(f"🚀 [START] Starting import job {task_id}")
    
    # Try to get job from both sources
    kuzu_job = get_job_from_kuzu(task_id)
    memory_job = import_jobs.get(task_id)
    
    print(f"📊 [START] Kuzu job found: {bool(kuzu_job)}")
    print(f"💾 [START] Memory job found: {bool(memory_job)}")
    
    job = kuzu_job or memory_job
    if not job:
        print(f"❌ [START] Import job {task_id} not found in start_import_job")
        return

    print(f"✅ [START] Starting import job {task_id} for user {job['user_id']}")
    job['status'] = 'running'
    update_job_in_kuzu(task_id, {'status': 'running'})
    if task_id in import_jobs:
        import_jobs[task_id]['status'] = 'running'

    try:
        csv_file_path = job['csv_file_path']
        mappings = job['field_mappings']
        user_id = job['user_id']
        
        print(f"Processing CSV file: {csv_file_path}")
        print(f"🔍 [MAPPING_DEBUG] Field mappings received: {mappings}")
        print(f"🔍 [MAPPING_DEBUG] Looking for ISBN/UID mapping: {mappings.get('ISBN/UID', 'NOT_FOUND')}")
        print(f"🔍 [MAPPING_DEBUG] All ISBN-related mappings: {[(k, v) for k, v in mappings.items() if 'isbn' in k.lower() or 'uid' in k.lower()]}")
        print(f"User ID: {user_id} (type: {type(user_id)})")
        
        # PHASE 1: Parse CSV and collect all data (entities + relationships)
        print(f"📋 [PHASE1] Starting CSV parsing and data collection...")
        books_to_create = []
        user_book_relationships = []
        
        # Read and process CSV
        with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
            # Try to detect if CSV has headers
            first_line = csvfile.readline().strip()
            csvfile.seek(0)  # Reset to beginning
            
            # Check if first line looks like an ISBN or a header
            has_headers = not (first_line.isdigit() or len(first_line) in [10, 13])
            
            print(f"First line: '{first_line}', Has headers: {has_headers}")
            
            if has_headers:
                reader = csv.DictReader(csvfile)
                # Debug: Check the first few rows to see actual data
                rows_list = list(reader)
                print(f"🔍 [CSV_DEBUG] Total rows in CSV: {len(rows_list)}")
                if rows_list:
                    print(f"🔍 [CSV_DEBUG] First row keys: {list(rows_list[0].keys())}")
                    print(f"🔍 [CSV_DEBUG] First row sample data: {dict(list(rows_list[0].items())[:5])}")  # First 5 fields
                    isbn_uid_value = rows_list[0].get('ISBN/UID', 'KEY_NOT_FOUND')
                    print(f"🔍 [CSV_DEBUG] First row ISBN/UID value: '{isbn_uid_value}' (type: {type(isbn_uid_value)})")
                reader = iter(rows_list)  # Convert back to iterator
            else:
                # For headerless CSV (like ISBN-only files), create a simple reader
                reader = csv.reader(csvfile)
                # Convert to dict format for consistency
                dict_reader = []
                for row_data in reader:
                    if row_data and row_data[0].strip():  # Skip empty rows
                        dict_reader.append({'isbn': row_data[0].strip()})
                reader = dict_reader
            
            for row_num, row in enumerate(reader, 1):
                if not has_headers:
                    # For headerless CSV, row is already a dict with 'isbn' key
                    pass
                else:
                    # For CSV with headers, row is already a dict from DictReader
                    pass
                try:
                    print(f"Processing row {row_num}: {row}")
                    
                    # Extract book data based on mappings
                    book_data = {}
                    global_custom_metadata = {}
                    personal_custom_metadata = {}
                    
                    if has_headers:
                        # Use field mappings for CSV with headers
                        print(f"🔍 [FIELD_MAPPING_DEBUG] Processing {len(mappings)} field mappings")
                        for csv_field, book_field in mappings.items():
                            raw_value = row.get(csv_field, '')
                            print(f"🔍 [FIELD_MAPPING_DEBUG] Checking field '{csv_field}' -> '{book_field}', raw_value: '{raw_value}' (type: {type(raw_value)})")
                            
                            # Apply Goodreads normalization to all values
                            if book_field == 'isbn':
                                value = normalize_goodreads_value(raw_value, 'isbn')
                                print(f"🔍 [ISBN_DEBUG] Field mapping: '{csv_field}' -> '{book_field}'")
                                print(f"🔍 [ISBN_DEBUG] Raw value from CSV: '{raw_value}' (type: {type(raw_value)})")
                                print(f"🔍 [ISBN_DEBUG] Normalized value: '{value}' (type: {type(value)}, bool: {bool(value)})")
                                if not value:
                                    print(f"❌ [ISBN_DEBUG] ISBN normalization returned empty/falsy value!")
                            else:
                                value = normalize_goodreads_value(raw_value, 'text')
                                print(f"🔍 [FIELD_MAPPING_DEBUG] Normalized value: '{value}' (bool: {bool(value)})")
                            
                            if value:  # Only process non-empty values
                                if book_field == 'isbn':
                                    print(f"Cleaned ISBN from '{raw_value}' to '{value}' (length: {len(value)})")
                                    book_data[book_field] = value
                                elif book_field.startswith('custom_global_'):
                                    # Extract custom global field name
                                    field_name = book_field[14:]  # Remove 'custom_global_' prefix
                                    global_custom_metadata[field_name] = value
                                    print(f"🌍 [CUSTOM_FIELD_DEBUG] Added global custom metadata: {field_name} = '{value}' (from CSV field: '{csv_field}')")
                                elif book_field.startswith('custom_personal_'):
                                    # Extract custom personal field name  
                                    field_name = book_field[16:]  # Remove 'custom_personal_' prefix
                                    personal_custom_metadata[field_name] = value
                                    print(f"👤 [CUSTOM_FIELD_DEBUG] Added personal custom metadata: {field_name} = '{value}' (from CSV field: '{csv_field}')")
                                else:
                                    book_data[book_field] = value
                                    print(f"🔍 [FIELD_MAPPING_DEBUG] Added to book_data: {book_field} = '{value}'")
                            else:
                                print(f"🔍 [FIELD_MAPPING_DEBUG] Skipping empty/falsy value for field '{csv_field}' -> '{book_field}'")
                    else:
                        # For headerless CSV, assume it's ISBN-only
                        isbn_value = row.get('isbn', '').strip()
                        if isbn_value:
                            book_data['isbn'] = isbn_value
                            print(f"ISBN from headerless CSV: '{isbn_value}' (length: {len(isbn_value)})")
                    
                    print(f"Extracted book data: {book_data}")
                    print(f"🌍 [CUSTOM_FIELD_DEBUG] Global custom metadata summary: {len(global_custom_metadata)} fields - {list(global_custom_metadata.keys())}")
                    print(f"👤 [CUSTOM_FIELD_DEBUG] Personal custom metadata summary: {len(personal_custom_metadata)} fields - {list(personal_custom_metadata.keys())}")
                    
                    # Skip if no title or ISBN
                    if not book_data.get('title') and not book_data.get('isbn'):
                        job['recent_activity'].append(f"Row {row_num}: Skipped - no title or ISBN")
                        print(f"Row {row_num}: Skipped - no title or ISBN")
                        continue
                    
                    # For ISBN-only imports, use ISBN as title temporarily
                    title = book_data.get('title', book_data.get('isbn', 'Unknown Title'))
                    if not book_data.get('title') and book_data.get('isbn'):
                        print(f"Using ISBN as title: {title}")
                    
                    # Update current book
                    job['current_book'] = title
                    print(f"Current book: {job['current_book']}")
                    
                    # Convert reading status from CSV to our enum
                    reading_status_str = book_data.get('reading_status', '').lower()
                    reading_status = None
                    if reading_status_str:
                        print(f"Original reading status: '{reading_status_str}'")
                        if reading_status_str in ['read', 'finished']:
                            reading_status = 'read'
                        elif reading_status_str in ['currently-reading', 'reading']:
                            reading_status = 'reading'
                        elif reading_status_str in ['to-read', 'want-to-read', 'plan-to-read']:
                            reading_status = 'plan_to_read'
                        elif reading_status_str in ['on-hold', 'paused']:
                            reading_status = 'on_hold'
                        elif reading_status_str in ['did-not-finish', 'dnf']:
                            reading_status = 'did_not_finish'
                        print(f"Converted reading status: '{reading_status}'")
                    else:
                        print("No reading status found")
                    
                    # 📝 [PERSONAL_DEBUG] Extract personal information from CSV
                    personal_info = {}
                    user_rating = None
                    personal_notes = None
                    date_read = None
                    date_added = None
                    review_text = None
                    
                    # Extract user rating (My Rating)
                    rating_raw = book_data.get('user_rating', book_data.get('my_rating', ''))
                    if rating_raw:
                        try:
                            user_rating = float(rating_raw)
                            personal_info['user_rating'] = user_rating
                            print(f"📝 [PERSONAL_DEBUG] User rating: '{rating_raw}' -> {user_rating}")
                        except (ValueError, TypeError) as e:
                            print(f"📝 [PERSONAL_DEBUG] Failed to parse user rating '{rating_raw}': {e}")
                    else:
                        print(f"📝 [PERSONAL_DEBUG] No user rating found in CSV")
                    
                    # Extract personal notes
                    personal_notes = book_data.get('personal_notes', book_data.get('private_notes', ''))
                    if personal_notes:
                        personal_info['personal_notes'] = personal_notes
                        print(f"📝 [PERSONAL_DEBUG] Personal notes: '{personal_notes[:100]}{'...' if len(personal_notes) > 100 else ''}'")
                    else:
                        print(f"📝 [PERSONAL_DEBUG] No personal notes found in CSV")
                    
                    # Extract review text
                    review_text = book_data.get('my_review', book_data.get('review', ''))
                    if review_text:
                        personal_info['review_text'] = review_text
                        print(f"📝 [PERSONAL_DEBUG] Review text: '{review_text[:100]}{'...' if len(review_text) > 100 else ''}'")
                    else:
                        print(f"📝 [PERSONAL_DEBUG] No review text found in CSV")
                    
                    # Extract date read
                    date_read_raw = book_data.get('date_read', '')
                    if date_read_raw:
                        try:
                            # Try parsing the date (common formats)
                            from datetime import datetime
                            for date_format in ['%Y-%m-%d', '%Y/%m/%d', '%m/%d/%Y', '%d/%m/%Y', '%Y-%m-%d %H:%M:%S']:
                                try:
                                    date_read = datetime.strptime(date_read_raw, date_format).date()
                                    personal_info['date_read'] = date_read
                                    print(f"📝 [PERSONAL_DEBUG] Date read: '{date_read_raw}' -> {date_read}")
                                    break
                                except ValueError:
                                    continue
                            if not date_read:
                                print(f"📝 [PERSONAL_DEBUG] Failed to parse date read '{date_read_raw}' - unsupported format")
                        except Exception as e:
                            print(f"📝 [PERSONAL_DEBUG] Error parsing date read '{date_read_raw}': {e}")
                    else:
                        print(f"📝 [PERSONAL_DEBUG] No date read found in CSV")
                    
                    # Extract date added
                    date_added_raw = book_data.get('date_added', '')
                    if date_added_raw:
                        try:
                            # Try parsing the date (common formats)
                            for date_format in ['%Y-%m-%d', '%Y/%m/%d', '%m/%d/%Y', '%d/%m/%Y', '%Y-%m-%d %H:%M:%S']:
                                try:
                                    date_added = datetime.strptime(date_added_raw, date_format).date()
                                    personal_info['date_added'] = date_added
                                    print(f"📝 [PERSONAL_DEBUG] Date added: '{date_added_raw}' -> {date_added}")
                                    break
                                except ValueError:
                                    continue
                            if not date_added:
                                print(f"📝 [PERSONAL_DEBUG] Failed to parse date added '{date_added_raw}' - unsupported format")
                        except Exception as e:
                            print(f"📝 [PERSONAL_DEBUG] Error parsing date added '{date_added_raw}': {e}")
                    else:
                        print(f"📝 [PERSONAL_DEBUG] No date added found in CSV")
                    
                    print(f"📝 [PERSONAL_DEBUG] Personal info summary: {len(personal_info)} fields - {list(personal_info.keys())}")
                    
                    # Prepare ISBN fields
                    isbn_value = book_data.get('isbn', '')
                    isbn13 = None
                    isbn10 = None
                    
                    if isbn_value:
                        # Clean ISBN (remove spaces, hyphens, etc.)
                        clean_isbn = ''.join(c for c in isbn_value if c.isdigit() or c.upper() == 'X')
                        print(f"ISBN processing: original='{isbn_value}', cleaned='{clean_isbn}', length={len(clean_isbn)}")
                        
                        if len(clean_isbn) == 13:
                            isbn13 = clean_isbn
                            print(f"Assigned to ISBN13: {isbn13}")
                        elif len(clean_isbn) == 10:
                            isbn10 = clean_isbn
                            print(f"Assigned to ISBN10: {isbn10}")
                        else:
                            current_app.logger.warning(f"ISBN '{clean_isbn}' has unexpected length {len(clean_isbn)}")
                            # Still try to use it, in case it's a valid format we don't recognize
                            if len(clean_isbn) > 10:
                                isbn13 = clean_isbn
                            else:
                                isbn10 = clean_isbn
                    
                    print(f"Final ISBN assignment: isbn13='{isbn13}', isbn10='{isbn10}'")
                    
                    # Fetch additional metadata if ISBN is available
                    # Start with CSV data as base
                    csv_title = title
                    csv_author = book_data.get('author', '')
                    description = book_data.get('description')
                    publisher_name = book_data.get('publisher')
                    page_count = book_data.get('page_count')
                    language = book_data.get('language', 'en')
                    cover_url = None
                    average_rating = None
                    rating_count = None
                    published_date = None
                    google_data = None  # Initialize google_data
                    ol_data = None  # Initialize ol_data
                    api_categories = None  # Initialize categories from API
                    
                    # Override with API data if available (prioritize API data for richer metadata)
                    # Use the cleaned ISBN for API calls
                    api_isbn = isbn13 or isbn10  # Prefer ISBN13, fallback to ISBN10
                    if api_isbn:
                        print(f"Fetching metadata for cleaned ISBN: {api_isbn}")
                        try:
                            # Try Google Books first
                            google_data = get_google_books_cover(api_isbn, fetch_title_author=True)
                            if google_data:
                                print(f"Got Google Books data: {google_data.keys()}")
                                # Prioritize API title over CSV title if available
                                if google_data.get('title'):
                                    title = google_data['title']
                                    print(f"Updated title from API: {title}")
                                # Prioritize API author over CSV author if available  
                                if google_data.get('author'):
                                    csv_author = google_data['author']
                                    print(f"Updated author from API: {csv_author}")
                                # Get additional metadata
                                description = description or google_data.get('description')
                                # Clean publisher name (remove quotes)
                                api_publisher = google_data.get('publisher')
                                if api_publisher:
                                    api_publisher = api_publisher.strip('"\'')  # Remove quotes
                                publisher_name = publisher_name or api_publisher
                                page_count = page_count or google_data.get('page_count')
                                language = language or google_data.get('language', 'en')
                                cover_url = google_data.get('cover')  # Use 'cover' key not 'cover_url'
                                average_rating = google_data.get('average_rating')
                                rating_count = google_data.get('rating_count')
                                published_date = google_data.get('published_date')
                                # Extract categories from Google Books
                                api_categories = google_data.get('categories')
                                if api_categories:
                                    print(f"Got categories from Google Books: {api_categories}")
                                if cover_url:
                                    print(f"Got cover URL from Google: {cover_url}")
                            else:
                                # Fallback to OpenLibrary
                                print("No Google Books data, trying OpenLibrary...")
                                ol_data = fetch_book_data(api_isbn)
                                if ol_data:
                                    print(f"Got OpenLibrary data: {ol_data.keys()}")
                                    # Prioritize API title over CSV title if available
                                    if ol_data.get('title'):
                                        title = ol_data['title']
                                        print(f"Updated title from OpenLibrary: {title}")
                                    if ol_data.get('author'):
                                        csv_author = ol_data['author']
                                        print(f"Updated author from OpenLibrary: {csv_author}")
                                    description = description or ol_data.get('description')
                                    # Clean publisher name (remove quotes)
                                    api_publisher = ol_data.get('publisher')
                                    if api_publisher:
                                        api_publisher = api_publisher.strip('"\'')  # Remove quotes
                                    publisher_name = publisher_name or api_publisher
                                    page_count = page_count or ol_data.get('page_count')
                                    language = language or ol_data.get('language', 'en')
                                    published_date = published_date or ol_data.get('published_date')
                                    # Extract categories from OpenLibrary (if not already from Google)
                                    if not api_categories:
                                        api_categories = ol_data.get('categories')
                                        if api_categories:
                                            print(f"Got categories from OpenLibrary: {api_categories}")
                        except Exception as api_error:
                            print(f"Error fetching metadata for ISBN {api_isbn}: {api_error}")
                    else:
                        print("No valid ISBN available for API lookup")
                    
                    # Determine final categories to use (prioritize CSV then API)
                    final_categories = book_data.get('categories') or api_categories
                    if final_categories:
                        print(f"Final categories to process: {final_categories}")
                    
                    # Rebuild authors list with updated author data
                    # Build contributors from author data with deduplication
                    contributors = []
                    added_author_names = set()  # Track added authors to prevent duplicates
                    
                    if csv_author:
                        # Handle primary author(s) from Google Books API
                        if google_data and google_data.get('authors_list'):
                            # Use individual authors from Google Books API
                            for i, author_name in enumerate(google_data['authors_list']):
                                author_name = author_name.strip()
                                if author_name and author_name.lower() not in added_author_names:
                                    person = Person(name=author_name)
                                    contribution = BookContribution(
                                        person=person,
                                        contribution_type=ContributionType.AUTHORED,
                                        order=i  # First author is primary
                                    )
                                    contributors.append(contribution)
                                    added_author_names.add(author_name.lower())
                                    print(f"Added author from Google Books: {author_name}")
                                elif author_name and author_name.lower() in added_author_names:
                                    print(f"Skipping duplicate author from Google Books: {author_name}")
                        elif ol_data and ol_data.get('authors_list'):
                            # Use individual authors from OpenLibrary API with metadata
                            for i, author_name in enumerate(ol_data['authors_list']):
                                author_name = author_name.strip()
                                if not author_name or author_name.lower() in added_author_names:
                                    if author_name:
                                        print(f"Skipping duplicate author from OpenLibrary: {author_name}")
                                    continue
                                # Fetch author metadata if available
                                author_id = None
                                ids = ol_data.get('author_ids', [])
                                if i < len(ids):
                                    author_id = ids[i]
                                meta = fetch_author_data(author_id) or {}
                                # Parse birth and death years
                                birth_year = None
                                death_year = None
                                bd = meta.get('birth_date')
                                dd = meta.get('death_date')
                                try:
                                    if bd and bd[:4].isdigit():
                                        birth_year = int(bd[:4])
                                except Exception:
                                    pass
                                try:
                                    if dd and dd[:4].isdigit():
                                        death_year = int(dd[:4])
                                except Exception:
                                    pass
                                person = Person(
                                    name=author_name,
                                    bio=meta.get('bio'),
                                    birth_year=birth_year,
                                    death_year=death_year,
                                    image_url=meta.get('photo_url')
                                )
                                contribution = BookContribution(
                                    person=person,
                                    contribution_type=ContributionType.AUTHORED,
                                    order=i  # First author is primary
                                )
                                contributors.append(contribution)
                                added_author_names.add(author_name.lower())
                                print(f"Added author from OpenLibrary: {author_name}")
                        else:
                            # Fallback to CSV author or joined API author string (only if no API authors)
                            if csv_author.lower() not in added_author_names:
                                person = Person(name=csv_author)
                                contribution = BookContribution(
                                    person=person,
                                    contribution_type=ContributionType.AUTHORED,
                                    order=0
                                )
                                contributors.append(contribution)
                                added_author_names.add(csv_author.lower())
                                print(f"Added primary author: {csv_author}")
                            else:
                                print(f"Skipping duplicate primary author: {csv_author}")
                        
                        # Handle additional authors from CSV - always check for duplicates regardless of API data
                        if book_data.get('additional_authors'):
                            additional_names = book_data['additional_authors'].split(',')
                            for name in additional_names:
                                name = name.strip()
                                if name and name.lower() not in added_author_names:
                                    person = Person(name=name)
                                    contribution = BookContribution(
                                        person=person,
                                        contribution_type=ContributionType.AUTHORED,
                                        order=len(contributors)  # Continue ordering
                                    )
                                    contributors.append(contribution)
                                    added_author_names.add(name.lower())
                                    print(f"Added additional author: {name}")
                                elif name and name.lower() in added_author_names:
                                    print(f"Skipping duplicate additional author: {name}")
                    
                    # Handle Contributors column from StoryGraph with deduplication
                    contributors_str = book_data.get('contributors', '').strip()
                    if contributors_str and contributors_str != '""' and contributors_str != '':
                        print(f"Processing Contributors field: '{contributors_str}'")
                        # Split contributors by comma and process each one
                        contributor_names = [name.strip() for name in contributors_str.split(',')]
                        for i, name_with_role in enumerate(contributor_names):
                            if name_with_role and name_with_role != '""':  # Skip empty or quoted empty strings
                                # Parse name and role - handle format like "David Wyatt (Contributor)"
                                name = name_with_role
                                contribution_type = ContributionType.CONTRIBUTED  # Default
                                
                                # Check if role is specified in parentheses
                                if '(' in name_with_role and ')' in name_with_role:
                                    # Extract name and role
                                    name = name_with_role.split('(')[0].strip()
                                    role = name_with_role.split('(')[1].split(')')[0].strip().lower()
                                    
                                    # Map role to contribution type
                                    if 'translator' in role or 'translation' in role:
                                        contribution_type = ContributionType.TRANSLATED
                                    elif 'editor' in role or 'edited' in role:
                                        contribution_type = ContributionType.EDITED
                                    elif 'illustrator' in role or 'illustrated' in role:
                                        contribution_type = ContributionType.ILLUSTRATED
                                    elif 'narrator' in role or 'narrated' in role:
                                        contribution_type = ContributionType.NARRATED
                                    elif 'foreword' in role:
                                        contribution_type = ContributionType.GAVE_FOREWORD
                                    elif 'introduction' in role:
                                        contribution_type = ContributionType.GAVE_INTRODUCTION
                                    elif 'afterword' in role:
                                        contribution_type = ContributionType.GAVE_AFTERWORD
                                    # else: keep default CONTRIBUTED
                                    
                                    print(f"Parsed contributor: name='{name}', role='{role}', type={contribution_type.value}")
                                
                                # Check for duplicates before adding
                                if name.lower() not in added_author_names:
                                    person = Person(name=name)
                                    contribution = BookContribution(
                                        person=person,
                                        contribution_type=contribution_type,
                                        order=len(contributors)  # Continue ordering after authors
                                    )
                                    contributors.append(contribution)
                                    added_author_names.add(name.lower())  # Track this contributor too
                                    print(f"Added contributor: {name} ({contribution_type.value})")
                                else:
                                    print(f"Skipping duplicate contributor: {name}")
                    else:
                        print(f"No contributors found or empty contributors field: '{contributors_str}'")
                    
                    print(f"Final metadata - Title: {title}, Contributors: {len(contributors)}, Cover: {cover_url}, Publisher: {publisher_name}")
                    
                    # Clean publisher name one more time to be safe
                    if publisher_name:
                        publisher_name = publisher_name.strip('"\'')
                    
                    domain_book = DomainBook(
                        title=title,
                        contributors=contributors,
                        isbn13=isbn13,
                        isbn10=isbn10,
                        description=description,
                        publisher=Publisher(name=publisher_name) if publisher_name else None,
                        page_count=int(page_count) if page_count and str(page_count).isdigit() else None,
                        language=language,
                        cover_url=cover_url,
                        average_rating=average_rating,
                        rating_count=rating_count,
                        published_date=_convert_published_date_to_date(published_date),
                        raw_categories=final_categories,  # Use raw_categories for processing
                        custom_metadata=global_custom_metadata,  # Add global custom metadata
                        created_at=datetime.now(),
                        updated_at=datetime.now()
                    )
                    
                    print(f"Created domain book: title='{domain_book.title}', contributors={len(domain_book.contributors)}, isbn13='{domain_book.isbn13}', isbn10='{domain_book.isbn10}'")
                    
                    # Create book using the service (globally)
                    print(f"Creating book: {domain_book.title}")
                    try:
                        created_book = book_service.find_or_create_book_sync(domain_book)
                        print(f"Created book result: {created_book}")
                        print(f"Created book type: {type(created_book)}")
                        if created_book:
                            # Get user's default location for import
                            default_locations = []
                            try:
                                print(f"📍 [IMPORT] Getting default location for user {user_id}")
                                from app.location_service import LocationService
                                from app.infrastructure.kuzu_graph import get_kuzu_connection
                                from config import Config
                                
                                kuzu_connection = get_kuzu_connection()
                                location_service = LocationService(kuzu_connection.connect())
                                default_location = location_service.get_default_location(str(user_id))
                                
                                if default_location:
                                    default_locations = [default_location.id]
                                    print(f"📍 [IMPORT] ✅ Found default location: {default_location.name} (ID: {default_location.id})")
                                else:
                                    print(f"📍 [IMPORT] ❌ No default location found for user {user_id}")
                                    # Check if user has any locations at all
                                    all_locations = location_service.get_user_locations(str(user_id))
                                    if not all_locations:
                                        print(f"📍 [IMPORT] 🏗️ User has no locations, creating default location...")
                                        default_locations_created = location_service.setup_default_locations(str(user_id))
                                        if default_locations_created:
                                            default_locations = [default_locations_created[0].id]
                                            print(f"📍 [IMPORT] ✅ Created and assigned default location: {default_locations_created[0].name} (ID: {default_locations_created[0].id})")
                                        else:
                                            print(f"📍 [IMPORT] ❌ Failed to create default locations")
                                    else:
                                        print(f"📍 [IMPORT] User has {len(all_locations)} locations but none are default")
                                        
                            except Exception as loc_error:
                                print(f"❌ [IMPORT] Error getting default location: {loc_error}")
                                import traceback
                                traceback.print_exc()
                            
                            print(f"📍 [IMPORT] Final default_locations list: {default_locations}")
                            
                            # Convert reading status string to enum
                            reading_status_enum = ReadingStatus.PLAN_TO_READ  # default
                            if reading_status:
                                if reading_status == 'read':
                                    reading_status_enum = ReadingStatus.READ
                                elif reading_status == 'reading':
                                    reading_status_enum = ReadingStatus.READING
                                elif reading_status == 'plan_to_read':
                                    reading_status_enum = ReadingStatus.PLAN_TO_READ
                                elif reading_status == 'on_hold':
                                    reading_status_enum = ReadingStatus.ON_HOLD
                                elif reading_status == 'did_not_finish':
                                    reading_status_enum = ReadingStatus.DNF

                            # Add to user's library with locations
                            print(f"📚 [IMPORT] Adding book to user library with locations: {default_locations}")
                            success = book_service.add_book_to_user_library_sync(
                                user_id=user_id,
                                book_id=created_book.id,
                                reading_status=reading_status_enum,
                                ownership_status=OwnershipStatus.OWNED,
                                locations=default_locations  # Now passing locations!
                            )
                            print(f"📚 [IMPORT] Add to library result: {success}")
                            
                            # Update personal information (rating, notes, dates) if available
                            if success and personal_info:
                                try:
                                    print(f"📝 [PERSONAL_UPDATE] Processing personal information: {personal_info}")
                                    
                                    # Prepare update data for user-book relationship
                                    update_data = {}
                                    if 'user_rating' in personal_info:
                                        update_data['user_rating'] = personal_info['user_rating']
                                        print(f"📝 [PERSONAL_UPDATE] Setting user rating: {personal_info['user_rating']}")
                                    
                                    if 'personal_notes' in personal_info:
                                        update_data['personal_notes'] = personal_info['personal_notes']
                                        print(f"📝 [PERSONAL_UPDATE] Setting personal notes: {len(personal_info['personal_notes'])} characters")
                                    
                                    if 'review_text' in personal_info:
                                        # Combine with personal notes or use as personal notes if no separate notes field
                                        if 'personal_notes' not in update_data:
                                            update_data['personal_notes'] = personal_info['review_text']
                                            print(f"📝 [PERSONAL_UPDATE] Using review as personal notes: {len(personal_info['review_text'])} characters")
                                        else:
                                            # Combine review with existing notes
                                            combined_notes = f"{update_data['personal_notes']}\n\nReview: {personal_info['review_text']}"
                                            update_data['personal_notes'] = combined_notes
                                            print(f"📝 [PERSONAL_UPDATE] Combined notes and review: {len(combined_notes)} characters")
                                    
                                    if 'date_read' in personal_info:
                                        update_data['date_read'] = personal_info['date_read']
                                        print(f"📝 [PERSONAL_UPDATE] Setting date read: {personal_info['date_read']}")
                                    
                                    if 'date_added' in personal_info:
                                        update_data['date_added'] = personal_info['date_added']
                                        print(f"📝 [PERSONAL_UPDATE] Setting date added: {personal_info['date_added']}")
                                    
                                    # Update the user-book relationship with personal information
                                    if update_data:
                                        print(f"📝 [PERSONAL_UPDATE] Updating user-book with: {list(update_data.keys())}")
                                        personal_update_success = book_service.update_user_book_sync(user_id, created_book.id, **update_data)
                                        print(f"📝 [PERSONAL_UPDATE] Update result: {personal_update_success}")
                                        
                                        if personal_update_success:
                                            print(f"✅ [PERSONAL_UPDATE] Successfully updated personal information")
                                            # Verify the update by reading back the data
                                            updated_user_book = book_service.get_user_book_sync(user_id, created_book.id)
                                            if updated_user_book:
                                                print(f"📝 [PERSONAL_UPDATE] Verification - updated fields: {[k for k in update_data.keys() if hasattr(updated_user_book, k)]}")
                                        else:
                                            print(f"❌ [PERSONAL_UPDATE] Failed to update personal information")
                                            job['recent_activity'].append(f"Row {row_num}: Failed to save personal info (rating, notes, dates)")
                                    else:
                                        print(f"📝 [PERSONAL_UPDATE] No personal information to update")
                                        
                                except Exception as personal_error:
                                    print(f"❌ [PERSONAL_UPDATE] Error processing personal information: {personal_error}")
                                    import traceback
                                    traceback.print_exc()
                                    job['recent_activity'].append(f"Row {row_num}: Personal info error - {str(personal_error)}")
                            
                            # Add personal custom metadata if available
                            if success and personal_custom_metadata:
                                try:
                                    print(f"📝 [CUSTOM_META] Processing personal custom metadata: {personal_custom_metadata}")
                                    print(f"📝 [CUSTOM_META] Number of fields: {len(personal_custom_metadata)}")
                                    
                                    # Validate and save personal metadata - TODO: Implement validation
                                    validated, errors = True, []  # custom_field_service.validate_and_save_metadata(personal_custom_metadata, user_id, is_global=False)
                                    if validated:
                                        # Track field usage
                                        # Update the user-book relationship with custom metadata
                                        user_book = book_service.get_user_book_sync(user_id, created_book.id)
                                        print(f"📝 [CUSTOM_META] Found user-book relationship: {bool(user_book)}")
                                        if user_book:
                                            print(f"📝 [CUSTOM_META] Updating user-book with metadata: {personal_custom_metadata}")
                                            # Use the new update_user_book_sync method with correct parameters
                                            metadata_success = book_service.update_user_book_sync(user_id, created_book.id, custom_metadata=personal_custom_metadata)
                                            print(f"📝 [CUSTOM_META] Update result: {metadata_success}")
                                            if metadata_success:
                                                print(f"✅ [CUSTOM_META] Successfully added personal custom metadata")
                                                # Verify the metadata was saved by reading it back
                                                updated_user_book = book_service.get_user_book_sync(user_id, created_book.id)
                                                if updated_user_book and hasattr(updated_user_book, 'custom_metadata'):
                                                    print(f"📝 [CUSTOM_META] Verification - saved metadata: {updated_user_book.custom_metadata}")
                                                else:
                                                    print(f"❌ [CUSTOM_META] Verification failed - no custom_metadata attribute found")
                                            else:
                                                print(f"❌ [CUSTOM_META] Failed to update user-book relationship with custom metadata")
                                                job['recent_activity'].append(f"Row {row_num}: Failed to save custom metadata")
                                        else:
                                            print(f"❌ [CUSTOM_META] Could not find user-book relationship to add metadata")
                                            job['recent_activity'].append(f"Row {row_num}: Could not find user-book relationship for custom metadata")
                                    else:
                                        print(f"❌ [CUSTOM_META] Personal metadata validation errors: {errors}")
                                        job['recent_activity'].append(f"Row {row_num}: Custom metadata validation errors")
                                except Exception as metadata_error:
                                    print(f"❌ [CUSTOM_META] Error processing personal custom metadata: {metadata_error}")
                                    import traceback
                                    traceback.print_exc()
                                    job['recent_activity'].append(f"Row {row_num}: Custom metadata error - {str(metadata_error)}")
                            
                            # Add global custom metadata if available
                            if success and global_custom_metadata:
                                try:
                                    print(f"🌍 [GLOBAL_META] Processing global custom metadata: {global_custom_metadata}")
                                    print(f"🌍 [GLOBAL_META] Number of fields: {len(global_custom_metadata)}")
                                    # Global metadata should have been attached to the book during creation
                                    # Let's verify it was saved
                                    # TODO: Add verification and update logic if needed
                                    print(f"🌍 [GLOBAL_META] Global metadata attached to book during creation")
                                except Exception as global_metadata_error:
                                    print(f"❌ [GLOBAL_META] Error processing global custom metadata: {global_metadata_error}")
                                    import traceback
                                    traceback.print_exc()
                                    job['recent_activity'].append(f"Row {row_num}: Global metadata error - {str(global_metadata_error)}")
                            
                            if success:
                                print(f"✅ [IMPORT] Successfully added book to user library")
                            else:
                                print(f"❌ [IMPORT] Failed to add book to user library")
                            print(f"Created book ID: {created_book.id}")
                    except Exception as create_error:
                        print(f"Error creating book: {create_error}")
                        created_book = None
                    
                    # Update reading status if provided
                    if created_book and reading_status:
                        try:
                            print(f"Updating reading status to: {reading_status}")
                            update_result = book_service.update_book_sync(created_book.id, user_id, reading_status=reading_status)
                            print(f"Update result: {update_result}")
                        except Exception as update_error:
                            print(f"Failed to update reading status: {update_error}")
                    
                    if created_book:
                        job['success'] += 1
                        status_msg = f" (status: {reading_status})" if reading_status else ""
                        
                        # Build comprehensive success message with what was imported
                        success_details = []
                        if personal_info:
                            personal_items = []
                            if 'user_rating' in personal_info:
                                personal_items.append(f"rating: {personal_info['user_rating']}")
                            if 'personal_notes' in personal_info or 'review_text' in personal_info:
                                personal_items.append("notes")
                            if 'date_read' in personal_info:
                                personal_items.append("date_read")
                            if 'date_added' in personal_info:
                                personal_items.append("date_added")
                            if personal_items:
                                success_details.append(f"personal: {', '.join(personal_items)}")
                        
                        if personal_custom_metadata:
                            success_details.append(f"personal_fields: {len(personal_custom_metadata)}")
                        
                        if global_custom_metadata:
                            success_details.append(f"global_fields: {len(global_custom_metadata)}")
                        
                        details_msg = f" ({', '.join(success_details)})" if success_details else ""
                        
                        job['recent_activity'].append(f"Row {row_num}: Successfully imported '{created_book.title}'{status_msg}{details_msg}")
                        print(f"✅ [SUCCESS] Row {row_num} imported successfully{details_msg}")
                    else:
                        job['errors'] += 1
                        job['error_messages'].append({
                            'row': row_num,
                            'error': 'Failed to create book',
                            'details': 'Unknown error'
                        })
                        job['recent_activity'].append(f"Row {row_num}: Failed to import")
                        print(f"ERROR: Row {row_num} failed to import")
                
                except Exception as e:
                    print(f"Exception in row {row_num}: {e}")
                    print(f"Row data: {row}")
                    job['errors'] += 1
                    job['error_messages'].append({
                        'row': row_num,
                        'error': str(e),
                        'details': f"Data: {row}"
                    })
                    job['recent_activity'].append(f"Row {row_num}: Error - {str(e)}")
                
                job['processed'] += 1
                # Update progress in Kuzu every 10 books to avoid too many updates
                if job['processed'] % 10 == 0:
                    update_job_in_kuzu(task_id, {'processed': job['processed']})
                print(f"Row {row_num} processed. Total processed: {job['processed']}")
        
        print(f"CSV processing completed. Success: {job['success']}, Errors: {job['errors']}")
        
        # Mark as completed
        job['status'] = 'completed'
        update_job_in_kuzu(task_id, {
            'status': 'completed',
            'processed': job['processed'],
            'success': job['success'],
            'errors': job['errors']
        })
        if task_id in import_jobs:
            import_jobs[task_id].update(job)
        job['current_book'] = None
        job['recent_activity'].append(f"Import completed! {job['success']} books imported, {job['errors']} errors")
        
        # Clean up temp file
        try:
            import os
            os.unlink(csv_file_path)
        except:
            pass
            
    except Exception as e:
        job['status'] = 'failed'
        if 'error_messages' not in job:
            job['error_messages'] = []
        job['error_messages'].append(str(e))
        update_job_in_kuzu(task_id, {'status': 'failed', 'error_messages': job['error_messages']})
        if task_id in import_jobs:
            import_jobs[task_id]['status'] = 'failed'
            if 'error_messages' not in import_jobs[task_id]:
                import_jobs[task_id]['error_messages'] = []
            import_jobs[task_id]['error_messages'].append(str(e))
        print(f"Import job {task_id} failed: {e}")

def normalize_goodreads_value(value, field_type='text'):
    """
    Normalize values from Goodreads CSV exports that use Excel text formatting.
    Goodreads exports often have values like ="123456789" or ="" to force text formatting.
    """
    if not value or not isinstance(value, str):
        return value.strip() if value else ''
    
    # Remove Excel text formatting: ="value" -> value
    if value.startswith('="') and value.endswith('"'):
        value = value[2:-1]  # Remove =" prefix and " suffix
    elif value.startswith('=') and value.endswith('"'):
        value = value[1:-1]  # Remove = prefix and " suffix  
    elif value == '=""':
        value = ''  # Empty quoted value
    
    # Additional cleaning for ISBN fields
    if field_type == 'isbn':
        # Remove any remaining quotes, equals, or whitespace
        value = value.replace('"', '').replace('=', '').strip()
        # Validate that it looks like an ISBN (digits, X, hyphens only)
        if value and not all(c.isdigit() or c in 'X-' for c in value):
            # If it doesn't look like an ISBN, it might be corrupted
            current_app.logger.warning(f"Potentially corrupted ISBN value: '{value}'")
    
    return value.strip()

# Kuzu functions for job storage
def store_job_in_kuzu(task_id, job_data):
    """Store import job data in Kuzu."""
    try:
        from .kuzu_services import job_service
        success = job_service.store_job(task_id, job_data)
        if success:
            print(f"✅ Stored job {task_id} in Kuzu")
        else:
            print(f"❌ Failed to store job {task_id} in Kuzu")
        return success
    except Exception as e:
        print(f"❌ Error storing job {task_id} in Kuzu: {e}")
        return False

def get_job_from_kuzu(task_id):
    """Retrieve import job data from Kuzu."""
    try:
        from .kuzu_services import job_service
        job_data = job_service.get_job(task_id)
        if job_data:
            print(f"✅ Retrieved job {task_id} from Kuzu")
        else:
            print(f"❌ Job {task_id} not found in Kuzu")
        return job_data
    except Exception as e:
        print(f"❌ Error retrieving job {task_id} from Kuzu: {e}")
        return None

def update_job_in_kuzu(task_id, updates):
    """Update specific fields in an import job stored in Kuzu."""
    try:
        from .kuzu_services import job_service
        success = job_service.update_job(task_id, updates)
        if success:
            print(f"✅ Updated job {task_id} in Kuzu with: {list(updates.keys())}")
        else:
            print(f"❌ Failed to update job {task_id} in Kuzu")
        return success
    except Exception as e:
        print(f"❌ Error updating job {task_id} in Kuzu: {e}")
        return False

# ========================================
# PEOPLE MANAGEMENT ROUTES
# ========================================

@bp.route('/people')
@login_required
def people():
    """Display all people with management options."""
    from app.debug_system import debug_log, debug_service_call, debug_template_data
    
    try:
        debug_log(f"🔍 [PEOPLE] Starting people page for user {current_user.id}", "PEOPLE_VIEW")
        
        # Get all persons with error handling for async issues
        # Helper function to handle potential coroutine returns
        def safe_call_sync_method(method, *args, **kwargs):
            """Safely call a sync method that might return a coroutine."""
            import inspect
            result = method(*args, **kwargs)
            if inspect.iscoroutine(result):
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # Can't use loop.run_until_complete if loop is already running
                        debug_log(f"⚠️ [PEOPLE] Loop is running, method {method.__name__} returned coroutine", "PEOPLE_VIEW")
                        return []  # Return empty list as fallback
                    else:
                        return loop.run_until_complete(result)
                except Exception as e:
                    debug_log(f"⚠️ [PEOPLE] Error running coroutine for {method.__name__}: {e}", "PEOPLE_VIEW")
                    return []
            return result
        
        debug_service_call("book_service", "list_all_persons_sync", {}, None, "BEFORE")
        all_persons = safe_call_sync_method(book_service.list_all_persons_sync)
        debug_service_call("book_service", "list_all_persons_sync", {}, all_persons, "AFTER")
        
        # Ensure we have a list
        if not isinstance(all_persons, list):
            debug_log(f"⚠️ [PEOPLE] Expected list, got {type(all_persons)}", "PEOPLE_VIEW")
            all_persons = []
        
        debug_log(f"📊 [PEOPLE] Found {len(all_persons)} persons in database", "PEOPLE_VIEW")
        
        # Convert dictionaries to objects for template compatibility
        processed_persons = []
        
        # Add book counts for each person
        for i, person in enumerate(all_persons):
            debug_log(f"🔍 [PEOPLE] Processing person {i+1}/{len(all_persons)}: {person.get('name', 'unknown') if isinstance(person, dict) else getattr(person, 'name', 'unknown')}", "PEOPLE_VIEW")
            
            # Convert dictionary to object if needed
            if isinstance(person, dict):
                class PersonObj:
                    def __init__(self, data):
                        for key, value in data.items():
                            setattr(self, key, value)
                        # Initialize safe defaults
                        self.book_count = 0
                        self.contributions = {}
                
                person_obj = PersonObj(person)
                person_id = person.get('id')
                person_name = person.get('name', 'unknown')
            else:
                person_obj = person
                person_obj.book_count = 0
                person_obj.contributions = {}
                person_id = getattr(person, 'id', None)
                person_name = getattr(person, 'name', 'unknown')
            
            try:
                # Try to get books for this person with safe call
                try:
                    debug_service_call("book_service", "get_books_by_person_sync", {"person_id": person_id, "user_id": str(current_user.id)}, None, "BEFORE")
                    books_by_type = safe_call_sync_method(book_service.get_books_by_person_sync, person_id, str(current_user.id))
                    debug_service_call("book_service", "get_books_by_person_sync", {"person_id": person_id, "user_id": str(current_user.id)}, books_by_type, "AFTER")
                    
                    # Safely handle the result
                    if books_by_type and isinstance(books_by_type, dict):
                        person_obj.contributions = books_by_type
                        debug_log(f"📚 [PEOPLE] Found contributions for {person_name}: {list(books_by_type.keys())}", "PEOPLE_VIEW")
                        
                        # Calculate total books safely
                        try:
                            total_books = 0
                            for contrib_type, book_list in books_by_type.items():
                                if book_list and hasattr(book_list, '__len__'):
                                    list_length = len(book_list)
                                    total_books += list_length
                                    debug_log(f"📊 [PEOPLE] {contrib_type}: {list_length} books - {book_list}", "PEOPLE_VIEW")
                                else:
                                    debug_log(f"📊 [PEOPLE] {contrib_type}: empty or invalid - {book_list}", "PEOPLE_VIEW")
                            
                            person_obj.book_count = total_books
                            debug_log(f"📊 [PEOPLE] Total books for {person_name}: {total_books}", "PEOPLE_VIEW")
                        except Exception as count_error:
                            debug_log(f"❌ [PEOPLE] Error counting books for {person_name}: {count_error}", "PEOPLE_VIEW")
                            person_obj.book_count = 0
                    else:
                        debug_log(f"❌ [PEOPLE] No contributions found for {person_name}", "PEOPLE_VIEW")
                        person_obj.book_count = 0
                
                except Exception as book_error:
                    debug_log(f"⚠️ [PEOPLE] Error getting books for person {person_name}: {book_error}", "PEOPLE_VIEW")
                    # Keep defaults: book_count = 0, contributions = {}
                
                # Add the processed person to our list
                processed_persons.append(person_obj)
                
            except Exception as person_error:
                debug_log(f"❌ [PEOPLE] Error processing person {person_name} ({person_id}): {person_error}", "PEOPLE_VIEW")
                import traceback
                traceback.print_exc()
                # Still add the person with defaults
                processed_persons.append(person_obj)
                current_app.logger.error(f"Error processing person: {person_error}")
        
        # Sort by name safely
        try:
            processed_persons.sort(key=lambda p: getattr(p, 'name', '').lower())
            debug_log(f"✅ [PEOPLE] Sorted {len(processed_persons)} persons by name", "PEOPLE_VIEW")
        except Exception as sort_error:
            debug_log(f"⚠️ [PEOPLE] Error sorting persons: {sort_error}", "PEOPLE_VIEW")
        
        # Show summary of what we found
        try:
            total_with_books = sum(1 for p in processed_persons if getattr(p, 'book_count', 0) > 0)
            debug_log(f"📊 [PEOPLE] Summary: {len(processed_persons)} total persons, {total_with_books} with books", "PEOPLE_VIEW")
        except Exception as summary_error:
            debug_log(f"⚠️ [PEOPLE] Error calculating summary: {summary_error}", "PEOPLE_VIEW")
        
        template_data = {'persons': processed_persons}
        debug_template_data('people.html', template_data, "PEOPLE_VIEW")
        
        return render_template('people.html', persons=processed_persons)
    
    except Exception as e:
        debug_log(f"❌ [PEOPLE] Error loading people page: {e}", "PEOPLE_VIEW")
        import traceback
        traceback.print_exc()
        current_app.logger.error(f"Error loading people page: {e}")
        flash('Error loading people page.', 'error')
        return redirect(url_for('main.library'))


@bp.route('/person/<person_id>')
@login_required
def person_details(person_id):
    """Display detailed information about a person."""
    from app.debug_system import debug_log, debug_person_details, debug_service_call, debug_template_data
    
    try:
        debug_log(f"🔍 [PERSON] Starting person details page for person_id: {person_id}, user: {current_user.id}", "PERSON_DETAILS")
        
        # Get person details
        debug_log(f"🔍 [PERSON] Calling get_person_by_id_sync for person_id: {person_id}", "PERSON_DETAILS")
        debug_service_call("book_service", "get_person_by_id_sync", {"person_id": person_id}, None, "BEFORE")
        person = book_service.get_person_by_id_sync(person_id)
        debug_service_call("book_service", "get_person_by_id_sync", {"person_id": person_id}, person, "AFTER")
        
        debug_log(f"📊 [PERSON] Got person: {person}", "PERSON_DETAILS")
        debug_log(f"📊 [PERSON] Person type: {type(person)}", "PERSON_DETAILS")
        
        if not person:
            debug_log(f"❌ [PERSON] Person not found for ID: {person_id}", "PERSON_DETAILS")
            flash('Person not found.', 'error')
            return redirect(url_for('main.people'))
        
        debug_log(f"✅ [PERSON] Found person: {person.name} (ID: {person.id})", "PERSON_DETAILS")
        
        # Enhanced person debugging
        debug_person_details(person, person_id, str(current_user.id), "DETAILS_VIEW")
        
        # Get books by this person for current user
        debug_log(f"🔍 [PERSON] Getting books by person for user {current_user.id}", "PERSON_DETAILS")
        debug_service_call("book_service", "get_books_by_person_sync", {"person_id": person_id, "user_id": str(current_user.id)}, None, "BEFORE")
        books_by_type = book_service.get_books_by_person_sync(person_id, str(current_user.id))
        debug_service_call("book_service", "get_books_by_person_sync", {"person_id": person_id, "user_id": str(current_user.id)}, books_by_type, "AFTER")
        debug_log(f"📊 [PERSON] Got books_by_type: {type(books_by_type)}", "PERSON_DETAILS")
        debug_log(f"📊 [PERSON] Books by type keys: {list(books_by_type.keys()) if books_by_type else 'None'}", "PERSON_DETAILS")
        
        # Convert service objects to template-compatible format
        converted_books_by_type = {}
        if books_by_type:
            for contribution_type, books in books_by_type.items():
                debug_log(f"📋 [PERSON] {contribution_type}: {len(books)} books", "PERSON_DETAILS")
                
                # Convert each book service object to a template-compatible object
                converted_books = []
                for book in books:
                    if hasattr(book, '__dict__'):
                        # Service object - ensure it has all required attributes
                        if not hasattr(book, 'uid') and hasattr(book, 'id'):
                            book.uid = book.id
                        if not hasattr(book, 'reading_status') or book.reading_status is None:
                            book.reading_status = 'unread'
                        if not hasattr(book, 'cover_url'):
                            book.cover_url = None
                        if not hasattr(book, 'published_date'):
                            book.published_date = None
                        converted_books.append(book)
                    elif isinstance(book, dict):
                        # Convert dict to object-like structure
                        class BookObj:
                            def __init__(self, data):
                                for key, value in data.items():
                                    setattr(self, key, value)
                                # Ensure required attributes are available
                                if not hasattr(self, 'uid') and hasattr(self, 'id'):
                                    self.uid = self.id
                                if not hasattr(self, 'reading_status') or self.reading_status is None:
                                    self.reading_status = 'unread'
                                if not hasattr(self, 'cover_url'):
                                    self.cover_url = None
                                if not hasattr(self, 'published_date'):
                                    self.published_date = None
                        
                        converted_books.append(BookObj(book))
                    else:
                        # Unknown book format - create a minimal object
                        class BookObj:
                            def __init__(self, source_book):
                                self.id = getattr(source_book, 'id', None)
                                self.uid = getattr(source_book, 'uid', None) or self.id
                                self.title = getattr(source_book, 'title', 'Unknown Title')
                                self.reading_status = getattr(source_book, 'reading_status', None) or 'unread'
                                self.cover_url = getattr(source_book, 'cover_url', None)
                                self.published_date = getattr(source_book, 'published_date', None)
                        
                        converted_books.append(BookObj(book))
                
                converted_books_by_type[contribution_type] = converted_books
        
        # Prepare template data
        template_data = {
            'person': person,
            'contributions_by_type': converted_books_by_type
        }
        debug_template_data('person_details.html', template_data, "PERSON_DETAILS")
        
        debug_log(f"✅ [PERSON] Rendering template", "PERSON_DETAILS")
        return render_template('person_details.html', 
                             person=person, 
                             contributions_by_type=converted_books_by_type)
    
    except Exception as e:
        debug_log(f"❌ [PERSON] Error loading person details for {person_id}: {e}", "PERSON_DETAILS")
        import traceback
        traceback.print_exc()
        current_app.logger.error(f"Error loading person details: {e}")
        flash('Error loading person details.', 'error')
        return redirect(url_for('main.people'))


@bp.route('/person/add', methods=['GET', 'POST'])
@login_required
def add_person():
    """Add a new person to the library"""
    if request.method == 'POST':
        try:
            # Get form data
            name = request.form.get('name', '').strip()
            bio = request.form.get('bio', '').strip()
            birth_year = request.form.get('birth_year')
            death_year = request.form.get('death_year')
            birth_place = request.form.get('birth_place', '').strip()
            website = request.form.get('website', '').strip()
            
            if not name:
                flash('Name is required.', 'error')
                return render_template('add_person.html', current_year=datetime.now().year)
            
            # Convert years to integers if provided
            birth_year_int = None
            death_year_int = None
            
            if birth_year:
                try:
                    birth_year_int = int(birth_year)
                except ValueError:
                    flash('Birth year must be a valid number.', 'error')
                    return render_template('add_person.html', current_year=datetime.now().year)
            
            if death_year:
                try:
                    death_year_int = int(death_year)
                except ValueError:
                    flash('Death year must be a valid number.', 'error')
                    return render_template('add_person.html', current_year=datetime.now().year)
            
            # Validate year range
            if birth_year_int and (birth_year_int < 0 or birth_year_int > datetime.now().year):
                flash('Birth year must be valid.', 'error')
                return render_template('add_person.html', current_year=datetime.now().year)
            
            if death_year_int and (death_year_int < 0 or death_year_int > datetime.now().year):
                flash('Death year must be valid.', 'error')
                return render_template('add_person.html', current_year=datetime.now().year)
            
            if birth_year_int and death_year_int and death_year_int < birth_year_int:
                flash('Death year cannot be before birth year.', 'error')
                return render_template('add_person.html', current_year=datetime.now().year)
            
            # Create person object
            person = Person(
                id=str(uuid.uuid4()),
                name=name,
                bio=bio if bio else None,
                birth_year=birth_year_int,
                death_year=death_year_int,
                created_at=datetime.now()
            )
            
            # Store person using the repository pattern
            try:
                # Create person data compatible with repository
                person_dict = {
                    'id': person.id,
                    'name': person.name,
                    'normalized_name': person.normalized_name,
                    'bio': person.bio,
                    'birth_year': person.birth_year,
                    'death_year': person.death_year,
                    'created_at': person.created_at
                }
                
                # Use the clean repository to create the person
                from .infrastructure.kuzu_clean_repositories import CleanKuzuPersonRepository
                person_repo = CleanKuzuPersonRepository()
                created_person = person_repo.create(person_dict)
                
                if created_person:
                    flash(f'Person "{name}" added successfully!', 'success')
                    return redirect(url_for('main.person_details', person_id=person.id))
                else:
                    flash('Failed to create person. Please try again.', 'error')
                    return render_template('add_person.html', current_year=datetime.now().year)
                    
            except Exception as storage_error:
                current_app.logger.error(f"Error storing person: {storage_error}")
                flash('Error saving person to database. Please try again.', 'error')
                return render_template('add_person.html', current_year=datetime.now().year)
                
                # Use the clean repository to create the person
                from .infrastructure.kuzu_clean_repositories import CleanKuzuPersonRepository
                person_repo = CleanKuzuPersonRepository()
                created_person = person_repo.create(person_dict)
                
                if created_person:
                    flash(f'Person "{name}" added successfully!', 'success')
                    return redirect(url_for('main.person_details', person_id=person.id))
                else:
                    flash('Failed to create person. Please try again.', 'error')
                    return render_template('add_person.html', current_year=datetime.now().year)
                    
            except Exception as storage_error:
                current_app.logger.error(f"Error storing person: {storage_error}")
                flash('Error saving person to database. Please try again.', 'error')
                return render_template('add_person.html', current_year=datetime.now().year)
            
        except Exception as e:
            current_app.logger.error(f"Error adding person: {e}")
            flash('Error adding person. Please try again.', 'error')
    
    return render_template('add_person.html', current_year=datetime.now().year)


@bp.route('/person/<person_id>/edit', methods=['GET', 'POST'])
@login_required
def edit_person(person_id):
    """Edit an existing person."""
    try:
        person = book_service.get_person_by_id_sync(person_id)
        if not person:
            flash('Person not found.', 'error')
            return redirect(url_for('main.people'))
        
        if request.method == 'POST':
            # Get form data
            name = request.form.get('name', '').strip()
            bio = request.form.get('bio', '').strip()
            birth_year = request.form.get('birth_year')
            death_year = request.form.get('death_year')
            birth_place = request.form.get('birth_place', '').strip()
            website = request.form.get('website', '').strip()
            openlibrary_id = request.form.get('openlibrary_id', '').strip()
            image_url = request.form.get('image_url', '').strip()
            
            if not name:
                flash('Name is required.', 'error')
                return render_template('edit_person.html', person=person, current_year=datetime.now().year)
            
            # Convert years to integers if provided
            birth_year_int = None
            death_year_int = None
            
            if birth_year:
                try:
                    birth_year_int = int(birth_year)
                except ValueError:
                    flash('Birth year must be a valid number.', 'error')
                    return render_template('edit_person.html', person=person, current_year=datetime.now().year)
            
            if death_year:
                try:
                    death_year_int = int(death_year)
                except ValueError:
                    flash('Death year must be a valid number.', 'error')
                    return render_template('edit_person.html', person=person, current_year=datetime.now().year)
            
            # Validate year range
            if birth_year_int and (birth_year_int < 0 or birth_year_int > datetime.now().year):
                flash('Birth year must be valid.', 'error')
                return render_template('edit_person.html', person=person, current_year=datetime.now().year)
            
            if death_year_int and (death_year_int < 0 or death_year_int > datetime.now().year):
                flash('Death year must be valid.', 'error')
                return render_template('edit_person.html', person=person, current_year=datetime.now().year)
            
            if birth_year_int and death_year_int and death_year_int < birth_year_int:
                flash('Death year cannot be before birth year.', 'error')
                return render_template('edit_person.html', person=person, current_year=datetime.now().year)
            
            # Update person data
            person.name = name
            person.bio = bio if bio else None
            person.birth_year = birth_year_int
            person.death_year = death_year_int
            person.birth_place = birth_place if birth_place else None
            person.website = website if website else None
            person.openlibrary_id = openlibrary_id if openlibrary_id else None
            person.image_url = image_url if image_url else None
            person.updated_at = datetime.now()
            
            # Update normalized name
            person.normalized_name = Person._normalize_name(person.name)
            
            # Update in KuzuDB
            from .infrastructure.kuzu_graph import get_graph_storage
            storage = get_graph_storage()
            
            person_data = {
                'name': person.name,
                'normalized_name': person.normalized_name,
                'bio': person.bio,
                'birth_year': person.birth_year,
                'death_year': person.death_year,
                'birth_place': person.birth_place,
                'website': person.website,
                'openlibrary_id': person.openlibrary_id,
                'image_url': person.image_url,
                'created_at': person.created_at.isoformat() if person.created_at else datetime.now().isoformat(),
                'updated_at': person.updated_at.isoformat()
            }
            
            storage.store_node('Person', person.id, person_data)
            
            flash(f'Person "{name}" updated successfully!', 'success')
            return redirect(url_for('main.person_details', person_id=person.id))
        
        return render_template('edit_person.html', person=person, current_year=datetime.now().year)
    
    except Exception as e:
        current_app.logger.error(f"Error editing person {person_id}: {e}")
        flash('Error editing person. Please try again.', 'error')
        return redirect(url_for('main.people'))


@bp.route('/person/<person_id>/delete', methods=['POST'])
@login_required
def delete_person(person_id):
    """Delete a person (with confirmation)."""
    try:
        # Helper function to handle potential coroutine returns
        def safe_call_sync_method(method, *args, **kwargs):
            """Safely call a sync method that might return a coroutine."""
            import inspect
            result = method(*args, **kwargs)
            if inspect.iscoroutine(result):
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # Can't use loop.run_until_complete if loop is already running
                        print(f"⚠️ [DELETE_PERSON] Loop is running, method {method.__name__} returned coroutine")
                        return None  # Return None as fallback
                    else:
                        return loop.run_until_complete(result)
                except Exception as e:
                    print(f"⚠️ [DELETE_PERSON] Error running coroutine for {method.__name__}: {e}")
                    return None
            return result
        
        person = safe_call_sync_method(book_service.get_person_by_id_sync, person_id)
        if not person:
            flash('Person not found.', 'error')
            return redirect(url_for('main.people'))
        
        person_name = getattr(person, 'name', 'Unknown Person')
        
        # Check if person has associated books by directly querying the storage layer
        # This bypasses any user filtering and checks for ANY books associated with this person
        from .infrastructure.kuzu_graph import get_graph_storage
        storage = get_graph_storage()
        
        # FIRST: Clean up orphaned relationships - relationships pointing to books that no longer exist
        print(f"🧹 [DELETE_PERSON] Starting orphaned relationship cleanup for person {person_id}")
        
        # Get all user's books first to check which ones actually exist
        user_books = safe_call_sync_method(book_service.get_all_books_with_user_overlay_sync, str(current_user.id))
        if user_books is None:
            user_books = []
        
        # Create a set of valid book IDs for quick lookup
        valid_book_ids = set()
        for book in user_books:
            book_id = getattr(book, 'id', None) or getattr(book, '_id', None)
            if book_id:
                valid_book_ids.add(str(book_id))
        
        print(f"🧹 [DELETE_PERSON] User has {len(valid_book_ids)} valid books in their library")
        
        # Find all relationships that point TO this person/author (from any book)
        orphaned_relationships_found = 0
        orphaned_relationships_cleaned = 0
        
        # Get ALL books in the system (not just user's books) to check for orphaned relationships
        all_book_nodes = storage.find_nodes_by_type('book')
        print(f"🧹 [DELETE_PERSON] Checking {len(all_book_nodes)} total books for orphaned relationships")
        
        for book_data in all_book_nodes:
            if not book_data or not book_data.get('_id'):
                continue
                
            book_id = book_data.get('_id')
            if not book_id or not isinstance(book_id, str):
                continue
            
            # Check if this book actually exists in the user's library
            book_exists_in_user_library = book_id in valid_book_ids
            
            # Get ALL relationships from this book
            all_relationships = storage.get_relationships('book', book_id)
            
            # Check for relationships pointing to our person/author
            for rel in all_relationships:
                rel_type = rel.get('relationship', 'unknown')
                to_type = rel.get('to_type')
                to_id = rel.get('to_id')
                
                if ((to_type == 'author' and to_id == person_id) or 
                    (to_type == 'person' and to_id == person_id)):
                    
                    orphaned_relationships_found += 1
                    book_title = book_data.get('title', 'Unknown Title')
                    
                    # If the book doesn't exist in user's library, this is an orphaned relationship
                    if not book_exists_in_user_library:
                        print(f"🗑️ [DELETE_PERSON] Removing orphaned relationship: book '{book_title}' ({book_id}) -> {rel_type} -> {to_type}:{to_id}")
                        try:
                            # Ensure we have valid string values before calling delete_relationship
                            if to_type and to_id and isinstance(to_type, str) and isinstance(to_id, str):
                                storage.delete_relationship('book', book_id, rel_type, to_type, to_id)
                                orphaned_relationships_cleaned += 1
                            else:
                                print(f"⚠️ [DELETE_PERSON] Invalid relationship data: to_type={to_type}, to_id={to_id}")
                        except Exception as cleanup_error:
                            print(f"⚠️ [DELETE_PERSON] Failed to clean orphaned relationship: {cleanup_error}")
        
        print(f"🧹 [DELETE_PERSON] Found {orphaned_relationships_found} total relationships to person")
        print(f"🧹 [DELETE_PERSON] Cleaned {orphaned_relationships_cleaned} orphaned relationships")
        
        # NOW: Count remaining valid books that have relationships to this person/author
        total_associated_books = 0
        associated_book_details = []
        
        print(f"🔍 [DELETE_PERSON] Checking remaining relationships after cleanup")
        
        for book in user_books:
            book_id = getattr(book, 'id', None) or getattr(book, '_id', None)
            if not book_id:
                continue
            
            book_id = str(book_id)
            
            # Check ALL relationships from this book (not just WRITTEN_BY)
            all_relationships = storage.get_relationships('book', book_id)
            
            # Check if any of these relationships point to our person/author
            for rel in all_relationships:
                rel_type = rel.get('relationship', 'unknown')
                to_type = rel.get('to_type')
                to_id = rel.get('to_id')
                
                if ((to_type == 'author' and to_id == person_id) or 
                    (to_type == 'person' and to_id == person_id)):
                    
                    book_title = getattr(book, 'title', 'Unknown Title')
                    total_associated_books += 1
                    associated_book_details.append(f"{book_title} (via {rel_type} -> {to_type})")
                    print(f"🔍 [DELETE_PERSON] Found valid book '{book_title}' linked via {rel_type} -> {to_type}:{to_id}")
                    break  # Found a relationship, count this book and move to next
        
        print(f"📊 [DELETE_PERSON] Total valid associated books found: {total_associated_books}")
        if associated_book_details:
            print(f"📋 [DELETE_PERSON] Valid associated books: {associated_book_details[:5]}")  # Show first 5
        
        if total_associated_books > 0:
            flash(f'Cannot delete "{person_name}" because they are associated with {total_associated_books} books. Please consider merging with another person instead.', 'error')
            return redirect(url_for('main.person_details', person_id=person_id))
        
        # Final cleanup: Remove any remaining relationships TO this person before deletion
        print(f"🗑️ [DELETE_PERSON] Performing final cleanup of all relationships TO person {person_id}")
        
        # Find and delete ALL relationships pointing to this person (both author and person types)
        all_book_nodes = storage.find_nodes_by_type('book')
        final_cleanup_count = 0
        
        for book_data in all_book_nodes:
            if not book_data or not book_data.get('_id'):
                continue
                
            book_id = book_data.get('_id')
            if not book_id or not isinstance(book_id, str):
                continue
            
            # Get ALL relationships from this book
            all_relationships = storage.get_relationships('book', book_id)
            
            # Remove any relationship pointing to our person/author
            for rel in all_relationships:
                rel_type = rel.get('relationship', 'unknown')
                to_type = rel.get('to_type')
                to_id = rel.get('to_id')
                
                if ((to_type == 'author' and to_id == person_id) or 
                    (to_type == 'person' and to_id == person_id)):
                    
                    print(f"🗑️ [DELETE_PERSON] Final cleanup: removing {book_id} -> {rel_type} -> {to_type}:{to_id}")
                    try:
                        # Ensure we have valid string values before calling delete_relationship
                        if (rel_type and to_type and to_id and 
                            isinstance(rel_type, str) and isinstance(to_type, str) and isinstance(to_id, str)):
                            storage.delete_relationship('book', book_id, rel_type, to_type, to_id)
                            final_cleanup_count += 1
                        else:
                            print(f"⚠️ [DELETE_PERSON] Invalid relationship data for final cleanup: rel_type={rel_type}, to_type={to_type}, to_id={to_id}")
                    except Exception as cleanup_error:
                        print(f"⚠️ [DELETE_PERSON] Failed final cleanup: {cleanup_error}")
        
        print(f"🗑️ [DELETE_PERSON] Final cleanup removed {final_cleanup_count} remaining relationships")
        
        # Delete the person node from Kuzu
        print(f"🗑️ [DELETE_PERSON] Deleting person node {person_id}")
        print(f"🔍 [DELETE_PERSON] Storage object: {storage}")
        
        # Check if person or author node exists in Kuzu
        print(f"🔍 [DELETE_PERSON] Checking if person node exists: {person_id}")
        person_node = storage.get_node('person', person_id)
        person_exists = person_node is not None
        print(f"🔍 [DELETE_PERSON] Person node exists: {person_exists}")
        
        print(f"🔍 [DELETE_PERSON] Checking if author node exists: {person_id}")
        author_node = storage.get_node('author', person_id)
        author_exists = author_node is not None
        print(f"🔍 [DELETE_PERSON] Author node exists: {author_exists}")
        
        deletion_success = False
        
        try:
            # Try deleting as person first
            if person_exists:
                print(f"🗑️ [DELETE_PERSON] Attempting to delete person node")
                deletion_success = storage.delete_node('person', person_id)
                print(f"🔍 [DELETE_PERSON] Person delete result: {deletion_success}")
            
            # Try deleting as author if person deletion failed or person node didn't exist
            if not deletion_success and author_exists:
                print(f"🗑️ [DELETE_PERSON] Attempting to delete author node")
                deletion_success = storage.delete_node('author', person_id)
                print(f"🔍 [DELETE_PERSON] Author delete result: {deletion_success}")
            
        except Exception as delete_error:
            print(f"💥 [DELETE_PERSON] Exception during delete operations: {delete_error}")
            traceback.print_exc()
            deletion_success = False
        
        if deletion_success:
            print(f"✅ [DELETE_PERSON] Successfully deleted person node {person_id}")
            flash(f'Person "{person_name}" deleted successfully.', 'success')
        else:
            print(f"❌ [DELETE_PERSON] Failed to delete person node {person_id}")
            flash(f'Error deleting person "{person_name}". Please try again.', 'error')
        
        return redirect(url_for('main.people'))
    
    except Exception as e:
        current_app.logger.error(f"Error deleting person {person_id}: {e}")
        flash('Error deleting person. Please try again.', 'error')
        return redirect(url_for('main.people'))


@bp.route('/person/<person_id>/refresh_metadata', methods=['POST'])
@login_required
def refresh_person_metadata(person_id):
    """Refresh person metadata from OpenLibrary."""
    try:
        from .utils import search_author_by_name, fetch_author_data
        
        # Get the current person
        person = book_service.get_person_by_id_sync(person_id)
        if not person:
            flash('Person not found.', 'error')
            return redirect(url_for('main.people'))
        
        person_name = getattr(person, 'name', '')
        current_openlibrary_id = getattr(person, 'openlibrary_id', None)
        
        metadata_updated = False
        
        # If person already has an OpenLibrary ID, fetch fresh data
        if current_openlibrary_id:
            current_app.logger.info(f"Refreshing metadata for person {person_name} using existing OpenLibrary ID: {current_openlibrary_id}")
            author_data = fetch_author_data(current_openlibrary_id)
            
            if author_data:
                # Update person with new metadata
                update_data = {}
                
                if author_data.get('bio') and author_data['bio'] != getattr(person, 'bio', ''):
                    update_data['bio'] = author_data['bio']
                
                if author_data.get('birth_date'):
                    # Parse birth year from date string
                    try:
                        if isinstance(author_data['birth_date'], str):
                            # Try to extract year from various date formats
                            import re
                            year_match = re.search(r'\b(19|20)\d{2}\b', author_data['birth_date'])
                            if year_match:
                                birth_year = int(year_match.group())
                                if birth_year != getattr(person, 'birth_year', None):
                                    update_data['birth_year'] = birth_year
                    except (ValueError, TypeError):
                        pass
                
                if author_data.get('death_date'):
                    # Parse death year from date string
                    try:
                        if isinstance(author_data['death_date'], str):
                            import re
                            year_match = re.search(r'\b(19|20)\d{2}\b', author_data['death_date'])
                            if year_match:
                                death_year = int(year_match.group())
                                if death_year != getattr(person, 'death_year', None):
                                    update_data['death_year'] = death_year
                    except (ValueError, TypeError):
                        pass
                
                if author_data.get('photo_url') and author_data['photo_url'] != getattr(person, 'image_url', ''):
                    update_data['image_url'] = author_data['photo_url']
                
                if update_data:
                    # Update the person in the database
                    success = book_service.update_person_sync(person_id, update_data)
                    if success:
                        metadata_updated = True
                        updated_fields = list(update_data.keys())
                        flash(f'Metadata refreshed for {person_name}. Updated: {", ".join(updated_fields)}.', 'success')
                    else:
                        flash(f'Failed to update metadata for {person_name}.', 'error')
                else:
                    flash(f'No new metadata found for {person_name}.', 'info')
            else:
                flash(f'Could not fetch metadata for {person_name} from OpenLibrary.', 'warning')
        
        # If no OpenLibrary ID, try to search for one
        else:
            current_app.logger.info(f"Searching for OpenLibrary data for person: {person_name}")
            author_data = search_author_by_name(person_name)
            
            if author_data and author_data.get('openlibrary_id'):
                # Update person with found metadata
                update_data = {
                    'openlibrary_id': author_data['openlibrary_id']
                }
                
                if author_data.get('bio'):
                    update_data['bio'] = author_data['bio']
                
                if author_data.get('birth_date'):
                    try:
                        if isinstance(author_data['birth_date'], str):
                            import re
                            year_match = re.search(r'\b(19|20)\d{2}\b', author_data['birth_date'])
                            if year_match:
                                update_data['birth_year'] = int(year_match.group())
                    except (ValueError, TypeError):
                        pass
                
                if author_data.get('death_date'):
                    try:
                        if isinstance(author_data['death_date'], str):
                            import re
                            year_match = re.search(r'\b(19|20)\d{2}\b', author_data['death_date'])
                            if year_match:
                                update_data['death_year'] = int(year_match.group())
                    except (ValueError, TypeError):
                        pass
                
                if author_data.get('photo_url'):
                    update_data['image_url'] = author_data['photo_url']
                
                # Update the person in the database
                success = book_service.update_person_sync(person_id, update_data)
                if success:
                    metadata_updated = True
                    updated_fields = [field for field in update_data.keys() if field != 'openlibrary_id']
                    if updated_fields:
                        flash(f'Found and linked OpenLibrary data for {person_name}. Updated: {", ".join(updated_fields)}.', 'success')
                    else:
                        flash(f'Found and linked OpenLibrary profile for {person_name}.', 'success')
                else:
                    flash(f'Failed to update metadata for {person_name}.', 'error')
            else:
                flash(f'No OpenLibrary data found for {person_name}.', 'info')
        
        # Log the refresh attempt
        if metadata_updated:
            current_app.logger.info(f"Successfully refreshed metadata for person {person_name} (ID: {person_id})")
        else:
            current_app.logger.info(f"No metadata updates found for person {person_name} (ID: {person_id})")
    
    except Exception as e:
        current_app.logger.error(f"Error refreshing metadata for person {person_id}: {e}")
        flash('Error refreshing person metadata. Please try again.', 'error')
    
    return redirect(url_for('main.person_details', person_id=person_id))


@bp.route('/persons/bulk_delete', methods=['POST'])
@login_required
def bulk_delete_persons():
    """Delete multiple persons selected from the people view."""
    print(f"Bulk delete persons route called by user {current_user.id}")
    print(f"Form data: {request.form}")
    
    selected_person_ids = request.form.getlist('selected_persons')
    force_delete = request.form.get('force_delete') == 'true'
    print(f"Selected person IDs: {selected_person_ids}")
    print(f"Force delete: {force_delete}")
    
    if not selected_person_ids:
        print("No persons selected for deletion")
        flash('No persons selected for deletion.', 'warning')
        return redirect(url_for('main.people'))
    
    deleted_count = 0
    failed_count = 0
    failed_persons = []
    
    for person_id in selected_person_ids:
        try:
            print(f"Attempting to delete person {person_id}")
            
            # Get person details first for the name - inline implementation
            person = None
            try:
                person = book_service.get_person_by_id_sync(person_id)
            except Exception as e:
                print(f"Error getting person {person_id}: {e}")
            
            person_name = getattr(person, 'name', f'Person {person_id}') if person else f'Person {person_id}'
            
            # Check if person has associated books - handle coroutine properly
            books_by_type = None
            try:
                result = book_service.get_books_by_person_sync(person_id, str(current_user.id))
                # Handle potential coroutine return
                import inspect
                if inspect.iscoroutine(result):
                    import asyncio
                    try:
                        loop = asyncio.get_event_loop()
                        if not loop.is_running():
                            books_by_type = loop.run_until_complete(result)
                    except Exception:
                        books_by_type = {}
                else:
                    books_by_type = result
            except Exception as e:
                print(f"Error getting books for person {person_id}: {e}")
                books_by_type = {}
            
            total_books = 0
            if books_by_type:
                if isinstance(books_by_type, dict):
                    for book_list in books_by_type.values():
                        if book_list and hasattr(book_list, '__len__'):
                            total_books += len(book_list)
                elif hasattr(books_by_type, '__len__'):
                    try:
                        total_books = len(books_by_type)
                    except (TypeError, AttributeError):
                        total_books = 0
            
            # Skip deletion if person has books and force_delete is False
            if total_books > 0 and not force_delete:
                print(f"Skipping person {person_id} ({person_name}) - has {total_books} associated books (force_delete={force_delete})")
                failed_count += 1
                failed_persons.append(f"{person_name} (has {total_books} books)")
                continue
            
            # If force_delete is True and person has books, remove book associations first
            if total_books > 0 and force_delete:
                print(f"Force deleting person {person_id} ({person_name}) - removing {total_books} book associations")
                try:
                    # Get the storage instance for cleanup
                    from .infrastructure.kuzu_graph import get_graph_storage
                    storage = get_graph_storage()
                    
                    # Clean up all relationships TO this person
                    print(f"🗑️ [FORCE_DELETE] Removing all relationships to person {person_id}")
                    
                    # Get all book nodes to check for relationships
                    all_book_nodes = storage.find_nodes_by_type('book')
                    relationships_removed = 0
                    
                    for book_data in all_book_nodes:
                        if not book_data or not book_data.get('_id'):
                            continue
                        
                        book_id = book_data.get('_id')
                        # Get all relationships from this book
                        book_relationships = []
                        
                        # Check different relationship types
                        for rel_type in ['WRITTEN_BY', 'NARRATED_BY', 'EDITED_BY', 'CONTRIBUTED_BY']:
                            rels = storage.get_relationships('book', book_id, rel_type)
                            book_relationships.extend(rels)
                        
                        # Remove relationships that point to our person
                        for rel in book_relationships:
                            to_type = rel.get('to_type')
                            to_id = rel.get('to_id')
                            rel_type = rel.get('relationship')
                            
                            if (to_type == 'person' and to_id == person_id) or \
                               (to_type == 'author' and to_id == person_id):
                                try:
                                    storage.remove_relationship('book', book_id, rel_type, to_type, to_id)
                                    relationships_removed += 1
                                    print(f"🗑️ [FORCE_DELETE] Removed relationship: book {book_id} -> {rel_type} -> {to_type}:{to_id}")
                                except Exception as cleanup_error:
                                    print(f"⚠️ [FORCE_DELETE] Failed to remove relationship: {cleanup_error}")
                    
                    print(f"🗑️ [FORCE_DELETE] Removed {relationships_removed} book relationships for person {person_id}")
                    
                except Exception as cleanup_error:
                    print(f"❌ [FORCE_DELETE] Error cleaning up book associations for {person_id}: {cleanup_error}")
                    # Continue with deletion even if cleanup partially failed
            
            # Perform the deletion manually like the individual delete
            from .infrastructure.kuzu_graph import get_graph_storage
            storage = get_graph_storage()
            
            deletion_success = False
            try:
                # Check if person or author node exists in Kuzu
                person_node = storage.get_node('person', person_id)
                author_node = storage.get_node('author', person_id)
                
                deleted_nodes_count = 0
                
                # Delete person node if it exists
                if person_node:
                    if storage.delete_node('person', person_id):
                        deleted_nodes_count += 1
                        print(f"✅ [BULK_DELETE_PERSONS] Deleted person node for {person_id}")
                
                # Delete author node if it exists
                if author_node:
                    if storage.delete_node('author', person_id):
                        deleted_nodes_count += 1
                        print(f"✅ [BULK_DELETE_PERSONS] Deleted author node for {person_id}")
                
                if deleted_nodes_count > 0:
                    deletion_success = True
                    print(f"✅ [BULK_DELETE_PERSONS] Deleted {deleted_nodes_count} nodes for {person_id}")
                else:
                    print(f"⚠️ [BULK_DELETE_PERSONS] No nodes found to delete for {person_id}")
                    # Still count as success if no nodes were found (person might have been already deleted)
                    deletion_success = True
                
            except Exception as delete_error:
                print(f"❌ [BULK_DELETE_PERSONS] Error deleting person {person_id}: {delete_error}")
                deletion_success = False
            
            if deletion_success:
                deleted_count += 1
                print(f"✅ [BULK_DELETE_PERSONS] Successfully deleted {person_name}")
            else:
                failed_count += 1
                failed_persons.append(person_name)
                print(f"❌ [BULK_DELETE_PERSONS] Failed to delete {person_name}")
                
        except Exception as e:
            print(f"Error deleting person {person_id}: {e}")
            failed_count += 1
            failed_persons.append(f"Person {person_id}")
    
    print(f"Bulk delete completed: {deleted_count} deleted, {failed_count} failed")
    
    # Provide feedback to user
    if deleted_count > 0:
        flash(f'Successfully deleted {deleted_count} person(s).', 'success')
    
    if failed_count > 0:
        # Categorize failures
        books_related_failures = [name for name in failed_persons if 'books' in name]
        other_failures = [name for name in failed_persons if 'books' not in name]
        
        if books_related_failures:
            if len(books_related_failures) <= 3:
                flash(f'Cannot delete: {", ".join(books_related_failures)}. Use "Merge People" to consolidate entries with books.', 'warning')
            else:
                flash(f'Cannot delete {len(books_related_failures)} person(s) because they have associated books. Use "Merge People" to consolidate entries.', 'warning')
        
        if other_failures:
            if len(other_failures) <= 3:
                flash(f'Failed to delete: {", ".join(other_failures)}. Please try again.', 'error')
            else:
                flash(f'Failed to delete {len(other_failures)} person(s). Please try again.', 'error')
    
    return redirect(url_for('main.people'))


@bp.route('/person/merge', methods=['GET', 'POST'])
@login_required
def merge_persons():
    """Merge two or more persons into one."""
    if request.method == 'POST':
        try:
            # Get form data
            primary_person_id = request.form.get('primary_person_id')
            merge_person_ids = request.form.getlist('merge_person_ids')
            
            if not primary_person_id:
                flash('Please select a primary person to merge into.', 'error')
                return redirect(url_for('main.merge_persons'))
            
            if not merge_person_ids:
                flash('Please select at least one person to merge.', 'error')
                return redirect(url_for('main.merge_persons'))
            
            if primary_person_id in merge_person_ids:
                flash('Cannot merge a person with themselves.', 'error')
                return redirect(url_for('main.merge_persons'))
            
            # Helper function to handle potential coroutine returns
            def safe_call_sync_method(method, *args, **kwargs):
                """Safely call a sync method that might return a coroutine."""
                import inspect
                result = method(*args, **kwargs)
                if inspect.iscoroutine(result):
                    import asyncio
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            # Can't use loop.run_until_complete if loop is already running
                            print(f"⚠️ [MERGE_PERSON] Loop is running, method {method.__name__} returned coroutine")
                            return None  # Return None as fallback
                        else:
                            return loop.run_until_complete(result)
                    except Exception as e:
                        print(f"⚠️ [MERGE_PERSON] Error running coroutine for {method.__name__}: {e}")
                        return None
                return result
            
            # Get persons
            primary_person = safe_call_sync_method(book_service.get_person_by_id_sync, primary_person_id)
            if not primary_person:
                flash('Primary person not found.', 'error')
                return redirect(url_for('main.merge_persons'))
            
            merge_persons = []
            for person_id in merge_person_ids:
                person = safe_call_sync_method(book_service.get_person_by_id_sync, person_id)
                if person:
                    merge_persons.append(person)
            
            if not merge_persons:
                flash('No valid persons found to merge.', 'error')
                return redirect(url_for('main.merge_persons'))
            
            # Perform merge operation
            from .infrastructure.kuzu_graph import get_graph_storage
            storage = get_graph_storage()
            
            merged_count = 0
            for merge_person in merge_persons:
                try:
                    # Find all relationships pointing to the merge person
                    # Update WRITTEN_BY relationships to point to primary person
                    all_book_nodes = storage.find_nodes_by_type('book')
                    
                    for book_data in all_book_nodes:
                        if not book_data or not book_data.get('_id'):
                            continue
                        
                        book_id = book_data.get('_id')
                        if not book_id or not isinstance(book_id, str):
                            continue
                            
                        relationships = storage.get_relationships('book', book_id, 'WRITTEN_BY')
                        
                        # Check if any relationships point to the person we're merging
                        for rel in relationships:
                            if ((rel.get('to_type') == 'author' and rel.get('to_id') == merge_person.id) or
                                (rel.get('to_type') == 'person' and rel.get('to_id') == merge_person.id)):
                                
                                # Delete old relationship - with type safety
                                to_type = rel.get('to_type')
                                to_id = rel.get('to_id')
                                if to_type and to_id and isinstance(to_type, str) and isinstance(to_id, str):
                                    storage.delete_relationship('book', book_id, 'WRITTEN_BY', to_type, to_id)
                                    
                                    # Create new relationship to primary person
                                    storage.create_relationship('book', book_id, 'WRITTEN_BY', 'person', primary_person_id)
                                    
                                    current_app.logger.info(f"Merged relationship: Book {book_id} now points to person {primary_person_id} instead of {merge_person.id}")
                    
                    # Delete the merged person
                    storage.delete_node('person', merge_person.id)
                    storage.delete_node('author', merge_person.id)  # Also delete any author node
                    merged_count += 1
                    
                except Exception as e:
                    current_app.logger.error(f"Error merging person {merge_person.id}: {e}")
                    continue
            
            if merged_count > 0:
                person_names = [p.name for p in merge_persons[:merged_count]]
                primary_person_name = getattr(primary_person, 'name', 'Unknown Person')
                flash(f'Successfully merged {merged_count} person(s) ({", ".join(person_names)}) into "{primary_person_name}".', 'success')
            else:
                flash('No persons were merged due to errors.', 'error')
            
            return redirect(url_for('main.person_details', person_id=primary_person_id))
        
        except Exception as e:
            current_app.logger.error(f"Error during person merge: {e}")
            flash('Error merging persons. Please try again.', 'error')
    
    # GET request - show merge form
    try:
        # Helper function to handle potential coroutine returns
        def safe_call_sync_method(method, *args, **kwargs):
            """Safely call a sync method that might return a coroutine."""
            import inspect
            result = method(*args, **kwargs)
            if inspect.iscoroutine(result):
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # Can't use loop.run_until_complete if loop is already running
                        print(f"⚠️ [MERGE_GET] Loop is running, method {method.__name__} returned coroutine")
                        return []  # Return empty list as fallback
                    else:
                        return loop.run_until_complete(result)
                except Exception as e:
                    print(f"⚠️ [MERGE_GET] Error running coroutine for {method.__name__}: {e}")
                    return []
            return result
        
        all_persons = safe_call_sync_method(book_service.list_all_persons_sync)
        if all_persons is None:
            all_persons = []
        all_persons.sort(key=lambda p: p.name.lower())
        return render_template('merge_persons.html', persons=all_persons)
    
    except Exception as e:
        current_app.logger.error(f"Error loading merge persons page: {e}")
        flash('Error loading merge page.', 'error')
        return redirect(url_for('main.people'))


@bp.route('/api/person/search')
@login_required
def api_search_persons():
    """API endpoint for searching persons (used in autocomplete)."""
    query = request.args.get('q', '').strip()
    
    if len(query) < 2:
        return jsonify([])
    
    try:
        # Helper function to handle potential coroutine returns
        def safe_call_sync_method(method, *args, **kwargs):
            """Safely call a sync method that might return a coroutine."""
            import inspect
            result = method(*args, **kwargs)
            if inspect.iscoroutine(result):
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # Can't use loop.run_until_complete if loop is already running
                        return []  # Return empty list as fallback
                    else:
                        return loop.run_until_complete(result)
                except Exception as e:
                    return []
            return result
        
        persons = safe_call_sync_method(book_service.search_persons_sync, query, 20)
        if persons is None:
            persons = []
        
        results = []
        for person in persons:
            # Get book count for each person
            try:
                books_by_type = safe_call_sync_method(book_service.get_books_by_person_sync, person.id, None)
                if books_by_type is None:
                    books_by_type = {}
                total_books = sum(len(books) for books in books_by_type.values())
            except:
                total_books = 0
            
            results.append({
                'id': person.id,
                'name': person.name,
                'bio': person.bio[:100] + '...' if person.bio and len(person.bio) > 100 else person.bio,
                'book_count': total_books,
                'birth_year': person.birth_year,
                'death_year': person.death_year
            })
        
        return jsonify(results)
    
    except Exception as e:
        current_app.logger.error(f"Error searching persons: {e}")
        return jsonify([])

@bp.route('/toggle_theme', methods=['POST'])
@login_required
def toggle_theme():
    """Toggle user's theme preference between light and dark."""
    try:
        data = request.get_json()
        current_theme = data.get('current_theme', 'light')
        
        # Toggle theme
        new_theme = 'dark' if current_theme == 'light' else 'light'
        
        # Store theme preference in session for authenticated users
        if current_user.is_authenticated:
            from .infrastructure.kuzu_graph import get_graph_storage
            # For themes, we use session storage instead of KuzuDB
            # since themes are UI preferences, not core data
        
        # Store in session
        session['theme'] = new_theme
        
        return jsonify({
            'success': True,
            'new_theme': new_theme
        })
        
    except Exception as e:
        current_app.logger.error(f"Error toggling theme: {e}")
        return jsonify({
            'success': False,
            'error': 'Failed to toggle theme'
        }), 500

@bp.route('/direct_import', methods=['GET', 'POST'])
@login_required
def direct_import():
    """Handle direct import for Goodreads/StoryGraph."""
    import tempfile
    import os
    
    # Check if we have a suggested file from the session
    suggested = request.args.get('suggested', 'false').lower() == 'true'
    import_type = request.args.get('import_type', 'goodreads')
    suggested_filename = session.get('direct_import_filename')
    
    if request.method == 'GET':
        return render_template('direct_import.html', 
                             suggested=suggested,
                             import_type=import_type,
                             suggested_filename=suggested_filename)
    
    # POST request - handle the import
    try:
        use_suggested_file = request.form.get('use_suggested_file') == 'true'
        
        if use_suggested_file and session.get('direct_import_file'):
            # Use the file from session (already uploaded)
            temp_path = session['direct_import_file']
            original_filename = session.get('direct_import_filename', 'import.csv')
        else:
            # Handle new file upload
            file = request.files.get('csv_file')
            if not file or not file.filename.endswith('.csv'):
                flash('Please select a valid CSV file.', 'error')
                return redirect(url_for('main.direct_import'))
            
            # Save file temporarily
            filename = secure_filename(file.filename)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.csv', prefix=f'direct_import_{current_user.id}_')
            file.save(temp_file.name)
            temp_path = temp_file.name
            original_filename = filename
        
        # Detect the file type by checking headers
        with open(temp_path, 'r', encoding='utf-8') as csvfile:
            first_line = csvfile.readline()
            
        # Determine import type based on headers
        goodreads_signatures = ['Book Id', 'Author l-f', 'Bookshelves', 'Exclusive Shelf']
        storygraph_signatures = ['Read Status', 'Moods', 'Pace', 'Character- or Plot-Driven?']
        
        detected_type = None
        if any(sig in first_line for sig in goodreads_signatures):
            detected_type = 'goodreads'
        elif any(sig in first_line for sig in storygraph_signatures):
            detected_type = 'storygraph'
        else:
            flash('This file does not appear to be a valid Goodreads or StoryGraph export.', 'error')
            return redirect(url_for('main.direct_import'))
        
        # Create an import job using the existing import infrastructure
        task_id = str(uuid.uuid4())
        
        # Get the appropriate field mappings for the detected platform
        if detected_type == 'goodreads':
            field_mappings = get_goodreads_field_mappings()
        else:  # storygraph
            field_mappings = get_storygraph_field_mappings()
        
        # Create import job data
        job_data = {
            'task_id': task_id,
            'user_id': current_user.id,
            'csv_file_path': temp_path,
            'field_mappings': field_mappings,
            'default_reading_status': 'library_only',
            'duplicate_handling': 'skip',
            'custom_fields_enabled': True,
            'direct_import_type': detected_type,
            'status': 'pending',
            'processed': 0,
            'success': 0,
            'errors': 0,
            'total': 0,
            'start_time': datetime.utcnow().isoformat(),
            'current_book': None,
            'error_messages': [],
            'recent_activity': []
        }
        
        # Count total rows
        try:
            with open(temp_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                job_data['total'] = sum(1 for _ in reader)
        except:
            job_data['total'] = 0
        
        # Store job data
        print(f"🏗️ [CREATE] Creating job {task_id} for user {current_user.id}")
        kuzu_success = store_job_in_kuzu(task_id, job_data)
        import_jobs[task_id] = job_data
        
        print(f"📊 [CREATE] Kuzu storage: {'✅' if kuzu_success else '❌'}")
        print(f"💾 [CREATE] Memory storage: ✅")
        print(f"🔧 [CREATE] Job status: {job_data['status']}")
        
        # Auto-create custom fields for the platform
        auto_create_custom_fields(field_mappings, current_user.id)
        
        # Start the import in background
        app = current_app._get_current_object()
        def run_import():
            with app.app_context():
                try:
                    start_import_job(task_id)
                except Exception as e:
                    if task_id in import_jobs:
                        import_jobs[task_id]['status'] = 'failed'
                        if 'error_messages' not in import_jobs[task_id]:
                            import_jobs[task_id]['error_messages'] = []
                        import_jobs[task_id]['error_messages'].append(str(e))
                    app.logger.error(f"Direct import job {task_id} failed: {e}")
        
        thread = threading.Thread(target=run_import)
        thread.daemon = True
        thread.start()
        
        # Clear session data
        session.pop('direct_import_file', None)
        session.pop('direct_import_filename', None)
        
        flash(f'Started {detected_type.title()} import! You can monitor progress on the next page.', 'success')
        return redirect(url_for('main.import_books_progress', task_id=task_id))
        
    except Exception as e:
        current_app.logger.error(f"Error in direct import: {e}")
        flash('An error occurred during import. Please try again.', 'error')
        return redirect(url_for('main.direct_import'))


def get_goodreads_field_mappings():
    """Get predefined field mappings for Goodreads CSV format."""
    return {
        'Title': 'title',
        'Author': 'author',
        # Note: 'Author l-f' is ignored - it's just the same author in "Last, First" format
        'Additional Authors': 'additional_authors',
        'ISBN': 'isbn',
        'ISBN13': 'isbn',
        'My Rating': 'rating',
        'Average Rating': 'custom_global_average_rating',
        'Publisher': 'publisher',
        'Binding': 'custom_global_binding',
        'Number of Pages': 'page_count',
        'Year Published': 'publication_year',
        'Original Publication Year': 'custom_global_original_publication_year',
        'Date Read': 'date_read',
        'Date Added': 'date_added',
        'Bookshelves': 'reading_status',  # Fixed: Goodreads bookshelves are reading statuses, not categories
        'Bookshelves with positions': 'reading_status',  # Fixed: These are also reading statuses
        'Exclusive Shelf': 'reading_status',
        'My Review': 'notes',
        'Spoiler': 'custom_global_spoiler_review',
        'Private Notes': 'custom_personal_private_notes',
        'Read Count': 'custom_global_read_count',
        'Owned Copies': 'custom_personal_owned_copies',
        'Book Id': 'custom_global_goodreads_book_id'
    }


def get_storygraph_field_mappings():
    """Get predefined field mappings for StoryGraph CSV format."""
    return {
        'Title': 'title',
        'Authors': 'author',
        'Contributors': 'contributors',  # New: Handle Contributors column
        'ISBN/UID': 'isbn',  # Fixed: Use actual StoryGraph column name
        'Star Rating': 'rating',  # Fixed: StoryGraph uses "Star Rating" not "My Rating"
        'Read Status': 'reading_status',
        'Date Started': 'start_date',
        'Last Date Read': 'date_read',  # Fixed: StoryGraph uses "Last Date Read"
        'Tags': 'categories',
        'Review': 'notes',  # Fixed: StoryGraph uses "Review" not "My Review"
        'Format': 'custom_global_format',
        'Moods': 'categories',  # Fixed: Moods contain genre-like descriptors that work well as categories
        'Pace': 'custom_global_pace',
        'Character- or Plot-Driven?': 'custom_global_character_plot_driven',
        'Strong Character Development?': 'custom_global_strong_character_development',
        'Loveable Characters?': 'custom_global_loveable_characters',
        'Diverse Characters?': 'custom_global_diverse_characters',
        'Flawed Characters?': 'custom_global_flawed_characters',
        'Content Warnings': 'custom_global_content_warnings',
        'Content Warning Description': 'custom_global_content_warning_description',
        'Owned?': 'custom_personal_owned'
    }